{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0a66216",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4262641",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5e056-5096-473e-9ecd-1b7f56582122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from dotenv import find_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import sys\n",
    "\n",
    "project_home = Path(find_dotenv()).parent\n",
    "sys.path.append(str(project_home))\n",
    "print(sys.path.append(project_home))\n",
    "from constants import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c198a317",
   "metadata": {},
   "source": [
    "## Data Pipline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4188b7f7",
   "metadata": {},
   "source": [
    "### Snowpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b14f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(project_home / 'creds.json', 'r') as ff:\n",
    "    conn_param=json.load(ff)\n",
    "\n",
    "session = Session.builder.configs(conn_param).create() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c1a8",
   "metadata": {},
   "source": [
    "### Database, Schema & resource creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af13e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_tag = \"\"\"comment='{\"origin\":\"sf_sit\",\"name\":\"user_license_rationalization\",\"version\":{\"major\":1, \"minor\":0},\"attributes\":{\"component\":\"streamlit\"}}'\"\"\"\n",
    "session.sql(f\"create or replace database {LICENSING_DB} {comment_tag}\").collect()\n",
    "session.sql(f\"create schema if not exists {LICENSING_DB}.{LICENSING_SCHEMA} {comment_tag}\").collect()\n",
    "session.sql(f\"create stage if not exists {LICENSING_DB}.{LICENSING_SCHEMA}.{DATA_STAGE} {comment_tag}\").collect()\n",
    "session.sql(f\"create stage if not exists {LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE} {comment_tag}\").collect()\n",
    "session.sql(f\"create stage if not exists {LICENSING_DB}.{LICENSING_SCHEMA}.{MODELS_STAGE} {comment_tag}\").collect()\n",
    "session.sql(f\"create stage if not exists {LICENSING_DB}.{LICENSING_SCHEMA}.{OBJECT_STAGE} {comment_tag}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res = session.sql(f'use schema {LICENSING_DB}.{LICENSING_SCHEMA}').collect()\n",
    "res = session.sql(f'select current_database()').collect()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed6ea1",
   "metadata": {},
   "source": [
    "### Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa8ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "today = pd.Timestamp('2030-12-31').date()\n",
    "\n",
    "def generate_app_logs (n_users, n_days_ago, random_state=0, faker_state = 0):\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    fake = Faker()\n",
    "    Faker.seed(faker_state)\n",
    "    # today = pd.Timestamp(datetime.now().date())\n",
    "    \n",
    "    employee_list = []\n",
    "    app_list = []\n",
    "    \n",
    "    for user in range(1, n_users):\n",
    "        user_email = fake.ascii_company_email()\n",
    "        department = fake.company()\n",
    "        division = fake.job()\n",
    "        title = fake.job()\n",
    "        \n",
    "        has_leaved_company = np.random.uniform (0,100)\n",
    "        if (has_leaved_company < 5): #Generate 5% people leaving the company \n",
    "            last_day_work_days = np.random.uniform (0, n_days_ago)\n",
    "            field_last_work_day = today - timedelta (days = day)\n",
    "        else:\n",
    "            last_day_work_days = 0\n",
    "            field_last_work_day = today + timedelta (days = 365) # set to the future for now\n",
    "        \n",
    "        n_logings = int(np.random.uniform (0, n_days_ago - last_day_work_days))  # Between never logging and every day\n",
    "        \n",
    "        for l in range (0, n_logings):\n",
    "            day = np.random.uniform(0, n_days_ago) # get a ramdom day\n",
    "            \n",
    "            ## LetÂ´s introduce some users who do not log in thet last 30 days\n",
    "            not_loggin = np.random.uniform (0,100)\n",
    "            if (not_loggin < 25):\n",
    "                login_day = today - timedelta (days = day + last_day_work_days + 50)\n",
    "            else:\n",
    "                login_day = today - timedelta (days = day + last_day_work_days)\n",
    "            app_id = random.choice([1,2])\n",
    "            app_list.append([app_id, user_email, login_day])\n",
    "            \n",
    "        employee_list.append([app_id, user_email, department, division, title, str(field_last_work_day)])\n",
    "        \n",
    "    df_employee = pd.DataFrame(employee_list, columns = ['APP_ID', 'SESSION_USER', 'DEPARTMENT', 'DIVISION', 'TITLE', 'LAST_DAY_OF_WORK'])\n",
    "    df_app = pd.DataFrame(app_list, columns = ['APP_ID', 'SESSION_USER', 'SNAPSHOT_DATETIME'])\n",
    "    \n",
    "    return df_app, df_employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb1bf4",
   "metadata": {},
   "source": [
    "## write the sample employee_metadata and okta_logs directly to the tables in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d432d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_1, df_employee = generate_app_logs (n_users=5000, n_days_ago =3650, random_state= 6, faker_state= 4)\n",
    "session.write_pandas(df=df_employee, table_name=TBL_EMPLOYEE_METADATA, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_EMPLOYEE_METADATA} SET {comment_tag}\").collect()\n",
    "session.write_pandas(df=df_app_1, table_name=TBL_OKTA_USERS, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_OKTA_USERS} SET {comment_tag}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf495fd",
   "metadata": {},
   "source": [
    "## write the sample app_logs data directly to the table in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d342db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_2, k = generate_app_logs (n_users=5000, n_days_ago =3650, random_state= 19, faker_state= 4)\n",
    "session.write_pandas(df=df_app_1, table_name=TBL_APP_LOGS, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_APP_LOGS} SET {comment_tag}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad7750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_working_days (n_days_ago = 365):\n",
    "    \n",
    "    today = pd.Timestamp('2030-12-31').date()\n",
    "    \n",
    "    calendar_list = []\n",
    "    \n",
    "    for d in range(0, n_days_ago):    \n",
    "        calendar_day = today - timedelta (days = d)\n",
    "                \n",
    "        num = np.random.uniform (0,100)\n",
    "        if (num < 20): # 20% of holidays\n",
    "            is_working_day = False\n",
    "        else:\n",
    "            is_working_day = True\n",
    "        \n",
    "        calendar_list.append([calendar_day, is_working_day])\n",
    "        \n",
    "    \n",
    "    df_cal = pd.DataFrame(calendar_list, columns = ['SNAPSHOT_DATETIME', 'WORK_DAY'])\n",
    "    \n",
    "    return df_cal    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cbea9",
   "metadata": {},
   "source": [
    "## write the sample work_days data directly to the table in the DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83aec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal = generate_working_days(n_days_ago = 3650)\n",
    "session.write_pandas(df=df_cal, table_name=TBL_WORK_DAYS, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_WORK_DAYS} SET {comment_tag}\").collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719eda36",
   "metadata": {},
   "source": [
    "## Populate MONITORED_APPS and WHITELISTED_USERS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "vals.append(['APP-1', 1])\n",
    "vals.append(['APP-2', 2])\n",
    "df = pd.DataFrame(vals, columns = ['APP_NAME', 'APP_ID'])\n",
    "session.write_pandas(df=df, table_name=TBL_MONITORED_APPS, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_MONITORED_APPS} SET {comment_tag}\").collect()\n",
    "\n",
    "vals = []\n",
    "vals.append('abc.vip@organization.com')\n",
    "df = pd.DataFrame(vals, columns = ['EMAIL'])\n",
    "session.write_pandas(df=df, table_name=TBL_WHITELISTED_USERS, database=LICENSING_DB, schema=LICENSING_SCHEMA, chunk_size=100000, compression='snappy', parallel=4, auto_create_table=True, overwrite=True )\n",
    "session.sql(f\"ALTER TABLE {LICENSING_DB}.{LICENSING_SCHEMA}.{TBL_WHITELISTED_USERS} SET {comment_tag}\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d05adae2",
   "metadata": {},
   "source": [
    "### upload solution artifacts to the DEPENDENCY stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5189fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session.sql(f\"PUT file://train.py @{LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE} overwrite=True auto_compress=False\").collect()\n",
    "session.sql(f\"PUT file://constants.py @{LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE} overwrite=True auto_compress=False\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29ab4816",
   "metadata": {},
   "source": [
    "### Local utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.udf.register_from_file(\n",
    "      file_path=\"train.py\"\n",
    "      , func_name=\"contains_anyof\"\n",
    "      , name=f\"{LICENSING_DB}.{LICENSING_SCHEMA}.udf_contains_anyof\"\n",
    "      , is_permanent=True\n",
    "      , packages = [\"snowflake-snowpark-python\"]\n",
    "      , imports=[f\"@{LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE}/constants.py\"]\n",
    "      , stage_location=f\"{LICENSING_DB}.{LICENSING_SCHEMA}.{OBJECT_STAGE}\"\n",
    "      , replace=True\n",
    ")\n",
    "session.sql(f\"ALTER FUNCTION {LICENSING_DB}.{LICENSING_SCHEMA}.udf_contains_anyof(STRING,ARRAY) SET {comment_tag}\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46b6077d",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51681e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sproc.register_from_file(\n",
    "    file_path = f\"@{LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE}/train.py\"\n",
    "    ,func_name = \"run_model_today\"\n",
    "    ,name=f\"{LICENSING_DB}.{LICENSING_SCHEMA}.run_model_today\"\n",
    "    ,input_types=[T.IntegerType(),T.IntegerType(),T.FloatType(),T.BooleanType(),T.BooleanType(),T.BooleanType(), T.BooleanType()]\n",
    "    ,return_type=T.VariantType()\n",
    "    ,is_permanent=True\n",
    "    ,replace=True\n",
    "    ,stage_location=f\"@{LICENSING_DB}.{LICENSING_SCHEMA}.{OBJECT_STAGE}\"\n",
    "    ,packages=['snowflake-snowpark-python','pandas','scikit-learn==1.2.1','joblib==1.1.1', 'numpy']\n",
    "    ,imports=[f\"@{LICENSING_DB}.{LICENSING_SCHEMA}.{DEPS_STAGE}/constants.py\"]\n",
    ")\n",
    "session.sql(f\"ALTER PROCEDURE {LICENSING_DB}.{LICENSING_SCHEMA}.run_model_today(INT, INT, FLOAT, BOOLEAN, BOOLEAN, BOOLEAN, BOOLEAN) SET {comment_tag}\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04883492",
   "metadata": {},
   "source": [
    "## License usage probablity prediction with revocation decision\n",
    "Local trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0528570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from train import run_model_today\n",
    "results = session.call('run_model_today', 1,120,0.5,False,False,False,False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7414d",
   "metadata": {},
   "source": [
    "## Run the section below if you are wanting to run application SiS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stage for Streamlit app files\n",
    "session.sql(f\"create stage if not exists {LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE {comment_tag}\").collect()\n",
    "\n",
    "# Upload Streamlit app files to the stage (excluding system files/directories)\n",
    "print(\"ðŸ“ Uploading app files to stage...\")\n",
    "\n",
    "# Main app file and environment\n",
    "session.sql(f\"PUT file://../app.py @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE overwrite=True auto_compress=False\").collect()\n",
    "\n",
    "# Copy sis_environment.yml to environment.yml temporarily for upload\n",
    "import shutil\n",
    "shutil.copy('sis_environment.yml', 'environment.yml')\n",
    "session.sql(f\"PUT file://environment.yml @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE overwrite=True auto_compress=False\").collect()\n",
    "# Clean up temporary file\n",
    "import os\n",
    "os.remove('environment.yml')\n",
    "print(\"  âœ… app.py and environment.yml uploaded\")\n",
    "\n",
    "# appPages Python files only (exclude __pycache__, .DS_Store, etc.)\n",
    "session.sql(f\"PUT file://../appPages/*.py @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/appPages overwrite=True auto_compress=False\").collect()\n",
    "session.sql(f\"PUT file://../appPages/*.md @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/appPages overwrite=True auto_compress=False\").collect()\n",
    "print(\"  âœ… appPages files uploaded\")\n",
    "\n",
    "# appUtil Python files only (exclude notebooks, __pycache__, etc.)\n",
    "session.sql(f\"PUT file://constants.py @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/appUtil overwrite=True auto_compress=False\").collect()\n",
    "session.sql(f\"PUT file://train.py @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/appUtil overwrite=True auto_compress=False\").collect()\n",
    "print(\"  âœ… appUtil files uploaded\")\n",
    "\n",
    "# Image files (PNG only)\n",
    "session.sql(f\"PUT file://../img/*.png @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/img overwrite=True auto_compress=False\").collect()\n",
    "print(\"  âœ… img files uploaded\")\n",
    "\n",
    "# Streamlit config files (if they exist)\n",
    "try:\n",
    "    session.sql(f\"PUT file://../.streamlit/config.toml @{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE/.streamlit overwrite=True auto_compress=False\").collect()\n",
    "    print(\"  âœ… .streamlit config uploaded\")\n",
    "except:\n",
    "    print(\"  âš ï¸  No .streamlit config found (optional)\")\n",
    "\n",
    "# Create Streamlit app using session.sql() \n",
    "streamlit_sql = f\"\"\"\n",
    "CREATE OR REPLACE STREAMLIT {LICENSING_DB}.{LICENSING_SCHEMA}.LICENSING_APP\n",
    "ROOT_LOCATION = '@{LICENSING_DB}.{LICENSING_SCHEMA}.STREAMLIT_STAGE'\n",
    "MAIN_FILE = '/app.py'\n",
    "QUERY_WAREHOUSE = 'APP_WH'\n",
    "{comment_tag}\n",
    "\"\"\"\n",
    "\n",
    "session.sql(streamlit_sql).collect()\n",
    "\n",
    "print(\"ðŸŽ‰ Streamlit app 'LICENSING_APP' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
