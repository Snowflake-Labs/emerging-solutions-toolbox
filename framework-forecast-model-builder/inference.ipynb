{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "md_title"
   },
   "source": [
    "# Inference Notebook\n",
    "Use the model trained in the modeling notebook to make predictions on an inference dataset.\n",
    "\n",
    "#### NOTE: The user must have an inference dataset available as a table or view in Snowflake before running this notebook.\n",
    "- If using a __direct multi-step forecasting__ pattern, the inference dataset does not need to contain records for the future datetime points.\n",
    "- If using a __global modeling__ pattern, the inference dataset must contain records for each future datetime to be forecasted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "md_instructions"
   },
   "source": [
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
    "## Instructions\n",
    "\n",
    "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
    "    - Adjust the values of the user constants\n",
    "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
    "    - The notebook will perform feature engineering steps and inference. Predictions will be stored in a Snowflake table.\n",
    "    \n",
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_imports"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.feature_store import FeatureStore, CreationMode\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "from forecast_model_builder.feature_engineering import (\n",
    "    apply_functions_in_a_loop,\n",
    "    expand_datetime,\n",
    "    recent_rolling_avg,\n",
    "    roll_up,\n",
    "    verify_current_frequency,\n",
    "    verify_valid_rollup_spec,\n",
    ")\n",
    "from forecast_model_builder.utils import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_establish_session"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session db.schema: FORECAST_MODEL_BUILDER.TEST\n",
      "Session warehouse: FORECAST_MODEL_BUILDER_WH\n",
      "Current Datetime: 2025-09-30 12:36:20.473785\n"
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "session = connect(connection_name=\"default\")\n",
    "session_db = session.connection.database\n",
    "session_schema = session.connection.schema\n",
    "session_wh = session.connection.warehouse\n",
    "print(f\"Session db.schema: {session_db}.{session_schema}\")\n",
    "print(f\"Session warehouse: {session_wh}\")\n",
    "\n",
    "# Query tag\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}'\n",
    "session.query_tag = query_tag\n",
    "\n",
    "# Get the current datetime  (This will be saved in the model storage table)\n",
    "run_dttm = datetime.now()\n",
    "print(f\"Current Datetime: {run_dttm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "md_USER_SETUP"
   },
   "source": [
    "-----\n",
    "# SETUP\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "language": "python",
    "name": "_set_global_variables"
   },
   "outputs": [],
   "source": [
    "# Establish cutoff datetime for the records that will be scored using the forecast model.\n",
    "# NOTE: For Direct Multi-Step Forecasting, this value will likely be the most recent date, since this pattern does not take in records for future dates as input.\n",
    "#       For Global Modeling, this value will be the first future datetime value in the records that will be scored.\n",
    "# NOTE: This cutoff will be applied AFTER feature engineering, so that lag features can be calculated.\n",
    "INFERENCE_START_TIMESTAMP = \"2025-01-01 00:00:00.000\"\n",
    "\n",
    "# Table name to store the PREDICTION results.\n",
    "# NOTE: If the table name is not fully qualified with DB.SCHEMA, the session's default database and schema will be used.\n",
    "# NOTE: Currently the code will overwrite the existing predictions table with the predictions from this run.\n",
    "INFERENCE_RESULT_TBL_NM = \"FORECAST_RESULTS\"\n",
    "\n",
    "# Input data for inference\n",
    "INFERENCE_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "INFERENCE_SCHEMA = \"BASE\"\n",
    "INFERENCE_FV = \"FORECAST_FEATURES\"\n",
    "\n",
    "# Name of the model to use for inference, as well as the Database and Schema of the model registry.\n",
    "# NOTE: The default model version from the registry will be used.\n",
    "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "MODEL_SCHEMA = \"MODELING\"\n",
    "MODEL_NAME = \"TEST_MODEL_1\"\n",
    "\n",
    "# Scaling up the warehouse may speed up execution time, especially if there are many partitions.\n",
    "# NOTE: If set to None, then the session warehouse will be used.\n",
    "INFERENCE_WH = \"STANDARD_XL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "name": "md_objects"
   },
   "source": [
    "-----\n",
    "# Establish objects needed for this run\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_other_objects"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session warehouse:          FORECAST_MODEL_BUILDER_WH\n",
      "WARNING: User does not have access to INFERENCE_WH = 'STANDARD_XL'. Inference will use 'FORECAST_MODEL_BUILDER_WH' instead. \n",
      "\n",
      "Model Version:              HOT_STARFISH_1\n",
      "Modeling Frequency:         day\n",
      "Train Separate Lead Models: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'online_config' is in private preview since 1.12.0. Do not use it in production.\n"
     ]
    }
   ],
   "source": [
    "# Derived Objects\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Notebook Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "SESSION_WH = session.connection.warehouse\n",
    "print(f\"Session warehouse:          {SESSION_WH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Check Inference Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "# Check that the user specified an available warehouse as INFERENCE_WH. If not, use the session warehouse.\n",
    "available_warehouses = [\n",
    "    row[\"NAME\"]\n",
    "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
    "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "if INFERENCE_WH in available_warehouses:\n",
    "    print(f\"Inference warehouse:        {INFERENCE_WH} \\n\")\n",
    "else:\n",
    "    print(\n",
    "        f\"WARNING: User does not have access to INFERENCE_WH = '{INFERENCE_WH}'. Inference will use '{SESSION_WH}' instead. \\n\"\n",
    "    )\n",
    "    INFERENCE_WH = SESSION_WH\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Fully qualified MODEL NAME\n",
    "# -----------------------------------------------------------------------\n",
    "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Get the model and the version name of the default version\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish registry object\n",
    "reg = registry.Registry(\n",
    "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
    ")\n",
    "\n",
    "# Get the model from the registry\n",
    "mv = reg.get_model(qualified_model_name).default\n",
    "\n",
    "# Get the default version name\n",
    "model_version_nm = mv.version_name\n",
    "\n",
    "print(f\"Model Version:              {model_version_nm}\")\n",
    "\n",
    "# --------------------------------\n",
    "# User Constants from Model Setup\n",
    "# --------------------------------\n",
    "stored_constants = mv.show_metrics()[\"user_settings\"]\n",
    "\n",
    "TIME_PERIOD_COLUMN = stored_constants[\"TIME_PERIOD_COLUMN\"]\n",
    "TARGET_COLUMN = stored_constants[\"TARGET_COLUMN\"]\n",
    "PARTITION_COLUMNS = stored_constants[\"PARTITION_COLUMNS\"]\n",
    "EXOGENOUS_COLUMNS = stored_constants[\"EXOGENOUS_COLUMNS\"]\n",
    "ALL_EXOG_COLS_HAVE_FUTURE_VALS = stored_constants[\"ALL_EXOG_COLS_HAVE_FUTURE_VALS\"]\n",
    "CREATE_LAG_FEATURE = stored_constants[\"CREATE_LAG_FEATURE\"]\n",
    "CURRENT_FREQUENCY = stored_constants[\"CURRENT_FREQUENCY\"]\n",
    "ROLLUP_FREQUENCY = stored_constants[\"ROLLUP_FREQUENCY\"]\n",
    "ROLLUP_AGGREGATIONS = stored_constants[\"ROLLUP_AGGREGATIONS\"]\n",
    "FORECAST_HORIZON = stored_constants[\"FORECAST_HORIZON\"]\n",
    "INFERENCE_APPROX_BATCH_SIZE = stored_constants[\"INFERENCE_APPROX_BATCH_SIZE\"]\n",
    "MODEL_BINARY_STORAGE_TBL_NM = stored_constants[\"MODEL_BINARY_STORAGE_TBL_NM\"]\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a window spec\n",
    "# -----------------------------------------------------------------------\n",
    "window_spec = Window.partitionBy(PARTITION_COLUMNS).orderBy(TIME_PERIOD_COLUMN)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a variable to hold the granularity at which we will be modeling\n",
    "# -----------------------------------------------------------------------\n",
    "CURRENT_FREQUENCY = CURRENT_FREQUENCY.lower()\n",
    "\n",
    "if ROLLUP_FREQUENCY is not None:\n",
    "    ROLLUP_FREQUENCY = ROLLUP_FREQUENCY.lower()\n",
    "    if ROLLUP_FREQUENCY.lower() == \"none\":\n",
    "        ROLLUP_FREQUENCY = None\n",
    "\n",
    "modeling_frequency = CURRENT_FREQUENCY if ROLLUP_FREQUENCY is None else ROLLUP_FREQUENCY\n",
    "print(f\"Modeling Frequency:         {modeling_frequency}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish modeling pattern\n",
    "# -----------------------------------------------------------------------\n",
    "# Either (1) train_separate_lead_models = False : all features have future values in the inference data, so we don't need a separate model for each lead\n",
    "# or (2) train_separate_lead_models = True : data contains exogenous variables that the inference data won't have future values for, requiring lead modeling\n",
    "train_separate_lead_models = (\n",
    "    False\n",
    "    if ALL_EXOG_COLS_HAVE_FUTURE_VALS is True or len(EXOGENOUS_COLUMNS) == 0\n",
    "    else True\n",
    ")\n",
    "print(f\"Train Separate Lead Models: {train_separate_lead_models}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Get feature view\n",
    "# --------------------------------\n",
    "\n",
    "fs = FeatureStore(\n",
    "    session,\n",
    "    database=session_db,\n",
    "    name=session_schema,\n",
    "    default_warehouse=SESSION_WH,\n",
    "    creation_mode=CreationMode.FAIL_IF_NOT_EXIST,\n",
    ")\n",
    "\n",
    "fv = mv.lineage(direction='upstream')[0]\n",
    "\n",
    "sdf = fs.read_feature_view(fv).cache_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "name": "md_inference"
   },
   "source": [
    "-----\n",
    "# Inference\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_perform_inference"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference input data row count: 2250\n",
      "Number of end partition invocations to expect in the query profile: 250\n",
      "Predictions\n",
      "-----------------------------------------------------------------------\n",
      "|\"_PRED_\"           |\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\n",
      "-----------------------------------------------------------------------\n",
      "|399.9723205566406  |STORE_ID_12_PRODUCT_ID_5   |2025-01-01 00:00:00  |\n",
      "|407.5075378417969  |STORE_ID_12_PRODUCT_ID_5   |2025-01-02 00:00:00  |\n",
      "-----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# INFERENCE\n",
    "# ------------------------------------------------------------------------\n",
    "# Prep data set for inference\n",
    "\n",
    "# If the inference dataset does not have the TARGET column already, add it and fill it with null values\n",
    "if TARGET_COLUMN not in sdf.columns:\n",
    "    sdf = sdf.with_column(TARGET_COLUMN, F.lit(None).cast(T.FloatType()))\n",
    "\n",
    "inference_input_df = sdf.filter(\n",
    "    F.col(TIME_PERIOD_COLUMN) >= INFERENCE_START_TIMESTAMP\n",
    ").drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "model_bytes_table = (\n",
    "    session.table(MODEL_BINARY_STORAGE_TBL_NM)\n",
    "    .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "    .filter(F.col(\"MODEL_VERSION\") == model_version_nm)\n",
    "    .select(\"GROUP_IDENTIFIER_STRING\", \"MODEL_BINARY\")\n",
    ")\n",
    "\n",
    "# NOTE: We inner joint to the model bytes table to ensure that we only try run inference on partitions that have a model.\n",
    "inference_input_df = inference_input_df.join(\n",
    "    model_bytes_table, on=[\"GROUP_IDENTIFIER_STRING\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "# Add a column called BATCH_GROUP,\n",
    "#   which has the property that for each unique value there are roughly the number of records specified in batch_size.\n",
    "# Use that to create a PARTITION_ID column that will be used to run inference in batches.\n",
    "# We do this to avoid running out of memory when performing inference on a large number of records.\n",
    "largest_partition_record_count = (\n",
    "    inference_input_df.group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "    .agg(F.count(\"*\").alias(\"PARTITION_RECORD_COUNT\"))\n",
    "    .agg(F.max(\"PARTITION_RECORD_COUNT\").alias(\"MAX_PARTITION_RECORD_COUNT\"))\n",
    "    .collect()[0][\"MAX_PARTITION_RECORD_COUNT\"]\n",
    ")\n",
    "batch_size = INFERENCE_APPROX_BATCH_SIZE\n",
    "number_of_batches = math.ceil(largest_partition_record_count / batch_size)\n",
    "inference_input_df = (\n",
    "    inference_input_df.with_column(\n",
    "        \"BATCH_GROUP\", F.abs(F.random(123)) % F.lit(number_of_batches)\n",
    "    )\n",
    "    .with_column(\n",
    "        \"PARTITION_ID\",\n",
    "        F.concat_ws(\n",
    "            F.lit(\"__\"), F.col(\"GROUP_IDENTIFIER_STRING\"), F.col(\"BATCH_GROUP\")\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"RANDOM_NUMBER\", \"BATCH_GROUP\")\n",
    ")\n",
    "\n",
    "# Look at a couple rows of the inference input data\n",
    "print(f\"Inference input data row count: {inference_input_df.count()}\")\n",
    "print(\n",
    "    f\"Number of end partition invocations to expect in the query profile: {inference_input_df.select('PARTITION_ID').distinct().count()}\"\n",
    ")\n",
    "\n",
    "# Perform inference from the model registry\n",
    "session.use_warehouse(INFERENCE_WH)\n",
    "\n",
    "# Use the model to score the input data\n",
    "inference_result = mv.run(inference_input_df, partition_column=\"PARTITION_ID\").select(\n",
    "    \"_PRED_\",\n",
    "    F.col(\"GROUP_IDENTIFIER_STRING_OUT_\").alias(\"GROUP_IDENTIFIER_STRING\"),\n",
    "    F.col(f\"{TIME_PERIOD_COLUMN}_OUT_\").alias(TIME_PERIOD_COLUMN),\n",
    ")\n",
    "\n",
    "print(\"Predictions\")\n",
    "inference_result.show(2)\n",
    "session.use_warehouse(SESSION_WH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_write_predictions_to_table"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to table: FORECAST_MODEL_BUILDER.TEST.FORECAST_RESULTS\n",
      "Sample records:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.dataframe.DataFrame at 0x31e093b00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write predictions to a Snowflake table.\n",
    "# Right now this is set up to overwrite the table if it already exists.\n",
    "inference_result.write.save_as_table(\n",
    "    INFERENCE_RESULT_TBL_NM,\n",
    "    mode=\"overwrite\",\n",
    "    comment='{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}',\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Predictions written to table: {session_db}.{session_schema}.{INFERENCE_RESULT_TBL_NM}\"\n",
    ")\n",
    "\n",
    "# Look at a few rows of the snowflake table\n",
    "print(\"Sample records:\")\n",
    "session.table(INFERENCE_RESULT_TBL_NM).limit(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "lastEditStatus": {
   "authorEmail": "kirk.mason@snowflake.com",
   "authorId": "340032177737",
   "authorName": "KMASON",
   "lastEditTime": 1745508327584,
   "notebookId": "gks23kx33gb5rjz54brg",
   "sessionId": "92cfcca6-675e-421b-a33e-6e491cd3179a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
