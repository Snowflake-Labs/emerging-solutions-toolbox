{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "collapsed": false,
    "name": "Overview",
    "resultHeight": 459
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "This notebook will guide you through an exploratory data analysis of your time series data, providing the following visualizations and statistics along with an explanation of the results.\n",
    "\n",
    "### Visualizations\n",
    "- Actuals plot\n",
    "- Seasonal decomposition\n",
    "- ACF/PACF\n",
    "\n",
    "### Stationarity Tests\n",
    "- Augmented Dickey-Fuller\n",
    "\n",
    "### Baseline Analysis (Optional)\n",
    "\n",
    "To start, run first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "setup",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "import json\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "session = get_active_session()\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"eda\"}}'\n",
    "session.query_tag = query_tag\n",
    "st.success(\"Import and setup successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481d505-76fa-4f51-8424-3f0af877e81a",
   "metadata": {
    "collapsed": false,
    "name": "select_overview",
    "resultHeight": 259
   },
   "source": [
    "# SETUP\n",
    "\n",
    "- Select your source table and optionally view a sample of your data set.\n",
    "- You will also need to set your date/time series field and target value.\n",
    "- Finally you will need to specify if this is a single or multi-series analysis.  If multi-series, you will have to select the series column and determine how many values of the series you want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d498e8-b064-473a-8a9d-59a91e954d2b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "set_variables"
   },
   "outputs": [],
   "source": [
    "# Set database, schema and table for analysis\n",
    "TS_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "TS_SCHEMA = \"BASE\"\n",
    "TS_TABLE_NM = \"DAILY_PARTITIONED_SAMPLE_DATA\"\n",
    "\n",
    "# Set Time and Target columns\n",
    "TIME_PERIOD_COLUMN = \"ORDER_TIMESTAMP\"\n",
    "TARGET_COLUMN = \"TARGET\"\n",
    "\n",
    "# Single or Multi-Series - Set as 1 for multi-series or 0 for single series\n",
    "MULTISERIES = 0\n",
    "\n",
    "# If Multi-Series, set Partitions\n",
    "PARTITION_COLUMNS = [\"STORE_ID\", \"PRODUCT_ID\"]\n",
    "\n",
    "# If Multi-Series, determine how many partitions/partition groups you want to visualize in this analysis.\n",
    "# - If you have multiple partitions, one partition combination counts as one group\n",
    "VISUALIZE_COUNT = 5\n",
    "\n",
    "# Aggregate data for use by seasonal decomposition.\n",
    "# - If your existing data is at a granularity that is smaller than an hour, use this value to aggregate to hour, day or higher.\n",
    "# - Options: Hourly = \"h\", Daily = \"d\", Weekly = \"w\", Monthly = \"m\"\n",
    "FREQUENCY = \"d\"\n",
    "\n",
    "# Set analysis period for your seasonal decomposition.\n",
    "# - This value will determine for what period seasonality patterns are analyzed.\n",
    "# - You can iterate on this value and rerun to understand multiple period patters.\n",
    "# - Options: Daily = 1, Weekly = 7, Monthly = 30, Annual = 365\n",
    "PERIOD = 30\n",
    "\n",
    "# Assuming START_DATE_BASELINE_FORECAST and END_DATE_BASELINE_FORECAST are strings in \"YYYY-MM-DD\" format\n",
    "START_DATE_BASELINE_FORECAST = \"2023-12-01\"  # Example start date for testing\n",
    "END_DATE_BASELINE_FORECAST = \"2023-12-31\"  # Example end date for testing\n",
    "BASELINE_RESULT_TBL_NM = \"BASELINE_RESULTS\"\n",
    "\n",
    "st.success(\"Variable setup is successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aaacfa-bf8b-4193-aed7-6b2cca46c66a",
   "metadata": {
    "collapsed": false,
    "name": "create_overview"
   },
   "source": [
    "# Create your dataframe\n",
    "\n",
    "Run the next cell to create the dataframe that will be used for analysis.\n",
    "\n",
    "Cells such as the one below are collapsed to allow for the user to have a seamless experience running the code without looking at it.  If users are interested in reviewing the underlying code, they can simply click the Code button next to the Run button to expand the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cecd27-71f4-4b21-9fd8-55610e848bba",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "create_dataframe"
   },
   "outputs": [],
   "source": [
    "session.use_schema(f\"{TS_DB}.{TS_SCHEMA}\")\n",
    "session.get_fully_qualified_current_schema()\n",
    "\n",
    "# Define the mapping dictionary for seasonal analysis\n",
    "value_to_label = {1: \"Daily\", 7: \"Weekly\", 30: \"Monthly\", 365: \"Annual\"}\n",
    "\n",
    "\n",
    "# Function to get the label based on the input value\n",
    "def get_label(PERIOD):\n",
    "    return value_to_label.get(\n",
    "        PERIOD, \"Unknown\"\n",
    "    )  # Default to \"Unknown\" if the value is not in the dictionary\n",
    "\n",
    "\n",
    "period_label = get_label(PERIOD)\n",
    "\n",
    "# Build the SQL expression for concatenation of column names and values\n",
    "partition_expr = \" || '_' || \".join(\n",
    "    [f\"'{col}' || '_' || {col}\" for col in PARTITION_COLUMNS]\n",
    ")\n",
    "\n",
    "\n",
    "# Build the SQL expression for concatenation of column names and values\n",
    "partition_expr = \" || '_' || \".join(\n",
    "    [f\"'{col}' || '_' || {col}\" for col in PARTITION_COLUMNS]\n",
    ")\n",
    "\n",
    "# Create the GROUP_IDENTIFIER expression\n",
    "group_identifier_expr = f\"CONCAT({partition_expr})\"\n",
    "\n",
    "limit_query = f\"\"\"select distinct {group_identifier_expr}  AS GROUP_IDENTIFIER from {TS_DB}.{TS_SCHEMA}.{TS_TABLE_NM} limit {VISUALIZE_COUNT}\"\"\"\n",
    "limit_sdf = session.sql(limit_query).to_pandas()\n",
    "\n",
    "group_identifiers = tuple(limit_sdf[\"GROUP_IDENTIFIER\"].tolist())\n",
    "data_query = f\"\"\"select {group_identifier_expr} as GROUP_IDENTIFIER,* from {TS_DB}.{TS_SCHEMA}.{TS_TABLE_NM} where {group_identifier_expr} in {group_identifiers} \"\"\"\n",
    "data_sdf = session.sql(data_query).to_pandas()\n",
    "\n",
    "st.success(\n",
    "    \"You have successfully created your dataset.  Here is a sample of 10 records.\"\n",
    ")\n",
    "st.write(data_sdf.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fcbcd-f24a-4af5-9940-5f5156168ea3",
   "metadata": {
    "collapsed": false,
    "name": "visualize_overview",
    "resultHeight": 115
   },
   "source": [
    "# Visualize Your Dataset\n",
    "\n",
    "Run the next cell to visualize your time series data actual trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4c5ee-86f0-42bc-9fa4-3cb3f3373d6d",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "visualize",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Plot data for each Partition\n",
    "if MULTISERIES == 0:\n",
    "    aggregated_data = (\n",
    "        data_sdf.groupby(TIME_PERIOD_COLUMN).sum(numeric_only=True).reset_index()\n",
    "    )\n",
    "    plt.figure(figsize=(6.5, 3))\n",
    "    plt.plot(\n",
    "        aggregated_data[TIME_PERIOD_COLUMN],\n",
    "        aggregated_data[TARGET_COLUMN],\n",
    "        label=\"Single Series Analysis\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    plt.title(f\"Single Series Analysis: {TARGET_COLUMN} Over Time\", fontsize=9)\n",
    "    plt.xlabel(TIME_PERIOD_COLUMN, fontsize=8)\n",
    "    plt.ylabel(TARGET_COLUMN, fontsize=8)\n",
    "    # plt.legend(fontsize=8)\n",
    "    # Change font for axis tick labels\n",
    "    plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "    plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "    plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    series_ids = data_sdf[\"GROUP_IDENTIFIER\"].unique()\n",
    "    for series_id in series_ids:\n",
    "        series_data = data_sdf[data_sdf[\"GROUP_IDENTIFIER\"] == series_id]\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.plot(\n",
    "            series_data[TIME_PERIOD_COLUMN],\n",
    "            series_data[TARGET_COLUMN],\n",
    "            label=f\"Series {series_id}\",\n",
    "            marker=\"o\",\n",
    "        )\n",
    "        plt.title(f\"Series {series_id}: {TARGET_COLUMN} Over Time\", fontsize=9)\n",
    "        plt.xlabel(TIME_PERIOD_COLUMN, fontsize=8)\n",
    "        plt.ylabel(TARGET_COLUMN, fontsize=8)\n",
    "        # plt.legend(fontsize=8)\n",
    "        # Change font for axis tick labels\n",
    "        plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "        plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "        plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# print(data_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0d1f2-5744-4441-8048-e9a097d5b5f3",
   "metadata": {
    "collapsed": false,
    "name": "seasonal_decomp_overview",
    "resultHeight": 154
   },
   "source": [
    "# Visualize and Analyze Seasonal Decomposition & ACF/PACF and ADF\n",
    "\n",
    "**Seasonality** in time series modeling refers to patterns that repeat at fixed intervals, such as daily, weekly, monthly, or yearly cycles. These patterns can significantly impact the accuracy of time series models, and understanding seasonality is crucial for making accurate predictions. Using Seasonal Decomposition and ACF/PACF analysis, we can understand seasonality better and build models that account for seasonality.  \n",
    "\n",
    "**Stationarity** in time series modeling refers to the property of a time series where its statistical properties, such as the mean, variance, and autocorrelation structure, remain constant over time.  The ADF test is a unit root test that will check whether a time series is stationary.  When time series are non-stationary, other data engineering techniques are necessary to account for the changing time series.  \n",
    "\n",
    "Run the next cell to visualize your Seasonal Decomposition and ACF/PACF analysis, and to analyze ADF for stationarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48ad20-c8bf-473b-9990-d9897d86c1ff",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "seasonal_decomp",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "if MULTISERIES == 0:\n",
    "    # Sort the data\n",
    "    data_sdf.sort_values(by=TIME_PERIOD_COLUMN, inplace=True)\n",
    "    st.write(data_sdf)\n",
    "\n",
    "    # Combine data for all columns by date\n",
    "    aggregate_data = data_sdf.groupby([TIME_PERIOD_COLUMN])[TARGET_COLUMN].sum()\n",
    "\n",
    "    # Set the timestamp column as the index\n",
    "    aggregate_data.index = pd.to_datetime(aggregate_data.index)\n",
    "\n",
    "    # Ensure data has a consistent frequency\n",
    "    aggregate_data = aggregate_data.asfreq(FREQUENCY)\n",
    "\n",
    "    # Handle missing values\n",
    "    aggregate_data = aggregate_data.ffill().dropna()\n",
    "\n",
    "    # print(f\"Analyzing {seasonality} Seasonality with Period = {period}\")\n",
    "    result = seasonal_decompose(aggregate_data, model=\"additive\", period=PERIOD)\n",
    "\n",
    "    # Plot decomposed components\n",
    "    st.write(f\"## {period_label} Seasonal Decomposition\", fontsize=9)\n",
    "    plt.figure(figsize=(6.5, 4))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(result.observed, label=\"Observed\", color=\"blue\")\n",
    "    plt.legend(loc=\"upper left\", fontsize=6)\n",
    "    plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "    plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "    plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "    plt.subplot(412)\n",
    "    plt.plot(result.trend, label=\"Trend\", color=\"orange\")\n",
    "    plt.legend(loc=\"upper left\", fontsize=6)\n",
    "    plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "    plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "    plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "    plt.subplot(413)\n",
    "    plt.plot(result.seasonal, label=\"Seasonal\", color=\"green\")\n",
    "    plt.legend(loc=\"upper left\", fontsize=6)\n",
    "    plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "    plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "    plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "    plt.subplot(414)\n",
    "    plt.plot(result.resid, label=\"Residuals\", color=\"red\")\n",
    "    plt.legend(loc=\"upper left\", fontsize=6)\n",
    "    plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "    plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "    plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "    plt.tight_layout()\n",
    "    # plt.suptitle(f\"{period_selectbox} Seasonal Decomposition\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ACF and PACF\n",
    "    st.write(\"## ACF and PACF Analysis\", y=1.02, fontsize=9)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(6.5, 4))\n",
    "\n",
    "    # ACF plot\n",
    "    plot_acf(aggregate_data.dropna(), lags=40, ax=ax[0])\n",
    "    ax[0].set_title(\"ACF\", fontsize=8)\n",
    "    ax[0].tick_params(axis=\"both\", labelsize=8)  # Adjust label size for ticks\n",
    "\n",
    "    # Correct way to set x and y tick labels with desired font properties\n",
    "    ax[0].tick_params(axis=\"x\", labelsize=7)  # Set x-axis tick label size\n",
    "    ax[0].tick_params(axis=\"y\", labelsize=7)  # Set y-axis tick label size\n",
    "\n",
    "    # PACF plot\n",
    "    plot_pacf(aggregate_data.dropna(), lags=40, ax=ax[1])\n",
    "    ax[1].set_title(\"PACF\", fontsize=8)\n",
    "    ax[1].tick_params(axis=\"both\", labelsize=8)  # Adjust label size for ticks\n",
    "\n",
    "    # Correct way to set x and y tick labels with desired font properties\n",
    "    ax[1].tick_params(axis=\"x\", labelsize=7)  # Set x-axis tick label size\n",
    "    ax[1].tick_params(axis=\"y\", labelsize=7)  # Set y-axis tick label size\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Perform Augmented Dickey-Fuller test\n",
    "    adf_result = adfuller(aggregate_data.dropna())\n",
    "    st.write(\"## Augmented Dickey-Fuller\")\n",
    "    st.write(f\"**ADF Statistic**: {adf_result[0]}\")\n",
    "    st.write(f\"**p-value**: {adf_result[1]}\")\n",
    "    st.write(\"**Critical Values**:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        st.write(f\"   **{key}**: {value}\")\n",
    "\n",
    "else:\n",
    "    series_ids = data_sdf[\"GROUP_IDENTIFIER\"].unique()\n",
    "    for series_id in series_ids:\n",
    "        # Sort the data\n",
    "        data_sdf.sort_values(by=[\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN], inplace=True)\n",
    "        # Filter data for the current series\n",
    "        series_data = data_sdf[data_sdf[\"GROUP_IDENTIFIER\"] == series_id]\n",
    "        # Set the timestamp column as the index\n",
    "        series_data.set_index(TIME_PERIOD_COLUMN, inplace=True)\n",
    "        # Focus only on the target variable column\n",
    "        time_series = series_data[TARGET_COLUMN]\n",
    "        # Ensure data has a consistent frequency\n",
    "        time_series = time_series.asfreq(FREQUENCY)\n",
    "        # Handle missing values\n",
    "        time_series = time_series.ffill().dropna()\n",
    "\n",
    "        # Seasonal Decomposition\n",
    "        result = seasonal_decompose(time_series, model=\"additive\", period=PERIOD)\n",
    "\n",
    "        # Plot decomposed components\n",
    "        st.write(f\"# Seasonal Decomposition for Series {series_id}\")\n",
    "        st.write(f\"## {PERIOD} Seasonal Decomposition for Series {series_id}\")\n",
    "        plt.figure(figsize=(6.5, 4))\n",
    "        plt.subplot(411)\n",
    "        plt.plot(result.observed, label=\"Observed\", color=\"blue\")\n",
    "        plt.legend(loc=\"upper left\", fontsize=6)\n",
    "        plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "        plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "        plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "        plt.subplot(412)\n",
    "        plt.plot(result.trend, label=\"Trend\", color=\"orange\")\n",
    "        plt.legend(loc=\"upper left\", fontsize=6)\n",
    "        plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "        plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "        plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "        plt.subplot(413)\n",
    "        plt.plot(result.seasonal, label=\"Seasonal\", color=\"green\")\n",
    "        plt.legend(loc=\"upper left\", fontsize=6)\n",
    "        plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "        plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "        plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "        plt.subplot(414)\n",
    "        plt.plot(result.resid, label=\"Residuals\", color=\"red\")\n",
    "        plt.legend(loc=\"upper left\", fontsize=6)\n",
    "        plt.tick_params(axis=\"both\", labelsize=8)  # Adjust label size\n",
    "        plt.xticks(fontsize=7, fontname=\"Arial\")  # Set font and size for x-axis ticks\n",
    "        plt.yticks(fontsize=7, fontname=\"Arial\")  # Set font and size for y-axis ticks\n",
    "        plt.tight_layout()\n",
    "        # plt.suptitle(f\"{period_selectbox} Seasonal Decomposition\", y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot ACF and PACF\n",
    "        st.write(f\"## ACF and PACF Analysis for Series {series_id}\", fontsize=9)\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(6.5, 4))\n",
    "\n",
    "        # ACF plot\n",
    "        plot_acf(time_series.dropna(), lags=40, ax=ax[0])\n",
    "        ax[0].set_title(f\"Series {series_id}: ACF\", fontsize=8)  # Set title font size\n",
    "        ax[0].tick_params(\n",
    "            axis=\"both\", labelsize=7\n",
    "        )  # Adjust tick label size for ACF plot\n",
    "        ax[0].tick_params(axis=\"x\", labelsize=7)  # Set x-axis label font size for ACF\n",
    "        ax[0].tick_params(axis=\"y\", labelsize=7)  # Set y-axis label font size for ACF\n",
    "\n",
    "        # PACF plot\n",
    "        plot_pacf(time_series.dropna(), lags=40, ax=ax[1])\n",
    "        ax[1].set_title(f\"Series {series_id}: PACF\", fontsize=8)  # Set title font size\n",
    "        ax[1].tick_params(\n",
    "            axis=\"both\", labelsize=7\n",
    "        )  # Adjust tick label size for PACF plot\n",
    "        ax[1].tick_params(axis=\"x\", labelsize=7)  # Set x-axis label font size for PACF\n",
    "        ax[1].tick_params(axis=\"y\", labelsize=7)  # Set y-axis label font size for PACF\n",
    "\n",
    "        # Show the plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Perform Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(time_series.dropna())\n",
    "        st.write(f\"## Augmented Dickey-Fuller for Series {series_id}\")\n",
    "        st.write(f\"**ADF Statistic**: {adf_result[0]}\")\n",
    "        st.write(f\"**p-value**: {adf_result[1]}\")\n",
    "        st.write(\"**Critical Values**:\")\n",
    "        for key, value in adf_result[4].items():\n",
    "            st.write(f\"   **{key}**: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663c9ad-e4a8-4e5c-815c-25fb37e47788",
   "metadata": {
    "collapsed": false,
    "name": "baseline_overview",
    "resultHeight": 493
   },
   "source": [
    "# Perform Baseline Analysis - Optional\n",
    "\n",
    "If you would like to perform basic non-ML-based forecasting (baseline modeling) on your data which can be used as a comparison for advanced modeling later, the next cell provides for a simple forecasting method.\n",
    "- First you will pick which range of dates you want to forecast.  \n",
    "    - The code will automatically set this as test data and use the rest of the data for forecasting. \n",
    "    - Your range of options will be for dates within the most recent 12 months of your data. You should have at least two years of prior data available for forecasting.\n",
    "- The forecasting method used here is calculating the average of the target value from the same day of week (Sunday through Saturday) of the same week of year (1-52) from prior years.  For example, to predict 2024/12/02 data:\n",
    "    - December 2, 2024 is a Monday, so a day of week value of 1\n",
    "    - It falls into the week of year 49\n",
    "    - The same dates from 2022 and 2023 that are the day of week = 1 and week of year = 49 are:\n",
    "        - 2022: 2022/12/05\n",
    "        - 2023: 2023/12/04\n",
    "    - The forecast will find the target variable values for those dates and average them to predict target for 2024/12/02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3b1fa-0c4e-4618-ba7d-d9a667bd178b",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "baseline_analysis"
   },
   "outputs": [],
   "source": [
    "BASELINE_RUNTIME = datetime.now()\n",
    "\n",
    "data = data_sdf[\n",
    "    data_sdf[TIME_PERIOD_COLUMN] < pd.to_datetime(START_DATE_BASELINE_FORECAST)\n",
    "].copy()\n",
    "\n",
    "# Extract Time Related Features\n",
    "data[TIME_PERIOD_COLUMN] = pd.to_datetime(data[TIME_PERIOD_COLUMN])\n",
    "data[\"DAY_OF_WEEK\"] = data[TIME_PERIOD_COLUMN].dt.dayofweek  # Monday=0, Sunday=6\n",
    "data[\"WEEK_OF_YEAR\"] = data[TIME_PERIOD_COLUMN].dt.isocalendar().week\n",
    "data[\"YEAR\"] = data[TIME_PERIOD_COLUMN].dt.year\n",
    "data.sort_values(by=[\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN], inplace=True)\n",
    "\n",
    "aggregated_df = (\n",
    "    data.groupby([TIME_PERIOD_COLUMN, \"DAY_OF_WEEK\", \"WEEK_OF_YEAR\"])[TARGET_COLUMN]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")  # Aggregate by TIME_PERIOD_COLUMN for baseline modeling\n",
    "\n",
    "# Extract Time Related Features\n",
    "data_sdf[TIME_PERIOD_COLUMN] = pd.to_datetime(data_sdf[TIME_PERIOD_COLUMN])\n",
    "data_sdf[\"DAY_OF_WEEK\"] = data_sdf[\n",
    "    TIME_PERIOD_COLUMN\n",
    "].dt.dayofweek  # Monday=0, Sunday=6\n",
    "data_sdf[\"WEEK_OF_YEAR\"] = data_sdf[TIME_PERIOD_COLUMN].dt.isocalendar().week\n",
    "data_sdf[\"YEAR\"] = data_sdf[TIME_PERIOD_COLUMN].dt.year\n",
    "data_sdf.sort_values(by=[\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN], inplace=True)\n",
    "\n",
    "aggjoin_df = (\n",
    "    data_sdf.groupby([TIME_PERIOD_COLUMN, \"DAY_OF_WEEK\", \"WEEK_OF_YEAR\"])[TARGET_COLUMN]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")  # Aggregate by TIME_PERIOD_COLUMN for target to actual analysis\n",
    "\n",
    "\n",
    "# Forecast\n",
    "forecast_dates = pd.date_range(\n",
    "    start=START_DATE_BASELINE_FORECAST, end=END_DATE_BASELINE_FORECAST, freq=\"d\"\n",
    ")\n",
    "forecast_df = pd.DataFrame({TIME_PERIOD_COLUMN: forecast_dates})\n",
    "forecast_df[\"DAY_OF_WEEK\"] = forecast_df[TIME_PERIOD_COLUMN].dt.dayofweek\n",
    "forecast_df[\"WEEK_OF_YEAR\"] = forecast_df[TIME_PERIOD_COLUMN].dt.isocalendar().week\n",
    "\n",
    "# Set TARGET values to NaN for dates between START_DATE_BASELINE_FORECAST and END_DATE_BASELINE_FORECAST\n",
    "forecast_df.loc[\n",
    "    (data[TIME_PERIOD_COLUMN] >= pd.to_datetime(START_DATE_BASELINE_FORECAST))\n",
    "    & (forecast_df[TIME_PERIOD_COLUMN] <= pd.to_datetime(END_DATE_BASELINE_FORECAST)),\n",
    "    TARGET_COLUMN,\n",
    "] = None\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "forecast_results = []\n",
    "\n",
    "if MULTISERIES == 0:\n",
    "    # Add DAY_OF_WEEK and WEEK_OF_YEAR to the data before aggregation\n",
    "    data[\"DAY_OF_WEEK\"] = data[TIME_PERIOD_COLUMN].dt.dayofweek\n",
    "    data[\"WEEK_OF_YEAR\"] = data[TIME_PERIOD_COLUMN].dt.isocalendar().week\n",
    "\n",
    "    # Aggregate the data by TIME_PERIOD_COLUMN (no grouping by GROUP_IDENTIFIER)\n",
    "    aggregated_data = (\n",
    "        data.groupby([TIME_PERIOD_COLUMN, \"DAY_OF_WEEK\", \"WEEK_OF_YEAR\"])[TARGET_COLUMN]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )  # Aggregate by TIME_PERIOD_COLUMN\n",
    "\n",
    "    # Create the forecast for each day in December 2023\n",
    "    for _, forecast_row in forecast_df.iterrows():\n",
    "        # Find matching data for the same day of the week and week of the year\n",
    "        matching_data = aggregated_data[\n",
    "            (aggregated_data[\"DAY_OF_WEEK\"] == forecast_row[\"DAY_OF_WEEK\"])\n",
    "            & (aggregated_data[\"WEEK_OF_YEAR\"] == forecast_row[\"WEEK_OF_YEAR\"])\n",
    "        ]\n",
    "\n",
    "        # Compute the average traffic (TARGET) from prior years\n",
    "        if not matching_data.empty:\n",
    "            forecast_traffic = matching_data[TARGET_COLUMN].mean()\n",
    "        else:\n",
    "            forecast_traffic = None  # No historical data available\n",
    "\n",
    "        # Append the forecast result\n",
    "        forecast_results.append(\n",
    "            {\n",
    "                TIME_PERIOD_COLUMN: forecast_row[TIME_PERIOD_COLUMN],\n",
    "                \"FORECAST\": forecast_traffic,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Convert forecast results to a DataFrame\n",
    "    forecast_results_df = pd.DataFrame(forecast_results)\n",
    "    # st.write(data)\n",
    "    # st.write(forecast_results_df)\n",
    "    # st.write(aggregated_df)\n",
    "    # st.write(aggjoin_df)\n",
    "\n",
    "    # Merge the forecast results with data_sdf on TIME_PERIOD_COLUMN to get the TARGET value from data_sdf\n",
    "    merged_df = pd.merge(\n",
    "        forecast_results_df,\n",
    "        aggjoin_df[[TIME_PERIOD_COLUMN, TARGET_COLUMN]],\n",
    "        on=[TIME_PERIOD_COLUMN],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Select only the required columns: ORDER_TIMESTAMP (or TIME_PERIOD_COLUMN), FORECAST, and TARGET (TRAFFIC)\n",
    "    merged_df = merged_df[[TIME_PERIOD_COLUMN, \"FORECAST\", TARGET_COLUMN]]\n",
    "\n",
    "    st.write(merged_df)\n",
    "\n",
    "    # Create the plot for the entire dataset (since there's no grouping by GROUP_IDENTIFIER anymore)\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 3))\n",
    "\n",
    "    # Plotting FORECAST vs TARGET for the entire dataset\n",
    "    ax.plot(\n",
    "        merged_df[TIME_PERIOD_COLUMN],\n",
    "        merged_df[\"FORECAST\"],\n",
    "        label=\"Forecast\",\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        markersize=3,\n",
    "    )\n",
    "    ax.plot(\n",
    "        merged_df[TIME_PERIOD_COLUMN],\n",
    "        merged_df[TARGET_COLUMN],\n",
    "        label=\"Target\",\n",
    "        marker=\"x\",\n",
    "        linestyle=\"--\",\n",
    "        markersize=3,\n",
    "    )\n",
    "\n",
    "    # Set labels and title for the plot\n",
    "    ax.set_xlabel(\"Order Timestamp\", fontsize=9)\n",
    "    ax.set_ylabel(\"Value\", fontsize=9)\n",
    "    ax.set_title(\"Forecast vs Target\", fontsize=9)\n",
    "\n",
    "    # Rotate x-axis labels for better visibility\n",
    "    ax.tick_params(axis=\"x\", rotation=45, labelsize=8)\n",
    "    ax.tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "    # Set Y-axis minimum to 0\n",
    "    ax.set_ylim(0, merged_df[[\"FORECAST\", TARGET_COLUMN]].max().max())\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Process each group\n",
    "    for group_id in data[\"GROUP_IDENTIFIER\"].unique():\n",
    "        group_data = data[data[\"GROUP_IDENTIFIER\"] == group_id]\n",
    "\n",
    "        # Create the forecast for each day in December 2023\n",
    "        for _, forecast_row in forecast_df.iterrows():\n",
    "            # Find past occurrences for the same day of the week in the same week of the year\n",
    "            matching_data = group_data[\n",
    "                (group_data[\"DAY_OF_WEEK\"] == forecast_row[\"DAY_OF_WEEK\"])\n",
    "                & (group_data[\"WEEK_OF_YEAR\"] == forecast_row[\"WEEK_OF_YEAR\"])\n",
    "            ]\n",
    "\n",
    "            # Compute the average traffic from prior years\n",
    "            if not matching_data.empty:\n",
    "                forecast_traffic = matching_data[TARGET_COLUMN].mean()\n",
    "            else:\n",
    "                forecast_traffic = None  # No historical data available\n",
    "\n",
    "            # Append the forecast result\n",
    "            forecast_results.append(\n",
    "                {\n",
    "                    \"GROUP_IDENTIFIER\": group_id,\n",
    "                    TIME_PERIOD_COLUMN: forecast_row[TIME_PERIOD_COLUMN],\n",
    "                    \"FORECAST\": forecast_traffic,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Convert forecast results to a DataFrame\n",
    "    forecast_results_df = pd.DataFrame(forecast_results)\n",
    "\n",
    "    # Merge the forecast results with data_sdf on GROUP_IDENTIFIER and TIME_PERIOD_COLUMN to get the TARGET value from data_sdf\n",
    "    merged_df = pd.merge(\n",
    "        forecast_results_df,\n",
    "        data_sdf[[\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN, TARGET_COLUMN]],\n",
    "        on=[\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Select only the required columns: GROUP_IDENTIFIER, TIME_PERIOD_COLUMN (or TIME), FORECAST, and TARGET (TRAFFIC)\n",
    "    merged_df = merged_df[\n",
    "        [\"GROUP_IDENTIFIER\", TIME_PERIOD_COLUMN, \"FORECAST\", TARGET_COLUMN]\n",
    "    ]\n",
    "\n",
    "    st.write(merged_df)\n",
    "\n",
    "    # Get the number of unique GROUP_IDENTIFIERs to determine the number of subplots\n",
    "    num_groups = len(merged_df[\"GROUP_IDENTIFIER\"].unique())\n",
    "\n",
    "    # Create subplots (one for each GROUP_IDENTIFIER)\n",
    "    fig, axes = plt.subplots(nrows=num_groups, ncols=1, figsize=(7.5, 3 * num_groups))\n",
    "\n",
    "    # If there's only one group, axes will not be an array, so we make it an array for consistency\n",
    "    if num_groups == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Loop through each GROUP_IDENTIFIER and plot on the corresponding subplot\n",
    "    for i, group_id in enumerate(merged_df[\"GROUP_IDENTIFIER\"].unique()):\n",
    "        group_data = merged_df[merged_df[\"GROUP_IDENTIFIER\"] == group_id]\n",
    "\n",
    "        # Plotting FORECAST vs TARGET for the current GROUP_IDENTIFIER\n",
    "        axes[i].plot(\n",
    "            group_data[TIME_PERIOD_COLUMN],\n",
    "            group_data[\"FORECAST\"],\n",
    "            label=\"Forecast\",\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            markersize=3,\n",
    "        )\n",
    "        axes[i].plot(\n",
    "            group_data[TIME_PERIOD_COLUMN],\n",
    "            group_data[TARGET_COLUMN],\n",
    "            label=\"Target\",\n",
    "            marker=\"x\",\n",
    "            linestyle=\"--\",\n",
    "            markersize=3,\n",
    "        )\n",
    "\n",
    "        # Set labels and title for each subplot\n",
    "        axes[i].set_xlabel(\"Time Period\", fontsize=9)\n",
    "        axes[i].set_ylabel(\"Value\", fontsize=9)\n",
    "        axes[i].set_title(f\"Group {group_id} - Forecast vs Target\", fontsize=9)\n",
    "\n",
    "        # Rotate x-axis labels for better visibility\n",
    "        axes[i].tick_params(axis=\"x\", rotation=45, labelsize=8)\n",
    "        axes[i].tick_params(axis=\"y\", labelsize=8)\n",
    "\n",
    "        # Set Y-axis minimum to 0 for each subplot\n",
    "        axes[i].set_ylim(0, group_data[[\"FORECAST\", TARGET_COLUMN]].max().max())\n",
    "\n",
    "        # Add legend\n",
    "        axes[i].legend(fontsize=8)\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "st.success(\"You have successfully run your baseline analysis.\")\n",
    "\n",
    "# Calculate MAPE\n",
    "merged_df[\"APE\"] = (\n",
    "    abs((merged_df[TARGET_COLUMN] - merged_df[\"FORECAST\"]) / merged_df[TARGET_COLUMN])\n",
    "    * 100\n",
    ")\n",
    "merged_df[\"BASELINE_RUNTIME\"] = BASELINE_RUNTIME\n",
    "\n",
    "overall_mape = merged_df[\"APE\"].mean()\n",
    "print(f\"The Overall MAPE (Mean Average Percentage Error) is {overall_mape:.2f}%.\")\n",
    "\n",
    "merged_df = session.create_dataframe(merged_df)\n",
    "merged_df.write.save_as_table(\n",
    "    BASELINE_RESULT_TBL_NM, mode=\"overwrite\", comment=query_tag\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "kirk.mason@snowflake.com",
   "authorId": "340032177737",
   "authorName": "KMASON",
   "lastEditTime": 1741125767234,
   "notebookId": "wyfdsi654wtmiadppteb",
   "sessionId": "b8cc2b34-6339-4618-ada1-75d2c39bc621"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
