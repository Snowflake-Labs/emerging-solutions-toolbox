{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "INTRODUCTION"
   },
   "source": [
    "# Forecast Model Builder Deployment\n",
    "\n",
    "This notebook deploys the Forecast Model Builder files to your Snowflake account. \n",
    "\n",
    "The Forecast Model Builder is a tool for efficiently building forecasts.  It includes a collection of notebooks (exploratory data analysis, modeling, and inference) and an orchestration layer for iterating on multiple projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "INSTRUCTIONS"
   },
   "source": [
    "# Instructions\n",
    "\n",
    "__Refer to the [README](https://github.com/Snowflake-Labs/emerging-solutions-toolbox/blob/main/framework-forecast-model-builder/README.md) for detailed instructions.__ \n",
    "\n",
    "NOTE: This notebook creates several Snowflake objects. It is recommending to use the default settings in following python cell, if the user has privileges to CREATE DB and WH. Users without those privileges who specify an __existing__ database and schema below, should follw the README instruction option that matches their privilege level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "_1_USER_CONSTANTS"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# Solution constants\n",
    "# -----------------------------------------------------------------------------------------\n",
    "\n",
    "# Establish the name of the database and warehouse to be used for the solution\n",
    "# NOTE: User can specify an existing database and warehouse, or can specify a new database and warehouse to be created.\n",
    "# NOTE: If the following database and/or warehouse do not exist,\n",
    "#       this notebook will try to create them (assuming user's role has appropriate PRIVILEGES to create objects)\n",
    "SOLUTION_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "DEPLOYMENT_WH = \"FORECAST_MODEL_BUILDER_WH\"\n",
    "\n",
    "# Establish the name of the schema that will be used to store the base objects (like staged files, stored procedures, etc)\n",
    "# NOTE: If SOLUTION_DB specified an already-existing db, then user must specify a schema that ALREADY EXISTS in that db.\n",
    "#       If SOLUTION_DB specified a new db to be created, then user must specify the name of a new schema that will be created in the db.\n",
    "SOLUTION_BASE_SCHEMA = \"BASE\"\n",
    "\n",
    "# Establish the name of the stage to deploy the notebook templates into\n",
    "# NOTE: If SOLUTION_DB specified an already-existing db, then user must specify the name of a stage that ALREADY EXISTS in SOLUTION_BASE_SCHEMA.\n",
    "#       If SOLUTION_DB specified a new db to be created, then user must specify the name of a new stage that will be created in SOLUTION_BASE_SCHEMA.\n",
    "DEPLOYMENT_STAGE = \"NOTEBOOK_TEMPLATES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f73c6d-73f5-4664-bcef-3a233951ab25",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_2_DEPLOYMENT"
   },
   "outputs": [],
   "source": [
    "# Deploys the solution.  Creates a database if it doesn't yet\n",
    "# exist, and adds supporting schemas and stages, a default warehouse, and\n",
    "# will add a git repository to automatically load files.\n",
    "# Will check for needed permissions and will notify if missing.\n",
    "# *Note* - the role running the notebook will own the objects.\n",
    "\n",
    "# Import python packages\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Solution constant to specify which framework directory to deploy from the Emerging Solutions Toolbox\n",
    "TOOLBOX_FOLDER_NAME = \"framework-forecast-model-builder\"\n",
    "\n",
    "# Permission variables\n",
    "can_create_db = False\n",
    "can_create_wh = False\n",
    "can_create_integration = False\n",
    "\n",
    "# Deployment variables\n",
    "database_deployed = False\n",
    "warehouse_deployed = False\n",
    "files_deployed = False\n",
    "zip_deployed = False\n",
    "git_repository_deployed = False\n",
    "confirm_message_sent = False\n",
    "\n",
    "\n",
    "# Checks the permissions of the current role\n",
    "def check_permissions(session):\n",
    "    global can_create_db\n",
    "    global can_create_wh\n",
    "    global can_create_integration\n",
    "\n",
    "    # Checks permissions of current role\n",
    "    current_role_sql = \"\"\"SELECT CURRENT_ROLE()\"\"\"\n",
    "\n",
    "    current_role = session.sql(current_role_sql).collect()[0][0]\n",
    "\n",
    "    admin_role_list = [\"ACCOUNTADMIN\", \"SYSADMIN\"]\n",
    "\n",
    "    if current_role not in admin_role_list:\n",
    "        grants_sql = \"\"\"SHOW GRANTS ON ACCOUNT\"\"\"\n",
    "        grants_df = session.sql(grants_sql)\n",
    "\n",
    "        create_db_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE DATABASE\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_db_df.count() > 0:\n",
    "            can_create_db = True\n",
    "\n",
    "        create_wh_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE WAREHOUSE\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_wh_df.count() > 0:\n",
    "            can_create_wh = True\n",
    "\n",
    "        create_integration_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE INTEGRATION\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_integration_df.count() > 0:\n",
    "            can_create_integration = True\n",
    "\n",
    "    else:\n",
    "        can_create_db = True\n",
    "        can_create_wh = True\n",
    "        can_create_integration = True\n",
    "\n",
    "\n",
    "# Deploys the database and supporting objects\n",
    "def deploy_database(session):\n",
    "    global database_deployed\n",
    "\n",
    "    # Check if database exists\n",
    "    db_check_sql = f\"\"\"SHOW DATABASES LIKE '{SOLUTION_DB}'\"\"\"\n",
    "    db_check_df = session.sql(db_check_sql)\n",
    "\n",
    "    if db_check_df.count() == 0 and can_create_db:\n",
    "        try:\n",
    "            # Create a database for the toolkit\n",
    "            create_db_sql = f\"\"\"CREATE DATABASE IF NOT EXISTS {SOLUTION_DB}\n",
    "            COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_db_sql).collect()\n",
    "        except Exception as e:\n",
    "            st.warning(\n",
    "                f\"{SOLUTION_DB} database not available and could not be created.  Please change your role or reach out to an admin.  Error: \"\n",
    "                + str(e),\n",
    "                icon=\"⚠️\",\n",
    "            )\n",
    "        finally:\n",
    "            # Remove the public schema (only during initial deployment)\n",
    "            remove_public_schema_sql = f\"\"\"DROP SCHEMA IF EXISTS {SOLUTION_DB}.PUBLIC\"\"\"\n",
    "\n",
    "            session.sql(remove_public_schema_sql).collect()\n",
    "\n",
    "            # Create a schema for the toolkit itself (other schemas will be created for each project)\n",
    "            create_base_schema_sql = f\"\"\"CREATE SCHEMA IF NOT EXISTS {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_base_schema_sql).collect()\n",
    "\n",
    "            # Create a stage for the notebook templates used for creating new projects\n",
    "            create_notebook_stage_sql = f\"\"\"CREATE STAGE IF NOT EXISTS {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}\n",
    "        DIRECTORY = (ENABLE = TRUE)\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_notebook_stage_sql).collect()\n",
    "\n",
    "            database_deployed = True\n",
    "\n",
    "    elif db_check_df.count() == 0 and not can_create_db:\n",
    "        st.warning(\n",
    "            f\"{SOLUTION_DB} database not available and the current role does not have the CREATE DATBASE permission.  Please change your role or reach out to an admin.\",\n",
    "            icon=\"⚠️\",\n",
    "        )\n",
    "\n",
    "    elif db_check_df.count() == 1:\n",
    "        database_deployed = True\n",
    "\n",
    "\n",
    "# Deploys the warehouse\n",
    "def deploy_warehouse(session):\n",
    "    global warehouse_deployed\n",
    "\n",
    "    # Check if warehouse exists\n",
    "    wh_check_sql = f\"\"\"SHOW WAREHOUSES LIKE '{DEPLOYMENT_WH}'\"\"\"\n",
    "    wh_check_df = session.sql(wh_check_sql)\n",
    "\n",
    "    if wh_check_df.count() == 0 and can_create_wh:\n",
    "        # Create a warehouse for the toolkit\n",
    "        create_wh_sql = f\"\"\"CREATE WAREHOUSE IF NOT EXISTS {DEPLOYMENT_WH}\n",
    "    WITH WAREHOUSE_SIZE = 'XSMALL'\n",
    "    WAREHOUSE_TYPE = 'STANDARD'\n",
    "    AUTO_SUSPEND = 10\n",
    "    AUTO_RESUME = TRUE\n",
    "    INITIALLY_SUSPENDED = TRUE\n",
    "    COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "        session.sql(create_wh_sql).collect()\n",
    "\n",
    "        warehouse_deployed = True\n",
    "\n",
    "    elif wh_check_df.count() == 0 and not can_create_wh:\n",
    "        st.info(\n",
    "            f\"{DEPLOYMENT_WH} warehouse not available, either use your own or rerun with a role with the CREATE WAREHOUSE permission.\",\n",
    "            icon=\"ℹ️\",\n",
    "        )\n",
    "\n",
    "    elif wh_check_df.count() == 1:\n",
    "        warehouse_deployed = True\n",
    "\n",
    "\n",
    "# Checks if files in the stage IS missing\n",
    "def check_stage(session):\n",
    "    global files_deployed\n",
    "\n",
    "    # Check if files are missing\n",
    "    files_check_sql = f\"\"\"LS @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/\"\"\"\n",
    "    if session.sql(files_check_sql).count() > 0:\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Deploys the git repository\n",
    "def deploy_api_integration(session):\n",
    "    if database_deployed and not files_deployed:\n",
    "        check_git_repository(session)\n",
    "        check_for_zip(session)\n",
    "\n",
    "        if git_repository_deployed:\n",
    "            add_files_from_git(session)\n",
    "\n",
    "        elif zip_deployed:\n",
    "            add_files_from_zip(session)\n",
    "\n",
    "        else:\n",
    "            st.info(\n",
    "                f\"If you do not want to use git, you can manually upload the zip file from the repository to {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE} and rerun this cell.\",\n",
    "                icon=\"ℹ️\",\n",
    "            )\n",
    "\n",
    "            # Load git API integrations\n",
    "            api_integrations_sql = \"SHOW API INTEGRATIONS\"\n",
    "            api_integrations_df = session.sql(api_integrations_sql)\n",
    "\n",
    "            if api_integrations_df.count() == 0:\n",
    "                if can_create_integration:\n",
    "                    api_name = st.empty()\n",
    "                    api_button = st.empty()\n",
    "\n",
    "                    api_integration_name = api_name.text_input(\n",
    "                        \"Name your API Integration\",\n",
    "                        key=\"api_name\",\n",
    "                        value=\"SNOWFLAKE_LABS_GIT_API_INTEGRATION\",\n",
    "                    )\n",
    "                    if api_button.button(\"Create Integration\", key=\"create_int_btn\"):\n",
    "                        api_integration_create_sql = f\"\"\"CREATE OR REPLACE API INTEGRATION {api_integration_name}\n",
    "        API_PROVIDER = git_https_api\n",
    "        API_ALLOWED_PREFIXES = ('https://github.com/Snowflake-Labs/')\n",
    "        ENABLED = TRUE;\"\"\"\n",
    "                        session.sql(api_integration_create_sql).collect()\n",
    "                        api_name.empty()\n",
    "                        api_button.empty()\n",
    "                        deploy_git(session)\n",
    "\n",
    "                else:\n",
    "                    st.warning(\n",
    "                        \"There are no API integrations and the current role does not have permission to create one or contact your admin or manually stage the files.\",\n",
    "                        icon=\"⚠️\",\n",
    "                    )\n",
    "            else:\n",
    "                deploy_git(session)\n",
    "    elif database_deployed and files_deployed:\n",
    "        # Check if git is added for status\n",
    "        check_git_repository(session)\n",
    "        check_for_zip(session)\n",
    "\n",
    "\n",
    "# Adds a git repository to the database\n",
    "def deploy_git(session):\n",
    "    global files_deployed\n",
    "    global confirm_message_sent\n",
    "    global git_repository_deployed\n",
    "\n",
    "    # Load git API integrations\n",
    "    api_integrations_sql = \"SHOW API INTEGRATIONS\"\n",
    "    api_integrations_df = session.sql(api_integrations_sql)\n",
    "\n",
    "    git_integrations = []\n",
    "\n",
    "    for row in api_integrations_df.collect():\n",
    "        api_integration_description_sql = (\n",
    "            f\"\"\"DESCRIBE API INTEGRATION \\\"{row[\"name\"]}\\\"\"\"\"\n",
    "        )\n",
    "        api_integration_description_df = session.sql(\n",
    "            api_integration_description_sql\n",
    "        ).filter(col('\"property_value\"') == \"GIT_HTTPS_API\")\n",
    "\n",
    "        if api_integration_description_df.count() > 0:\n",
    "            git_integrations.append(row[\"name\"])\n",
    "\n",
    "    # Create repo if not exists\n",
    "    api_select = st.empty()\n",
    "\n",
    "    selected_api_integration = api_select.selectbox(\n",
    "        \"Select an API Integration\",\n",
    "        options=git_integrations,\n",
    "        help=\"If none of these work, please contact your admin.\",\n",
    "    )\n",
    "\n",
    "    repo_button = st.empty()\n",
    "    if repo_button.button(\"Create Git Repository\", key=\"create_git_repo_btn\"):\n",
    "        try:\n",
    "            repo_sql = f\"\"\"CREATE GIT REPOSITORY IF NOT EXISTS {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.EMERGING_SOLUTIONS_TOOLBOX\n",
    "API_INTEGRATION = \"{selected_api_integration}\"\n",
    "ORIGIN = 'https://github.com/Snowflake-Labs/emerging-solutions-toolbox.git'\"\"\"\n",
    "\n",
    "            session.sql(repo_sql).collect()\n",
    "            add_files_from_git(session)\n",
    "            api_select.empty()\n",
    "            repo_button.empty()\n",
    "            git_repository_deployed = True\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(\n",
    "                \"Could not create repository.  Try another API Integration or contact an admin.\\n  Error: \"\n",
    "                + str(e),\n",
    "                icon=\"⚠️\",\n",
    "            )\n",
    "\n",
    "    confirmation_message()\n",
    "\n",
    "\n",
    "# Writes a confirmation message if not already written\n",
    "def confirmation_message():\n",
    "    global confirm_message_sent\n",
    "\n",
    "    if database_deployed and files_deployed:\n",
    "        if not confirm_message_sent:\n",
    "            status_df = pd.DataFrame(\n",
    "                [\n",
    "                    [\"Database Deployed\", database_deployed],\n",
    "                    [\"Warehouse Deployed\", warehouse_deployed],\n",
    "                    [\n",
    "                        \"Git Repository or Zip Deployed\",\n",
    "                        git_repository_deployed or zip_deployed,\n",
    "                    ],\n",
    "                    [\"Files Deployed\", files_deployed],\n",
    "                ],\n",
    "                columns=[\"Step\", \"Complete\"],\n",
    "            )\n",
    "\n",
    "            st.write(status_df)\n",
    "\n",
    "            st.success(\n",
    "                \"Solution from \" + str(TOOLBOX_FOLDER_NAME) + \" fully deployed!\",\n",
    "                icon=\"✅\",\n",
    "            )\n",
    "            confirm_message_sent = True\n",
    "\n",
    "\n",
    "# Checks if the git repository has been added\n",
    "def check_git_repository(session):\n",
    "    global git_repository_deployed\n",
    "\n",
    "    # Check for git repository\n",
    "    git_repository_sql = f\"\"\"SHOW GIT REPOSITORIES LIKE 'EMERGING_SOLUTIONS_TOOLBOX' IN DATABASE {SOLUTION_DB}\"\"\"\n",
    "    git_repository_df = session.sql(git_repository_sql)\n",
    "\n",
    "    if git_repository_df.count() > 0:\n",
    "        git_repository_deployed = True\n",
    "\n",
    "\n",
    "# Checks if the zip file has been uploaded\n",
    "def check_for_zip(session):\n",
    "    global zip_deployed\n",
    "\n",
    "    # Check for zip file\n",
    "    zip_check_sql = f\"\"\"LS @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE} PATTERN='.*emerging-solutions-toolbox-main.zip'\"\"\"\n",
    "    zip_check_df = session.sql(zip_check_sql)\n",
    "\n",
    "    if zip_check_df.count() > 0:\n",
    "        zip_deployed = True\n",
    "\n",
    "\n",
    "# Adds the notebooks and python files to the stage from the git repository\n",
    "def add_files_from_git(session):\n",
    "    global files_deployed\n",
    "\n",
    "    if not files_deployed:\n",
    "        copy_notebooks_sql = f\"\"\"COPY FILES\n",
    "INTO @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/\n",
    "FROM @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.EMERGING_SOLUTIONS_TOOLBOX/branches/main/\"\"\"\n",
    "\n",
    "        session.sql(copy_notebooks_sql).collect()\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Adds the notebooks and python files to the stage from a zip file\n",
    "def add_files_from_zip(session):\n",
    "    global files_deployed\n",
    "\n",
    "    if not files_deployed:\n",
    "        f = session.file.get_stream(\n",
    "            f\"@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main.zip\"\n",
    "        )\n",
    "        with zipfile.ZipFile(f, \"r\") as git_zip:\n",
    "            git_zip.extractall()\n",
    "\n",
    "        path_list = [\n",
    "            os.path.join(dirpath, f)\n",
    "            for (dirpath, dirnames, filenames) in os.walk(\n",
    "                \"emerging-solutions-toolbox-main\"\n",
    "            )\n",
    "            for f in filenames\n",
    "        ]\n",
    "\n",
    "        for path in path_list:\n",
    "            directory, file_name = os.path.split(path)\n",
    "\n",
    "            put_result = session.file.put(\n",
    "                path,\n",
    "                f\"@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/{directory}\",\n",
    "                auto_compress=False,\n",
    "            )\n",
    "\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Deploys code specific for the solution\n",
    "def deploy_solution_specific_code(session):\n",
    "    if files_deployed:\n",
    "        # Deploy sample data\n",
    "        sample_data_deployed = False\n",
    "\n",
    "        sample_data_check_sql = f\"\"\"SHOW TABLES LIKE 'DAILY_PARTITIONED_SAMPLE_DATA' IN SCHEMA {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}\"\"\"\n",
    "        if session.sql(sample_data_check_sql).count() > 0:\n",
    "            sample_data_deployed = True\n",
    "\n",
    "        if not sample_data_deployed:\n",
    "            create_file_format_sql = f\"\"\"CREATE OR REPLACE FILE FORMAT {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.CSV_FORMAT \n",
    "        TYPE = CSV\n",
    "        FIELD_DELIMITER = ','\n",
    "        PARSE_HEADER = TRUE\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_file_format_sql).collect()\n",
    "\n",
    "            create_table_sql = f\"\"\"CREATE OR REPLACE TABLE {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.DAILY_PARTITIONED_SAMPLE_DATA USING TEMPLATE (\n",
    "    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \n",
    "     FROM TABLE (INFER_SCHEMA(\n",
    "     LOCATION=>'@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/sample_data/daily_partitioned_sample_data.csv',\n",
    "     FILE_FORMAT=>'{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.CSV_FORMAT')))\"\"\"\n",
    "            session.sql(create_table_sql).collect()\n",
    "\n",
    "            tag_table_sql = f\"\"\"ALTER TABLE {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.DAILY_PARTITIONED_SAMPLE_DATA SET COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(tag_table_sql).collect()\n",
    "\n",
    "            load_table_sql = f\"\"\"COPY INTO {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.DAILY_PARTITIONED_SAMPLE_DATA\n",
    "    from '@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/sample_data/daily_partitioned_sample_data.csv'\n",
    "    FILE_FORMAT = (SKIP_HEADER = 1)\"\"\"\n",
    "\n",
    "            session.sql(load_table_sql).collect()\n",
    "\n",
    "        # Check for notebooks - don't overwrite if present\n",
    "        notebooks_deployed = False\n",
    "\n",
    "        notebook_check_sql = f\"\"\"LS @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS PATTERN='.*.ipynb'\"\"\"\n",
    "        if session.sql(notebook_check_sql).count() > 0:\n",
    "            notebooks_deployed = True\n",
    "\n",
    "        # Copy notebooks and python library files to the right spots\n",
    "        if not notebooks_deployed:\n",
    "            copy_notebooks_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS/\n",
    "    FROM @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/\n",
    "    PATTERN = '.*.ipynb'\"\"\"\n",
    "\n",
    "            session.sql(copy_notebooks_sql).collect()\n",
    "\n",
    "            copy_enivronment_yml_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS/\n",
    "    FROM @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/\n",
    "    FILES = ('environment.yml')\"\"\"\n",
    "\n",
    "            session.sql(copy_enivronment_yml_sql).collect()\n",
    "\n",
    "            copy_libraries_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS/forecast_model_builder/\n",
    "    FROM @{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/forecast_model_builder/\n",
    "    PATTERN = '.*.py'\"\"\"\n",
    "\n",
    "            session.sql(copy_libraries_sql).collect()\n",
    "\n",
    "        # Create a stored procedure for creating solution projects\n",
    "        sp_deploy_sql = f'''CREATE OR REPLACE PROCEDURE {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.CREATE_PROJECT(project_name VARCHAR, warehouse VARCHAR)\n",
    "    RETURNS VARCHAR\n",
    "    LANGUAGE PYTHON\n",
    "    RUNTIME_VERSION = '3.11'\n",
    "    HANDLER = 'create_project'\n",
    "    PACKAGES = ('snowflake-snowpark-python')\n",
    "    COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\n",
    "    EXECUTE AS CALLER\n",
    "    AS\n",
    "$$\n",
    "def create_project(session, project_name, warehouse):\n",
    "    create_schema_sql = f\"\"\"CREATE SCHEMA IF NOT EXISTS {SOLUTION_DB}.{{project_name}}\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_eda_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_DB}.{{project_name}}.{{project_name}}__EDA\n",
    "    FROM '@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'eda.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_modeling_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_DB}.{{project_name}}.{{project_name}}__MODELING\n",
    "    FROM '@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'modeling.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_inference_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_DB}.{{project_name}}.{{project_name}}__INFERENCE\n",
    "    FROM '@{SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'inference.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    session.sql(create_schema_sql).collect()\n",
    "    session.sql(create_eda_notebook_sql).collect()\n",
    "    session.sql(create_modeling_notebook_sql).collect()\n",
    "    session.sql(create_inference_notebook_sql).collect()\n",
    "\n",
    "    return f\"\"\"Project created\"\"\"\n",
    "$$;'''\n",
    "\n",
    "        session.sql(sp_deploy_sql).collect()\n",
    "\n",
    "\n",
    "check_permissions(session)\n",
    "deploy_database(session)\n",
    "deploy_warehouse(session)\n",
    "check_stage(session)\n",
    "deploy_api_integration(session)\n",
    "deploy_solution_specific_code(session)\n",
    "confirmation_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70618532-cec0-4411-b67c-db4fdb83f305",
   "metadata": {
    "collapsed": false,
    "name": "PROJECTS"
   },
   "source": [
    "**Project Creation**\n",
    "\n",
    "The next step creates a *project*.  A project is new schema with independent set of notebooks and python files, all generated from the base templates. \n",
    " Projects allow for easy isolation of work, while allowing admins to set defaults on the base stages.\n",
    "\n",
    "- The notebooks are indepenent copies from the base notebook templates\n",
    "- The supporting python files are copied for each notebook\n",
    "    - This allows for immediate use of the files without using the imported custom packages UI\n",
    "    - It does mean that the python files are *independent* copies for each notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfb62-6373-44db-a426-48c86d7ab6ce",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_3_PROJECT_DEPLOY"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Set default value\n",
    "if \"project_name_txt\" not in st.session_state:\n",
    "    st.session_state.project_name_txt = \"\"\n",
    "\n",
    "\n",
    "def create_project(project_name):\n",
    "    if project_name != SOLUTION_BASE_SCHEMA:\n",
    "        with st.spinner():\n",
    "            session.sql(sql_script).collect()\n",
    "\n",
    "\n",
    "st.header(\"New Project\")\n",
    "st.write(\"Each project gets its own schema and set of notebooks\")\n",
    "project_name = st.text_input(\"Set your project name\", key=\"project_name_txt\")\n",
    "\n",
    "project_name = project_name.upper().replace(\" \", \"_\")\n",
    "\n",
    "warehouse_check_sql = f\"\"\"SHOW WAREHOUSES LIKE '{DEPLOYMENT_WH}'\"\"\"\n",
    "warehouse_df = session.sql(warehouse_check_sql)\n",
    "\n",
    "if warehouse_df.count() > 0:\n",
    "    warehouse = DEPLOYMENT_WH\n",
    "else:\n",
    "    warehouse_sql = \"\"\"SELECT CURRENT_WAREHOUSE()\"\"\"\n",
    "    warehouse = session.sql(warehouse_sql).collect()[0][0]\n",
    "\n",
    "sql_script = f\"\"\"CALL {SOLUTION_DB}.{SOLUTION_BASE_SCHEMA}.CREATE_PROJECT('{project_name}', '{warehouse}')\"\"\"\n",
    "\n",
    "create_col, delete_col = st.columns([6.2, 1])\n",
    "\n",
    "with create_col:\n",
    "    create_btn = st.button(\n",
    "        \"Create\", key=\"project_create_btn\", on_click=create_project, args={project_name}\n",
    "    )\n",
    "\n",
    "\n",
    "with delete_col:\n",
    "    if st.button(\"Delete\", key=\"project_delete_btn\"):\n",
    "        if project_name != SOLUTION_BASE_SCHEMA:\n",
    "            with st.spinner():\n",
    "                delete_sql = f\"\"\"DROP SCHEMA {SOLUTION_DB}.{project_name} CASCADE\"\"\"\n",
    "                session.sql(delete_sql).collect()\n",
    "\n",
    "\n",
    "projects_sql = f\"\"\"SHOW SCHEMAS IN DATABASE {SOLUTION_DB}\"\"\"\n",
    "projects_df = session.sql(projects_sql)\n",
    "\n",
    "project_list = []\n",
    "\n",
    "for row in projects_df.to_local_iterator():\n",
    "    if row[\"name\"] not in [SOLUTION_BASE_SCHEMA, \"INFORMATION_SCHEMA\"]:\n",
    "        project_list.append(row[\"name\"])\n",
    "\n",
    "st.subheader(\"Current Projects\")\n",
    "st.dataframe(project_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f535428-5e4f-4b3a-96e0-082a040e855b",
   "metadata": {
    "collapsed": false,
    "name": "NEXT_STEPS"
   },
   "source": [
    "**Next Steps**\n",
    "\n",
    "Once you've created your project, go to your Notebooks and start with <your project name>__EDA"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "kirk.mason@snowflake.com",
   "authorId": "340032177737",
   "authorName": "KMASON",
   "lastEditTime": 1742323896995,
   "notebookId": "jo4uqeucjfgab2qkviia",
   "sessionId": "bce10524-fa52-4436-a4ff-14f4713be06f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
