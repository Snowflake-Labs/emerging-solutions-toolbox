{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "INTRODUCTION"
   },
   "source": "**Forecasting Tookit**\n\nThe Forecasting Tookit is a tool for efficiently building forecasts.  It includes a collection of notebooks (exploratory data analysis, modeling, and inference) and an orchestration layer for iterating on multiple projects.\n\nTo start, run each cell one at a time, and follow the directions from the cell output.  You have the option to either use a git integration to pull from the [Emerging Solutions Toolbox](https://github.com/Snowflake-Labs/emerging-solutions-toolbox) or to upload a zipped copy of it to the created stage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f73c6d-73f5-4664-bcef-3a233951ab25",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "DEPLOYMENT"
   },
   "outputs": [],
   "source": [
    "# Deploys the solution.  Creates a database if it doesn't yet\n",
    "# exist, and adds supporting schemas and stages, a default warehouse, and\n",
    "# will add a git repository to automatically load files.\n",
    "# Will check for needed permissions and will notify if missing.\n",
    "# *Note* - the role running the notebook will own the objects.\n",
    "\n",
    "# Import python packages\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Solution constants\n",
    "SOLUTION_NAME = \"FORECAST_MODEL_BUILDER\"\n",
    "TOOLBOX_FOLDER_NAME = \"framework-forecast-model-builder\"\n",
    "DEPLOYMENT_STAGE = \"NOTEBOOK_TEMPLATES\"\n",
    "\n",
    "# Permission variables\n",
    "can_create_db = False\n",
    "can_create_wh = False\n",
    "can_create_integration = False\n",
    "\n",
    "# Deployment variables\n",
    "database_deployed = False\n",
    "warehouse_deployed = False\n",
    "files_deployed = False\n",
    "zip_deployed = False\n",
    "git_repository_deployed = False\n",
    "confirm_message_sent = False\n",
    "\n",
    "\n",
    "# Checks the permissions of the current role\n",
    "def check_permissions(session):\n",
    "    global can_create_db\n",
    "    global can_create_wh\n",
    "    global can_create_integration\n",
    "\n",
    "    # Checks permissions of current role\n",
    "    current_role_sql = \"\"\"SELECT CURRENT_ROLE()\"\"\"\n",
    "\n",
    "    current_role = session.sql(current_role_sql).collect()[0][0]\n",
    "\n",
    "    admin_role_list = [\"ACCOUNTADMIN\", \"SYSADMIN\"]\n",
    "\n",
    "    if current_role not in admin_role_list:\n",
    "        grants_sql = \"\"\"SHOW GRANTS ON ACCOUNT\"\"\"\n",
    "        grants_df = session.sql(grants_sql)\n",
    "\n",
    "        create_db_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE DATABASE\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_db_df.count() > 0:\n",
    "            can_create_db = True\n",
    "\n",
    "        create_wh_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE WAREHOUSE\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_wh_df.count() > 0:\n",
    "            can_create_wh = True\n",
    "\n",
    "        create_integration_df = grants_df.filter(\n",
    "            (col('\"privilege\"') == \"CREATE INTEGRATION\")\n",
    "            & (col('\"grantee_name\"') == current_role)\n",
    "        )\n",
    "\n",
    "        if create_integration_df.count() > 0:\n",
    "            can_create_integration = True\n",
    "\n",
    "    else:\n",
    "        can_create_db = True\n",
    "        can_create_wh = True\n",
    "        can_create_integration = True\n",
    "\n",
    "\n",
    "# Deploys the database and supporting objects\n",
    "def deploy_database(session):\n",
    "    global database_deployed\n",
    "\n",
    "    # Check if database exists\n",
    "    db_check_sql = f\"\"\"SHOW DATABASES LIKE '{SOLUTION_NAME}'\"\"\"\n",
    "    db_check_df = session.sql(db_check_sql)\n",
    "\n",
    "    if db_check_df.count() == 0 and can_create_db:\n",
    "        try:\n",
    "            # Create a database for the toolkit\n",
    "            create_db_sql = f\"\"\"CREATE DATABASE IF NOT EXISTS {SOLUTION_NAME}\n",
    "            COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_db_sql).collect()\n",
    "        except Exception as e:\n",
    "            st.warning(\n",
    "                f\"{SOLUTION_NAME} database not available and could not be created.  Please change your role or reach out to an admin.  Error: \"\n",
    "                + str(e),\n",
    "                icon=\"⚠️\",\n",
    "            )\n",
    "        finally:\n",
    "            # Remove the public schema (only during initial deployment)\n",
    "            remove_public_schema_sql = (\n",
    "                f\"\"\"DROP SCHEMA IF EXISTS {SOLUTION_NAME}.PUBLIC\"\"\"\n",
    "            )\n",
    "\n",
    "            session.sql(remove_public_schema_sql).collect()\n",
    "\n",
    "            # Create a schema for the toolkit itself (other schemas will be created for each project)\n",
    "            create_base_schema_sql = f\"\"\"CREATE SCHEMA IF NOT EXISTS {SOLUTION_NAME}.BASE\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_base_schema_sql).collect()\n",
    "\n",
    "            # Create a stage for the notebook templates used for creating new projects\n",
    "            create_notebook_stage_sql = f\"\"\"CREATE STAGE IF NOT EXISTS {SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}\n",
    "        DIRECTORY = (ENABLE = TRUE)\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_notebook_stage_sql).collect()\n",
    "\n",
    "            database_deployed = True\n",
    "\n",
    "    elif db_check_df.count() == 0 and not can_create_db:\n",
    "        st.warning(\n",
    "            f\"{SOLUTION_NAME} database not available and the current role does not have the CREATE DATBASE permission.  Please change your role or reach out to an admin.\",\n",
    "            icon=\"⚠️\",\n",
    "        )\n",
    "\n",
    "    elif db_check_df.count() == 1:\n",
    "        database_deployed = True\n",
    "\n",
    "\n",
    "# Deploys the warehouse\n",
    "def deploy_warehouse(session):\n",
    "    global warehouse_deployed\n",
    "\n",
    "    # Check if warehouse exists\n",
    "    wh_check_sql = f\"\"\"SHOW WAREHOUSES LIKE '{SOLUTION_NAME}_WH'\"\"\"\n",
    "    wh_check_df = session.sql(wh_check_sql)\n",
    "\n",
    "    if wh_check_df.count() == 0 and can_create_wh:\n",
    "        # Create a warehouse for the toolkit\n",
    "        create_wh_sql = f\"\"\"CREATE WAREHOUSE IF NOT EXISTS {SOLUTION_NAME}_WH\n",
    "    WITH WAREHOUSE_SIZE = 'XSMALL'\n",
    "    WAREHOUSE_TYPE = 'STANDARD'\n",
    "    AUTO_SUSPEND = 10\n",
    "    AUTO_RESUME = TRUE\n",
    "    INITIALLY_SUSPENDED = TRUE\n",
    "    COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "        session.sql(create_wh_sql).collect()\n",
    "\n",
    "        warehouse_deployed = True\n",
    "\n",
    "    elif wh_check_df.count() == 0 and not can_create_wh:\n",
    "        st.info(\n",
    "            f\"{SOLUTION_NAME}_WH warehouse not available, either use your own or rerun with a role with the CREATE WAREHOUSE permission.\",\n",
    "            icon=\"ℹ️\",\n",
    "        )\n",
    "\n",
    "    elif wh_check_df.count() == 1:\n",
    "        warehouse_deployed = True\n",
    "\n",
    "\n",
    "# Checks if files in the stage IS missing\n",
    "def check_stage(session):\n",
    "    global files_deployed\n",
    "\n",
    "    # Check if files are missing\n",
    "    files_check_sql = f\"\"\"LS @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/\"\"\"\n",
    "    if session.sql(files_check_sql).count() > 0:\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Deploys the git repository\n",
    "def deploy_api_integration(session):\n",
    "    if database_deployed and not files_deployed:\n",
    "        check_git_repository(session)\n",
    "        check_for_zip(session)\n",
    "\n",
    "        if git_repository_deployed:\n",
    "            add_files_from_git(session)\n",
    "\n",
    "        elif zip_deployed:\n",
    "            add_files_from_zip(session)\n",
    "\n",
    "        else:\n",
    "            st.info(\n",
    "                f\"If you do not want to use git, you can manually upload the zip file from the repository to {SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE} and rerun this cell.\",\n",
    "                icon=\"ℹ️\",\n",
    "            )\n",
    "\n",
    "            # Load git API integrations\n",
    "            api_integrations_sql = \"SHOW API INTEGRATIONS\"\n",
    "            api_integrations_df = session.sql(api_integrations_sql)\n",
    "\n",
    "            if api_integrations_df.count() == 0:\n",
    "                if can_create_integration:\n",
    "                    api_name = st.empty()\n",
    "                    api_button = st.empty()\n",
    "\n",
    "                    api_integration_name = api_name.text_input(\n",
    "                        \"Name your API Integration\",\n",
    "                        key=\"api_name\",\n",
    "                        value=\"SNOWFLAKE_LABS_GIT_API_INTEGRATION\",\n",
    "                    )\n",
    "                    if api_button.button(\"Create Integration\", key=\"create_int_btn\"):\n",
    "                        api_integration_create_sql = f\"\"\"CREATE OR REPLACE API INTEGRATION {api_integration_name}\n",
    "        API_PROVIDER = git_https_api\n",
    "        API_ALLOWED_PREFIXES = ('https://github.com/Snowflake-Labs/')\n",
    "        ENABLED = TRUE;\"\"\"\n",
    "                        session.sql(api_integration_create_sql).collect()\n",
    "                        api_name.empty()\n",
    "                        api_button.empty()\n",
    "                        deploy_git(session)\n",
    "\n",
    "                else:\n",
    "                    st.warning(\n",
    "                        \"There are no API integrations and the current role does not have permission to create one or contact your admin or manually stage the files.\",\n",
    "                        icon=\"⚠️\",\n",
    "                    )\n",
    "            else:\n",
    "                deploy_git(session)\n",
    "    elif database_deployed and files_deployed:\n",
    "        # Check if git is added for status\n",
    "        check_git_repository(session)\n",
    "        check_for_zip(session)\n",
    "\n",
    "\n",
    "# Adds a git repository to the database\n",
    "def deploy_git(session):\n",
    "    global files_deployed\n",
    "    global confirm_message_sent\n",
    "    global git_repository_deployed\n",
    "\n",
    "    # Load git API integrations\n",
    "    api_integrations_sql = \"SHOW API INTEGRATIONS\"\n",
    "    api_integrations_df = session.sql(api_integrations_sql)\n",
    "\n",
    "    git_integrations = []\n",
    "\n",
    "    for row in api_integrations_df.collect():\n",
    "        api_integration_description_sql = (\n",
    "            f\"\"\"DESCRIBE API INTEGRATION \\\"{row[\"name\"]}\\\"\"\"\"\n",
    "        )\n",
    "        api_integration_description_df = session.sql(\n",
    "            api_integration_description_sql\n",
    "        ).filter(col('\"property_value\"') == \"GIT_HTTPS_API\")\n",
    "\n",
    "        if api_integration_description_df.count() > 0:\n",
    "            git_integrations.append(row[\"name\"])\n",
    "\n",
    "    # Create repo if not exists\n",
    "    api_select = st.empty()\n",
    "\n",
    "    selected_api_integration = api_select.selectbox(\n",
    "        \"Select an API Integration\",\n",
    "        options=git_integrations,\n",
    "        help=\"If none of these work, please contact your admin.\",\n",
    "    )\n",
    "\n",
    "    repo_button = st.empty()\n",
    "    if repo_button.button(\"Create Git Repository\", key=\"create_git_repo_btn\"):\n",
    "        try:\n",
    "            repo_sql = f\"\"\"CREATE GIT REPOSITORY IF NOT EXISTS {SOLUTION_NAME}.BASE.EMERGING_SOLUTIONS_TOOLBOX\n",
    "API_INTEGRATION = \"{selected_api_integration}\"\n",
    "ORIGIN = 'https://github.com/Snowflake-Labs/emerging-solutions-toolbox.git'\"\"\"\n",
    "\n",
    "            session.sql(repo_sql).collect()\n",
    "            add_files_from_git(session)\n",
    "            api_select.empty()\n",
    "            repo_button.empty()\n",
    "            git_repository_deployed = True\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(\n",
    "                \"Could not create repository.  Try another API Integration or contact an admin.\\n  Error: \"\n",
    "                + str(e),\n",
    "                icon=\"⚠️\",\n",
    "            )\n",
    "\n",
    "    confirmation_message()\n",
    "\n",
    "\n",
    "# Writes a confirmation message if not already written\n",
    "def confirmation_message():\n",
    "    global confirm_message_sent\n",
    "\n",
    "    if database_deployed and files_deployed:\n",
    "        if not confirm_message_sent:\n",
    "            status_df = pd.DataFrame(\n",
    "                [\n",
    "                    [\"Database Deployed\", database_deployed],\n",
    "                    [\"Warehouse Deployed\", warehouse_deployed],\n",
    "                    [\n",
    "                        \"Git Repository or Zip Deployed\",\n",
    "                        git_repository_deployed or zip_deployed,\n",
    "                    ],\n",
    "                    [\"Files Deployed\", files_deployed],\n",
    "                ],\n",
    "                columns=[\"Step\", \"Complete\"],\n",
    "            )\n",
    "\n",
    "            st.write(status_df)\n",
    "\n",
    "            st.success(str(SOLUTION_NAME) + \" fully deployed!\", icon=\"✅\")\n",
    "            confirm_message_sent = True\n",
    "\n",
    "\n",
    "# Checks if the git repository has been added\n",
    "def check_git_repository(session):\n",
    "    global git_repository_deployed\n",
    "\n",
    "    # Check for git repository\n",
    "    git_repository_sql = f\"\"\"SHOW GIT REPOSITORIES LIKE 'EMERGING_SOLUTIONS_TOOLBOX' IN DATABASE {SOLUTION_NAME}\"\"\"\n",
    "    git_repository_df = session.sql(git_repository_sql)\n",
    "\n",
    "    if git_repository_df.count() > 0:\n",
    "        git_repository_deployed = True\n",
    "\n",
    "\n",
    "# Checks if the zip file has been uploaded\n",
    "def check_for_zip(session):\n",
    "    global zip_deployed\n",
    "\n",
    "    # Check for zip file\n",
    "    zip_check_sql = f\"\"\"LS @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE} PATTERN='.*emerging-solutions-toolbox-main.zip'\"\"\"\n",
    "    zip_check_df = session.sql(zip_check_sql)\n",
    "\n",
    "    if zip_check_df.count() > 0:\n",
    "        zip_deployed = True\n",
    "\n",
    "\n",
    "# Adds the notebooks and python files to the stage from the git repository\n",
    "def add_files_from_git(session):\n",
    "    global files_deployed\n",
    "\n",
    "    if not files_deployed:\n",
    "        copy_notebooks_sql = f\"\"\"COPY FILES\n",
    "INTO @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/\n",
    "FROM @{SOLUTION_NAME}.BASE.EMERGING_SOLUTIONS_TOOLBOX/branches/main/\"\"\"\n",
    "\n",
    "        session.sql(copy_notebooks_sql).collect()\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Adds the notebooks and python files to the stage from a zip file\n",
    "def add_files_from_zip(session):\n",
    "    global files_deployed\n",
    "\n",
    "    if not files_deployed:\n",
    "        f = session.file.get_stream(\n",
    "            f\"@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main.zip\"\n",
    "        )\n",
    "        with zipfile.ZipFile(f, \"r\") as git_zip:\n",
    "            git_zip.extractall()\n",
    "\n",
    "        path_list = [\n",
    "            os.path.join(dirpath, f)\n",
    "            for (dirpath, dirnames, filenames) in os.walk(\n",
    "                \"emerging-solutions-toolbox-main\"\n",
    "            )\n",
    "            for f in filenames\n",
    "        ]\n",
    "\n",
    "        for path in path_list:\n",
    "            directory, file_name = os.path.split(path)\n",
    "\n",
    "            put_result = session.file.put(\n",
    "                path,\n",
    "                f\"@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/{directory}\",\n",
    "                auto_compress=False,\n",
    "            )\n",
    "\n",
    "        files_deployed = True\n",
    "\n",
    "\n",
    "# Deploys code specific for the solution\n",
    "def deploy_solution_specific_code(session):\n",
    "    if files_deployed:\n",
    "        # Deploy sample data\n",
    "        sample_data_deployed = False\n",
    "\n",
    "        sample_data_check_sql = f\"\"\"SHOW TABLES LIKE 'DAILY_PARTITIONED_SAMPLE_DATA' IN SCHEMA {SOLUTION_NAME}.BASE\"\"\"\n",
    "        if session.sql(sample_data_check_sql).count() > 0:\n",
    "            sample_data_deployed = True\n",
    "\n",
    "        if not sample_data_deployed:\n",
    "            create_file_format_sql = f\"\"\"CREATE OR REPLACE FILE FORMAT {SOLUTION_NAME}.BASE.CSV_FORMAT \n",
    "        TYPE = CSV\n",
    "        FIELD_DELIMITER = ','\n",
    "        PARSE_HEADER = TRUE\n",
    "        COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(create_file_format_sql).collect()\n",
    "\n",
    "            create_table_sql = f\"\"\"CREATE OR REPLACE TABLE {SOLUTION_NAME}.BASE.DAILY_PARTITIONED_SAMPLE_DATA USING TEMPLATE (\n",
    "    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*)) \n",
    "     FROM TABLE (INFER_SCHEMA(\n",
    "     LOCATION=>'@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/sample_data/daily_partitioned_sample_data.csv',\n",
    "     FILE_FORMAT=>'{SOLUTION_NAME}.BASE.CSV_FORMAT')))\"\"\"\n",
    "            session.sql(create_table_sql).collect()\n",
    "\n",
    "            tag_table_sql = f\"\"\"ALTER TABLE {SOLUTION_NAME}.BASE.DAILY_PARTITIONED_SAMPLE_DATA SET COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\"\"\"\n",
    "\n",
    "            session.sql(tag_table_sql).collect()\n",
    "\n",
    "            load_table_sql = f\"\"\"COPY INTO {SOLUTION_NAME}.BASE.DAILY_PARTITIONED_SAMPLE_DATA\n",
    "    from '@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/sample_data/daily_partitioned_sample_data.csv'\n",
    "    FILE_FORMAT = (SKIP_HEADER = 1)\"\"\"\n",
    "\n",
    "            session.sql(load_table_sql).collect()\n",
    "\n",
    "        # Check for notebooks - don't overwrite if present\n",
    "        notebooks_deployed = False\n",
    "\n",
    "        notebook_check_sql = f\"\"\"LS @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS PATTERN='.*.ipynb'\"\"\"\n",
    "        if session.sql(notebook_check_sql).count() > 0:\n",
    "            notebooks_deployed = True\n",
    "\n",
    "        # Copy notebooks and python library files to the right spots\n",
    "        if not notebooks_deployed:\n",
    "            copy_notebooks_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS/\n",
    "    FROM @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/\n",
    "    PATTERN = '.*.ipynb'\"\"\"\n",
    "\n",
    "            session.sql(copy_notebooks_sql).collect()\n",
    "\n",
    "            copy_enivronment_yml_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS/\n",
    "    FROM @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/\n",
    "    FILES = ('environment.yml')\"\"\"\n",
    "\n",
    "            session.sql(copy_enivronment_yml_sql).collect()\n",
    "\n",
    "            copy_libraries_sql = f\"\"\"COPY FILES\n",
    "    INTO @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS/forecast_model_builder/\n",
    "    FROM @{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/emerging-solutions-toolbox-main/{TOOLBOX_FOLDER_NAME}/forecast_model_builder/\n",
    "    PATTERN = '.*.py'\"\"\"\n",
    "\n",
    "            session.sql(copy_libraries_sql).collect()\n",
    "\n",
    "        # Create a stored procedure for creating solution projects\n",
    "        sp_deploy_sql = f'''CREATE OR REPLACE PROCEDURE {SOLUTION_NAME}.BASE.CREATE_PROJECT(project_name VARCHAR, warehouse VARCHAR)\n",
    "    RETURNS VARCHAR\n",
    "    LANGUAGE PYTHON\n",
    "    RUNTIME_VERSION = '3.11'\n",
    "    HANDLER = 'create_project'\n",
    "    PACKAGES = ('snowflake-snowpark-python')\n",
    "    COMMENT = '{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{\"major\":1, \"minor\":0}}, \"attributes\":{{\"component\":\"deployment\"}}}}'\n",
    "    EXECUTE AS CALLER\n",
    "    AS\n",
    "$$\n",
    "def create_project(session, project_name, warehouse):\n",
    "    create_schema_sql = f\"\"\"CREATE SCHEMA IF NOT EXISTS {SOLUTION_NAME}.{{project_name}}\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_eda_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_NAME}.{{project_name}}.{{project_name}}__EDA\n",
    "    FROM '@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'eda.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_modeling_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_NAME}.{{project_name}}.{{project_name}}__MODELING\n",
    "    FROM '@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'modeling.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    create_inference_notebook_sql = f\"\"\"CREATE NOTEBOOK IF NOT EXISTS {SOLUTION_NAME}.{{project_name}}.{{project_name}}__INFERENCE\n",
    "    FROM '@{SOLUTION_NAME}.BASE.{DEPLOYMENT_STAGE}/NOTEBOOKS'\n",
    "    MAIN_FILE = 'inference.ipynb'\n",
    "    QUERY_WAREHOUSE = {{warehouse}}\n",
    "    IDLE_AUTO_SHUTDOWN_TIME_SECONDS = 3600\n",
    "    COMMENT = '{{{{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{{{{\"major\":1, \"minor\":0}}}}, \"attributes\":{{{{\"component\":\"deployment\"}}}}}}}}'\"\"\"\n",
    "\n",
    "    session.sql(create_schema_sql).collect()\n",
    "    session.sql(create_eda_notebook_sql).collect()\n",
    "    session.sql(create_modeling_notebook_sql).collect()\n",
    "    session.sql(create_inference_notebook_sql).collect()\n",
    "\n",
    "    return f\"\"\"Project created\"\"\"\n",
    "$$;'''\n",
    "\n",
    "        session.sql(sp_deploy_sql).collect()\n",
    "\n",
    "\n",
    "check_permissions(session)\n",
    "deploy_database(session)\n",
    "deploy_warehouse(session)\n",
    "check_stage(session)\n",
    "deploy_api_integration(session)\n",
    "deploy_solution_specific_code(session)\n",
    "confirmation_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70618532-cec0-4411-b67c-db4fdb83f305",
   "metadata": {
    "collapsed": false,
    "name": "PROJECTS"
   },
   "source": "**Project Creation**\n\nThe next step creates a *project*.  A project is new schema with independent set of notebooks and python files, all generated from the base templates. \n Projects allow for easy isolation of work, while allowing admins to set defaults on the base stages.\n\n- The notebooks are indepenent copies from the base notebook templates\n- The python files are copied for each notebook\n    - This allows for immediate use of the files without using the imported custom packages UI\n    - It does mean that the python files are *independent* copies for each notebook"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfb62-6373-44db-a426-48c86d7ab6ce",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "PROJECT_DEPLOY"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "# Set default value\n",
    "if \"project_name_txt\" not in st.session_state:\n",
    "    st.session_state.project_name_txt = \"\"\n",
    "\n",
    "\n",
    "def create_project(project_name):\n",
    "    if project_name != \"BASE\":\n",
    "        with st.spinner():\n",
    "            session.sql(sql_script).collect()\n",
    "\n",
    "\n",
    "st.header(\"New Project\")\n",
    "st.write(\"Each project gets its own schema and set of notebooks\")\n",
    "project_name = st.text_input(\"Set your project name\", key=\"project_name_txt\")\n",
    "\n",
    "project_name = project_name.upper().replace(\" \", \"_\")\n",
    "\n",
    "warehouse_check_sql = \"\"\"SHOW WAREHOUSES LIKE 'FORECASTING_WH'\"\"\"\n",
    "warehouse_df = session.sql(warehouse_check_sql)\n",
    "\n",
    "if warehouse_df.count() > 0:\n",
    "    warehouse = \"FORECASTING_WH\"\n",
    "else:\n",
    "    warehouse_sql = \"\"\"SELECT CURRENT_WAREHOUSE()\"\"\"\n",
    "    warehouse = session.sql(warehouse_sql).collect()[0][0]\n",
    "\n",
    "sql_script = f\"\"\"CALL FORECAST_MODEL_BUILDER.BASE.CREATE_PROJECT('{project_name}', '{warehouse}')\"\"\"\n",
    "\n",
    "create_col, delete_col = st.columns([6.2, 1])\n",
    "\n",
    "with create_col:\n",
    "    create_btn = st.button(\n",
    "        \"Create\", key=\"project_create_btn\", on_click=create_project, args={project_name}\n",
    "    )\n",
    "\n",
    "\n",
    "with delete_col:\n",
    "    if st.button(\"Delete\", key=\"project_delete_btn\"):\n",
    "        if project_name != \"BASE\":\n",
    "            with st.spinner():\n",
    "                delete_sql = (\n",
    "                    f\"\"\"DROP SCHEMA FORECAST_MODEL_BUILDER.{project_name} CASCADE\"\"\"\n",
    "                )\n",
    "                session.sql(delete_sql).collect()\n",
    "\n",
    "\n",
    "projects_sql = \"\"\"SHOW SCHEMAS IN DATABASE FORECAST_MODEL_BUILDER\"\"\"\n",
    "projects_df = session.sql(projects_sql)\n",
    "\n",
    "project_list = []\n",
    "\n",
    "for row in projects_df.to_local_iterator():\n",
    "    if row[\"name\"] not in [\"BASE\", \"INFORMATION_SCHEMA\"]:\n",
    "        project_list.append(row[\"name\"])\n",
    "\n",
    "st.subheader(\"Current Projects\")\n",
    "st.dataframe(project_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f535428-5e4f-4b3a-96e0-082a040e855b",
   "metadata": {
    "collapsed": false,
    "name": "NEXT_STEPS"
   },
   "source": "**Next Steps**\n\nOnce you've created your project, go to your Notebooks and start with <your project name>__EDA"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "brett.klein@snowflake.com",
   "authorId": "354777188352",
   "authorName": "BKLEIN",
   "lastEditTime": 1741027097574,
   "notebookId": "msn4qrjy7e4ctvv5x5za",
   "sessionId": "2970455d-e5dd-47b1-b069-7cc997ea858f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
