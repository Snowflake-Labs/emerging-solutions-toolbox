{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000000",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_title"
      },
      "source": [
        "# Evaluation Notebook\n",
        "Use the model trained in the modeling notebook to make and evaluate predictions on a test dataset.\n",
        "\n",
        "#### NOTE: The user must have split data into train/test datasets in the modeling notebook before running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000001",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_instructions"
      },
      "source": [
        "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
        "## Instructions\n",
        "\n",
        "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
        "    - Adjust the values of the user constants\n",
        "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
        "    - The notebook will perform inference and evaluation. \n",
        "    - PROMOTE_MODEL is set to false. If evaluation is satisfactory, change the value at the end of the notebook and run the last 2 cells. If model is promoted, predictions on test data will be stored in a Snowflake table and a model monitor will be create to track future inference values.\n",
        "    \n",
        "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce110000-1111-2222-3333-ffffff000002",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_imports"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import math\n",
        "import json\n",
        "from datetime import datetime\n",
        "from snowflake.ml.registry import registry\n",
        "from snowflake.ml.dataset import Dataset\n",
        "from snowflake.snowpark import functions as F\n",
        "from snowflake.snowpark import types as T\n",
        "from snowflake.snowpark.window import Window\n",
        "from snowflake.snowpark import DataFrame as SnowparkDataFrame\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from forecast_model_builder.utils import connect, perform_inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000004",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_USER_SETUP"
      },
      "source": [
        "-----\n",
        "# SETUP\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce110000-1111-2222-3333-ffffff000005",
      "metadata": {
        "language": "python",
        "name": "_set_global_variables"
      },
      "outputs": [],
      "source": [
        "# Name of project\n",
        "PROJECT_SCHEMA = \"TEST_PROJECT\"\n",
        "\n",
        "# Set warehouse\n",
        "SESSION_WH = \"FORECAST_MODEL_BUILDER_WH\"\n",
        "\n",
        "# Table name to store the PREDICTION results.\n",
        "# NOTE: If the table name is not fully qualified with DB.SCHEMA, the session's default database and schema will be used.\n",
        "# NOTE: Model version will be appended to the table name to save predictions from a particular run.\n",
        "INFERENCE_RESULT_TBL_NM = \"FORECAST_RESULTS\"\n",
        "\n",
        "# Input data for inference\n",
        "INFERENCE_DB = \"FORECAST_MODEL_BUILDER\"\n",
        "INFERENCE_SCHEMA = \"BASE\"\n",
        "INFERENCE_FV = \"FORECAST_FEATURES\"\n",
        "\n",
        "# Name of the model to use for inference, as well as the Database and Schema of the model registry.\n",
        "# NOTE: The default model version from the registry will be used.\n",
        "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
        "MODEL_SCHEMA = \"MODELING\"\n",
        "MODEL_NAME = \"TEST_MODEL_1\"\n",
        "\n",
        "# If using direct multistep forecasting, set LEAD to the lead model you wish to evaluate.\n",
        "# Otherwise, set to 0\n",
        "LEAD = 0\n",
        "\n",
        "# Scaling up the warehouse may speed up execution time, especially if there are many partitions.\n",
        "# NOTE: If set to None, then the session warehouse will be used.\n",
        "INFERENCE_WH = \"STANDARD_XL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce110000-1111-2222-3333-ffffff000003",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_establish_session"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session db.schema: FORECAST_MODEL_BUILDER.TEST_PROJECT\n",
            "Session warehouse: FORECAST_MODEL_BUILDER_WH\n",
            "Current Datetime: 2026-02-17 11:55:57.621871\n"
          ]
        }
      ],
      "source": [
        "# Establish session\n",
        "session = connect(connection_name=\"default\")\n",
        "session.use_database(MODEL_DB)\n",
        "session_db = MODEL_DB\n",
        "session.use_schema(PROJECT_SCHEMA)\n",
        "session_schema = PROJECT_SCHEMA\n",
        "print(f\"Session db.schema: {session_db}.{session_schema}\")\n",
        "print(f\"Session warehouse: {SESSION_WH}\")\n",
        "\n",
        "# Query tag\n",
        "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}'\n",
        "session.query_tag = query_tag\n",
        "\n",
        "# Get the current datetime  (This will be saved in the model storage table)\n",
        "run_dttm = datetime.now()\n",
        "print(f\"Current Datetime: {run_dttm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000006",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_objects"
      },
      "source": [
        "-----\n",
        "# Establish objects needed for this run\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce110000-1111-2222-3333-ffffff000007",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_set_other_objects"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session warehouse:          FORECAST_MODEL_BUILDER_WH\n",
            "WARNING: User does not have access to INFERENCE_WH = 'STANDARD_XL'. Inference will use 'FORECAST_MODEL_BUILDER_WH' instead. \n",
            "\n",
            "Model Version:              DULL_BIRD_3\n"
          ]
        }
      ],
      "source": [
        "# Derived Objects\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Notebook Warehouse\n",
        "# -----------------------------------------------------------------------\n",
        "session.use_warehouse(SESSION_WH)\n",
        "print(f\"Session warehouse:          {SESSION_WH}\")\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Check Inference Warehouse\n",
        "# -----------------------------------------------------------------------\n",
        "# Check that the user specified an available warehouse as INFERENCE_WH. If not, use the session warehouse.\n",
        "available_warehouses = [\n",
        "    row[\"NAME\"]\n",
        "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
        "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
        "    .collect()\n",
        "]\n",
        "\n",
        "if INFERENCE_WH in available_warehouses:\n",
        "    print(f\"Inference warehouse:        {INFERENCE_WH} \\n\")\n",
        "else:\n",
        "    print(\n",
        "        f\"WARNING: User does not have access to INFERENCE_WH = '{INFERENCE_WH}'. Inference will use '{SESSION_WH}' instead. \\n\"\n",
        "    )\n",
        "    INFERENCE_WH = SESSION_WH\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Fully qualified MODEL NAME\n",
        "# -----------------------------------------------------------------------\n",
        "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Get the model and the version name of the default version\n",
        "# -----------------------------------------------------------------------\n",
        "# Establish registry object\n",
        "reg = registry.Registry(\n",
        "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
        ")\n",
        "\n",
        "# Get the model from the registry\n",
        "mv = reg.get_model(qualified_model_name).last()\n",
        "\n",
        "# Get the default version name\n",
        "model_version_nm = mv.version_name\n",
        "\n",
        "print(f\"Model Version:              {model_version_nm}\")\n",
        "\n",
        "# --------------------------------\n",
        "# User Constants from Model Setup\n",
        "# --------------------------------\n",
        "stored_constants = mv.show_metrics()[\"user_settings\"]\n",
        "\n",
        "TIME_PERIOD_COLUMN = stored_constants[\"TIME_PERIOD_COLUMN\"]\n",
        "TARGET_COLUMN = stored_constants[\"TARGET_COLUMN\"]\n",
        "PARTITION_COLUMNS = stored_constants[\"PARTITION_COLUMNS\"]\n",
        "ALL_EXOG_COLS_HAVE_FUTURE_VALS = stored_constants[\"ALL_EXOG_COLS_HAVE_FUTURE_VALS\"]\n",
        "USE_CONTEXT = stored_constants[\"USE_CONTEXT\"]\n",
        "\n",
        "if not USE_CONTEXT:\n",
        "    MODEL_BINARY_STORAGE_TBL_NM = stored_constants[\"MODEL_BINARY_STORAGE_TBL_NM\"]\n",
        "\n",
        "if (not ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD==0):\n",
        "    raise ValueError(\n",
        "        \"\"\"If using direct multistep modeling approach, LEAD must be set to a number \n",
        "        greater than 0 to filter results to a particular lead model\"\"\"\n",
        "    )\n",
        "if (ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD>0):\n",
        "    raise ValueError(\n",
        "        \"\"\"If using global modeling approach, LEAD must be set to a 0\"\"\"\n",
        "    )\n",
        "# --------------------------------\n",
        "# Get datasets\n",
        "# --------------------------------\n",
        "\n",
        "def load_df_from_ds(fully_qualified_name, version):\n",
        "    ds_db, ds_schema, ds_name = fully_qualified_name.split('.')\n",
        "\n",
        "    return Dataset(\n",
        "        session=session,\n",
        "        database=ds_db,\n",
        "        schema=ds_schema,\n",
        "        name=ds_name,\n",
        "        selected_version=version\n",
        "    ).read.to_snowpark_dataframe()\n",
        "\n",
        "train_df = load_df_from_ds(\n",
        "    fully_qualified_name=mv.show_metrics()['train_dataset']['name'],\n",
        "    version=mv.show_metrics()['train_dataset']['version']\n",
        ").drop(\"GROUP_IDENTIFIER\")\n",
        "\n",
        "test_df = load_df_from_ds(\n",
        "    fully_qualified_name=mv.show_metrics()['test_dataset']['name'],\n",
        "    version=mv.show_metrics()['test_dataset']['version']\n",
        ").drop(\"GROUP_IDENTIFIER\")\n",
        "\n",
        "# Filter to a particular lead model if performing direct multi step forecasting\n",
        "if LEAD > 0:\n",
        "    train_df = train_df.filter(\n",
        "            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n",
        "        )\n",
        "    test_df = test_df.filter(\n",
        "            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000012",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_inference"
      },
      "source": [
        "-----\n",
        "# Inference\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce110000-1111-2222-3333-ffffff000013",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_perform_inference"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions\n",
            "Inference input data row count: 344750\n",
            "Number of end partition invocations to expect in the query profile: 1750\n",
            "Inference input data row count: 22750\n",
            "Number of end partition invocations to expect in the query profile: 250\n",
            "-----------------------------------------------------------------------------------\n",
            "|\"_PRED_\"           |\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"DATASET\"  |\n",
            "-----------------------------------------------------------------------------------\n",
            "|644.90185546875    |STORE_ID_8_PRODUCT_ID_3    |2024-12-06 00:00:00  |TEST       |\n",
            "|634.2355346679688  |STORE_ID_8_PRODUCT_ID_3    |2024-12-17 00:00:00  |TEST       |\n",
            "-----------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# INFERENCE\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "print(\"Predictions\")\n",
        "session.use_warehouse(INFERENCE_WH)\n",
        "\n",
        "train_result = perform_inference(session, train_df, mv).with_column(\"DATASET\",F.lit(\"TRAIN\")).cache_result()\n",
        "test_result = perform_inference(session, test_df, mv).with_column(\"DATASET\",F.lit(\"TEST\")).cache_result()\n",
        "test_result.show(2)\n",
        "\n",
        "\n",
        "inference_result = train_result.union_all_by_name(test_result)\n",
        "session.use_warehouse(SESSION_WH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce110000-1111-2222-3333-ffffff000008",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_pred_v_actual"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has 250 partitions\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "|\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"MODEL_TARGET\"     |\"PREDICTED\"        |\"DATASET\"  |\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "|STORE_ID_9_PRODUCT_ID_8    |2023-10-18 00:00:00  |876.3895263671875  |877.7786254882812  |TRAIN      |\n",
            "|STORE_ID_9_PRODUCT_ID_8    |2023-07-01 00:00:00  |925.4633178710938  |926.1979370117188  |TRAIN      |\n",
            "|STORE_ID_9_PRODUCT_ID_8    |2023-07-05 00:00:00  |913.06884765625    |912.70751953125    |TRAIN      |\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get predicted vs. actual dataframes for train and test\n",
        "\n",
        "sdf = train_df.union_all_by_name(test_df).select(\"GROUP_IDENTIFIER_STRING\",TIME_PERIOD_COLUMN,TARGET_COLUMN)\n",
        "\n",
        "pred_v_actuals = (\n",
        "    inference_result\n",
        "    .join(sdf, on=[\"GROUP_IDENTIFIER_STRING\", TIME_PERIOD_COLUMN])\n",
        "    .select(\n",
        "        \"GROUP_IDENTIFIER_STRING\", \n",
        "        TIME_PERIOD_COLUMN, \n",
        "        TARGET_COLUMN,\n",
        "        F.col(\"_PRED_\").alias(\"PREDICTED\"),\n",
        "        \"DATASET\"\n",
        "    )\n",
        ")\n",
        "\n",
        "inference_partition_count = pred_v_actuals.select(\"GROUP_IDENTIFIER_STRING\").distinct().count()\n",
        "\n",
        "training_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TRAIN\")\n",
        "\n",
        "test_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TEST\")\n",
        "print(f\"Dataset has {inference_partition_count} partitions\")\n",
        "pred_v_actuals.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ce110000-1111-2222-3333-ffffff000009",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_partition_weights"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|\"GROUP_IDENTIFIER_STRING\"  |\"PARTITION_MODEL_TARGET_SUM\"  |\"MIN(ORDER_TIMESTAMP)\"  |\"MAX(ORDER_TIMESTAMP)\"  |\"TOTAL_MODEL_TARGET\"  |\"PARTITION_WEIGHT\"     |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|STORE_ID_17_PRODUCT_ID_1   |1456821.035583496             |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.00795719234225625    |\n",
            "|STORE_ID_24_PRODUCT_ID_6   |1438105.7832641602            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007854969173588749   |\n",
            "|STORE_ID_22_PRODUCT_ID_3   |1411616.508605957             |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007710284103622175   |\n",
            "|STORE_ID_21_PRODUCT_ID_9   |1404011.4907226562            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007668745308817853   |\n",
            "|STORE_ID_13_PRODUCT_ID_8   |1399447.1314697266            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.00764381466626973    |\n",
            "|STORE_ID_10_PRODUCT_ID_4   |1386199.2810058594            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007571454581065291   |\n",
            "|STORE_ID_15_PRODUCT_ID_9   |1380295.8366699219            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.00753920982284484    |\n",
            "|STORE_ID_7_PRODUCT_ID_5    |1377805.5510253906            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007525607821379534   |\n",
            "|STORE_ID_14_PRODUCT_ID_10  |1371382.2724609375            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.007490523715812117   |\n",
            "|STORE_ID_9_PRODUCT_ID_5    |1340434.5415039062            |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963    |0.0073214864478385235  |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate weights of each partition for weighted metrics\n",
        "\n",
        "total_window = Window.partition_by()\n",
        "\n",
        "partition_weights = (\n",
        "    pred_v_actuals\n",
        "    .group_by(\"GROUP_IDENTIFIER_STRING\")\n",
        "    .agg(F.sum(TARGET_COLUMN).alias(f'PARTITION_{TARGET_COLUMN}_SUM'), F.min(TIME_PERIOD_COLUMN), F.max(TIME_PERIOD_COLUMN))\n",
        "    .with_column(f\"TOTAL_{TARGET_COLUMN}\", F.sum(f'PARTITION_{TARGET_COLUMN}_SUM').over(total_window))\n",
        "    .with_column(\"PARTITION_WEIGHT\", F.col(f'PARTITION_{TARGET_COLUMN}_SUM')/F.col(f\"TOTAL_{TARGET_COLUMN}\"))\n",
        ")\n",
        "\n",
        "partition_weights.sort(F.col(\"PARTITION_WEIGHT\").desc()).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6211445-d2e9-432b-9608-c642609efe72",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_overall_performance"
      },
      "source": [
        "# Overall Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ce110000-1111-2222-3333-ffffff000010",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_produce_metrics"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET\n",
            "Avg and Median of each metric over all the partitions:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<snowflake.snowpark.dataframe.DataFrame at 0x174f27f20>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION SET\n",
            "Avg and Median of each metric over all the partitions:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<snowflake.snowpark.dataframe.DataFrame at 0x174fa5400>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def produce_metrics(sdf: SnowparkDataFrame) -> SnowparkDataFrame:\n",
        "    # Row-level metrics\n",
        "    row_actual_v_fcst = (\n",
        "        sdf\n",
        "        .with_column(\"PRED_ERROR\", F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
        "        .with_column(\n",
        "            \"ABS_ERROR\", F.abs(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
        "        )\n",
        "        .with_column(\n",
        "            \"APE\",\n",
        "            F.when(F.col(TARGET_COLUMN) == 0, F.lit(None)).otherwise(\n",
        "                F.abs(F.col(\"ABS_ERROR\") / F.col(TARGET_COLUMN))\n",
        "            ),\n",
        "        )\n",
        "        .with_column(\"SQ_ERROR\", F.pow(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"), 2))\n",
        "    )\n",
        "    \n",
        "    # Metrics per partition\n",
        "    partition_metrics = row_actual_v_fcst.group_by(\"GROUP_IDENTIFIER_STRING\").agg(\n",
        "        F.avg(\"APE\").alias(\"MAPE\"),\n",
        "        F.avg(\"ABS_ERROR\").alias(\"MAE\"),\n",
        "        F.sqrt(F.avg(\"SQ_ERROR\")).alias(\"RMSE\"),\n",
        "        F.count(\"*\").alias(\"TOTAL_PRED_COUNT\"),\n",
        "    )\n",
        "    \n",
        "    # Overall modeling process across all partitions\n",
        "    overall_avg_metrics = partition_metrics.agg(\n",
        "        F.avg(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
        "        F.avg(\"MAE\").alias(\"OVERALL_MAE\"),\n",
        "        F.avg(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
        "    ).with_column(\"AGGREGATION\", F.lit(\"AVG\"))\n",
        "    \n",
        "    overall_weighted_avg_metrics = (\n",
        "        partition_metrics\n",
        "            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
        "            .agg(\n",
        "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAPE\")).alias(\"OVERALL_MAPE\"),\n",
        "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAE\")).alias(\"OVERALL_MAE\"),\n",
        "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"RMSE\")).alias(\"OVERALL_RMSE\"),\n",
        "                 )\n",
        "            .with_column(\"AGGREGATION\", F.lit(\"WEIGHTED_AVG\"))\n",
        "    )\n",
        "    \n",
        "    overall_median_metrics = partition_metrics.agg(\n",
        "        F.median(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
        "        F.median(\"MAE\").alias(\"OVERALL_MAE\"),\n",
        "        F.median(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
        "    ).with_column(\"AGGREGATION\", F.lit(\"MEDIAN\"))\n",
        "    \n",
        "    overall_metrics = (\n",
        "        overall_avg_metrics\n",
        "            .union(overall_median_metrics)\n",
        "            .union(overall_weighted_avg_metrics)\n",
        "            .select(\"AGGREGATION\", \"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n",
        "            .sort(\"AGGREGATION\")\n",
        "    )\n",
        "    \n",
        "    # Show the metrics\n",
        "    if inference_partition_count == 1:\n",
        "        print(\n",
        "            \"There is only 1 partition, so these values are the metrics for that single model:\"\n",
        "        )\n",
        "        display(\n",
        "            overall_median_metrics.select(\"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n",
        "        )\n",
        "    else:\n",
        "        print(\"Avg and Median of each metric over all the partitions:\")\n",
        "        display(overall_metrics)\n",
        "\n",
        "    return row_actual_v_fcst, partition_metrics\n",
        "\n",
        "print(\"TRAINING SET\")\n",
        "train_metric_sdf, train_partition_metrics = produce_metrics(training_pred_v_actuals)\n",
        "print(\"VALIDATION SET\")\n",
        "test_metric_sdf, test_partition_metrics = produce_metrics(test_pred_v_actuals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464115e1-dacb-4fbd-8638-6595bc802457",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_partition_performance"
      },
      "source": [
        "# Partition Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d63dd9d6-9a83-440e-a41c-cf58961ce3c7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_metric_distribution"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'modin' has no attribute 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics):\n\u001b[32m     17\u001b[39m     visible = (i == \u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistribution_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mboxpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpointpos\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhovertext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistribution_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGROUP_IDENTIFIER_STRING\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhoverinfo\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext+x\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvisible\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisible\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m fig.update_layout(\n\u001b[32m     30\u001b[39m     title=\u001b[33m\"\u001b[39m\u001b[33mMAPE Distribution\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     template=\u001b[33m\"\u001b[39m\u001b[33mplotly_white\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     ]\n\u001b[32m     61\u001b[39m )\n\u001b[32m     63\u001b[39m fig.show()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/graph_objs/_figure.py:343\u001b[39m, in \u001b[36mFigure.add_trace\u001b[39m\u001b[34m(self, trace, row, col, secondary_y, exclude_empty_subplots)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_trace\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m, trace, row=\u001b[38;5;28;01mNone\u001b[39;00m, col=\u001b[38;5;28;01mNone\u001b[39;00m, secondary_y=\u001b[38;5;28;01mNone\u001b[39;00m, exclude_empty_subplots=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    270\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mFigure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    272\u001b[39m \n\u001b[32m    273\u001b[39m \u001b[33;03m    Add a trace to the figure\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    341\u001b[39m \n\u001b[32m    342\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecondary_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_empty_subplots\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:2123\u001b[39m, in \u001b[36mBaseFigure.add_trace\u001b[39m\u001b[34m(self, trace, row, col, secondary_y, exclude_empty_subplots)\u001b[39m\n\u001b[32m   2114\u001b[39m         \u001b[38;5;28mself\u001b[39m.add_trace(\n\u001b[32m   2115\u001b[39m             trace,\n\u001b[32m   2116\u001b[39m             row=r,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2119\u001b[39m             exclude_empty_subplots=exclude_empty_subplots,\n\u001b[32m   2120\u001b[39m         )\n\u001b[32m   2121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_traces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m    \u001b[49m\u001b[43msecondary_ys\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msecondary_y\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msecondary_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_empty_subplots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_empty_subplots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/graph_objs/_figure.py:421\u001b[39m, in \u001b[36mFigure.add_traces\u001b[39m\u001b[34m(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_traces\u001b[39m(\n\u001b[32m    346\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    347\u001b[39m     data,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m     exclude_empty_subplots=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    352\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mFigure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    353\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    354\u001b[39m \n\u001b[32m    355\u001b[39m \u001b[33;03m    Add traces to the figure\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    419\u001b[39m \n\u001b[32m    420\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_traces\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecondary_ys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_empty_subplots\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:2207\u001b[39m, in \u001b[36mBaseFigure.add_traces\u001b[39m\u001b[34m(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\u001b[39m\n\u001b[32m   2139\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2140\u001b[39m \u001b[33;03mAdd traces to the figure\u001b[39;00m\n\u001b[32m   2141\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2203\u001b[39m \u001b[33;03mFigure(...)\u001b[39;00m\n\u001b[32m   2204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2206\u001b[39m \u001b[38;5;66;03m# Validate traces\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2207\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_validator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_coerce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[38;5;66;03m# Set trace indexes\u001b[39;00m\n\u001b[32m   2210\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ind, new_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:2626\u001b[39m, in \u001b[36mBaseDataValidator.validate_coerce\u001b[39m\u001b[34m(self, v, skip_invalid, _validate)\u001b[39m\n\u001b[32m   2624\u001b[39m         invalid_els.append(v_el)\n\u001b[32m   2625\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2626\u001b[39m     trace = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_trace_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_invalid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_invalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mv_el\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2629\u001b[39m     res.append(trace)\n\u001b[32m   2631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m type_in_v_el:\n\u001b[32m   2632\u001b[39m     \u001b[38;5;66;03m# Restore type in v_el\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/graph_objs/_box.py:2976\u001b[39m, in \u001b[36mBox.__init__\u001b[39m\u001b[34m(self, arg, alignmentgroup, boxmean, boxpoints, customdata, customdatasrc, dx, dy, fillcolor, hoverinfo, hoverinfosrc, hoverlabel, hoveron, hovertemplate, hovertemplatesrc, hovertext, hovertextsrc, ids, idssrc, jitter, legend, legendgroup, legendgrouptitle, legendrank, legendwidth, line, lowerfence, lowerfencesrc, marker, mean, meansrc, median, mediansrc, meta, metasrc, name, notched, notchspan, notchspansrc, notchwidth, offsetgroup, opacity, orientation, pointpos, q1, q1src, q3, q3src, quartilemethod, sd, sdmultiple, sdsrc, selected, selectedpoints, showlegend, showwhiskers, sizemode, stream, text, textsrc, uid, uirevision, unselected, upperfence, upperfencesrc, visible, whiskerwidth, width, x, x0, xaxis, xcalendar, xhoverformat, xperiod, xperiod0, xperiodalignment, xsrc, y, y0, yaxis, ycalendar, yhoverformat, yperiod, yperiod0, yperiodalignment, ysrc, zorder, **kwargs)\u001b[39m\n\u001b[32m   2974\u001b[39m \u001b[38;5;28mself\u001b[39m._set_property(\u001b[33m\"\u001b[39m\u001b[33mhovertemplate\u001b[39m\u001b[33m\"\u001b[39m, arg, hovertemplate)\n\u001b[32m   2975\u001b[39m \u001b[38;5;28mself\u001b[39m._set_property(\u001b[33m\"\u001b[39m\u001b[33mhovertemplatesrc\u001b[39m\u001b[33m\"\u001b[39m, arg, hovertemplatesrc)\n\u001b[32m-> \u001b[39m\u001b[32m2976\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_property\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhovertext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhovertext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28mself\u001b[39m._set_property(\u001b[33m\"\u001b[39m\u001b[33mhovertextsrc\u001b[39m\u001b[33m\"\u001b[39m, arg, hovertextsrc)\n\u001b[32m   2978\u001b[39m \u001b[38;5;28mself\u001b[39m._set_property(\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m, arg, ids)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:4403\u001b[39m, in \u001b[36mBasePlotlyType._set_property\u001b[39m\u001b[34m(self, name, arg, provided)\u001b[39m\n\u001b[32m   4397\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_set_property\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, arg, provided):\n\u001b[32m   4398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4399\u001b[39m \u001b[33;03m    Initialize a property of this object using the provided value\u001b[39;00m\n\u001b[32m   4400\u001b[39m \u001b[33;03m    or a value popped from the arguments dictionary. If neither\u001b[39;00m\n\u001b[32m   4401\u001b[39m \u001b[33;03m    is available, do not set the property.\u001b[39;00m\n\u001b[32m   4402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4403\u001b[39m     \u001b[43m_set_property_provided_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovided\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:398\u001b[39m, in \u001b[36m_set_property_provided_value\u001b[39m\u001b[34m(obj, name, arg, provided)\u001b[39m\n\u001b[32m    396\u001b[39m val = provided \u001b[38;5;28;01mif\u001b[39;00m provided \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m val\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[43mobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m = val\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:4932\u001b[39m, in \u001b[36mBasePlotlyType.__setitem__\u001b[39m\u001b[34m(self, prop, value)\u001b[39m\n\u001b[32m   4928\u001b[39m         \u001b[38;5;28mself\u001b[39m._set_array_prop(prop, value)\n\u001b[32m   4930\u001b[39m     \u001b[38;5;66;03m# ### Handle simple property ###\u001b[39;00m\n\u001b[32m   4931\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4932\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4934\u001b[39m     \u001b[38;5;66;03m# Make sure properties dict is initialized\u001b[39;00m\n\u001b[32m   4935\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_props()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/plotly/basedatatypes.py:5271\u001b[39m, in \u001b[36mBasePlotlyType._set_prop\u001b[39m\u001b[34m(self, prop, val)\u001b[39m\n\u001b[32m   5268\u001b[39m validator = \u001b[38;5;28mself\u001b[39m._get_validator(prop)\n\u001b[32m   5270\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5271\u001b[39m     val = \u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_coerce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   5273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._skip_invalid:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:1066\u001b[39m, in \u001b[36mStringValidator.validate_coerce\u001b[39m\u001b[34m(self, v)\u001b[39m\n\u001b[32m   1063\u001b[39m np = get_module(\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# If not strict, let numpy cast elements to strings\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m v = \u001b[43mcopy_to_readonly_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mU\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[38;5;66;03m# Check no_blank\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.no_blank:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/_plotly_utils/basevalidators.py:96\u001b[39m, in \u001b[36mcopy_to_readonly_numpy_array\u001b[39m\u001b[34m(v, kind, force_numeric)\u001b[39m\n\u001b[32m     87\u001b[39m kind_default_dtypes = {\n\u001b[32m     88\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muint32\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mi\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mint32\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     92\u001b[39m }\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# With `pass_through=True`, the original object will be returned if unable to convert\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# to a Narwhals DataFrame or Series.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m v = \u001b[43mnw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_native\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_series\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_through\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, nw.Series):\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v.dtype == nw.Datetime \u001b[38;5;129;01mand\u001b[39;00m v.dtype.time_zone \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    100\u001b[39m         \u001b[38;5;66;03m# Remove time zone so that local time is displayed\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/narwhals/stable/v1/__init__.py:1619\u001b[39m, in \u001b[36mfrom_native\u001b[39m\u001b[34m(native_object, strict, pass_through, eager_only, eager_or_interchange_only, series_only, allow_series)\u001b[39m\n\u001b[32m   1613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m native_object\n\u001b[32m   1615\u001b[39m pass_through = validate_strict_and_pass_though(\n\u001b[32m   1616\u001b[39m     strict, pass_through, pass_through_default=\u001b[38;5;28;01mFalse\u001b[39;00m, emit_deprecation_warning=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1617\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1619\u001b[39m result = \u001b[43m_from_native_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnative_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpass_through\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_through\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[43meager_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43meager_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[43meager_or_interchange_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43meager_or_interchange_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseries_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseries_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_series\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVersion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mV1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _stableify(result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/narwhals/translate.py:530\u001b[39m, in \u001b[36m_from_native_impl\u001b[39m\u001b[34m(native_object, pass_through, eager_only, eager_or_interchange_only, series_only, allow_series, version)\u001b[39m\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Series(\n\u001b[32m    520\u001b[39m         PandasLikeSeries(\n\u001b[32m    521\u001b[39m             native_object,\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    527\u001b[39m     )\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Modin\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_modin_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnative_object\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnarwhals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pandas_like\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasLikeDataFrame\n\u001b[32m    533\u001b[39m     mpd = get_modin()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/narwhals/dependencies.py:150\u001b[39m, in \u001b[36mis_modin_dataframe\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_modin_dataframe\u001b[39m(df: Any) -> TypeIs[mpd.DataFrame]:\n\u001b[32m    149\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check whether `df` is a modin DataFrame without importing modin.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (mpd := \u001b[43mget_modin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, mpd.DataFrame)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/narwhals/dependencies.py:57\u001b[39m, in \u001b[36mget_modin\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get modin.pandas module (if already imported - else return None).\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (modin := sys.modules.get(\u001b[33m\"\u001b[39m\u001b[33mmodin\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpandas\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mAttributeError\u001b[39m: module 'modin' has no attribute 'pandas'"
          ]
        }
      ],
      "source": [
        "if (len(PARTITION_COLUMNS) > 0) & (inference_partition_count > 1):\n",
        "    distribution_df = test_partition_metrics.to_pandas()\n",
        "    \n",
        "    table_to_show_sdf = (\n",
        "        test_partition_metrics\n",
        "        .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
        "        .with_column(\"WGHT_PCT\", F.col(\"PARTITION_WEIGHT\")*100)\n",
        "        .select(\"GROUP_IDENTIFIER_STRING\", \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\")\n",
        "    )\n",
        "    table_df = table_to_show_sdf.to_pandas()\n",
        "    \n",
        "    metrics = [\"MAPE\", \"MAE\", \"RMSE\"]\n",
        "    \n",
        "    fig = go.Figure()\n",
        "    \n",
        "    for i, metric in enumerate(metrics):\n",
        "        visible = (i == 0)\n",
        "        fig.add_trace(go.Box(\n",
        "            x=distribution_df[metric],\n",
        "            name=metric,\n",
        "            boxpoints=\"all\",\n",
        "            jitter=0.3,\n",
        "            pointpos=-1.8,\n",
        "            hovertext=distribution_df[\"GROUP_IDENTIFIER_STRING\"],\n",
        "            hoverinfo=\"text+x\",\n",
        "            visible=visible\n",
        "        ))\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title=\"MAPE Distribution\",\n",
        "        template=\"plotly_white\",\n",
        "        updatemenus=[\n",
        "            dict(\n",
        "                active=0,\n",
        "                buttons=[\n",
        "                    dict(\n",
        "                        label=metric,\n",
        "                        method=\"update\",\n",
        "                        args=[\n",
        "                            {\"visible\": [m == metric for m in metrics]},\n",
        "                            {\"title\": f\"{metric} Distribution\",\n",
        "                             \"xaxis\": {\"title\": metric, \"range\": [distribution_df[metric].min(), distribution_df[metric].max()]}}\n",
        "                        ]\n",
        "                    ) for metric in metrics\n",
        "                ],\n",
        "                direction=\"down\",\n",
        "                showactive=True,\n",
        "                x=0.0,\n",
        "                xanchor=\"left\",\n",
        "                y=1.15,\n",
        "                yanchor=\"top\"\n",
        "            )\n",
        "        ],\n",
        "        xaxis=dict(\n",
        "            rangeslider=dict(visible=True),\n",
        "            title=\"MAPE\"\n",
        "        ),\n",
        "        annotations=[\n",
        "            dict(text=\"Metric:\", x=0, xref=\"paper\", y=1.12, yref=\"paper\", showarrow=False, xanchor=\"right\")\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "    print(\"## BEST Performing Partitions (sorted by MAPE)\")\n",
        "    display(table_df.sort_values(\"MAPE\", key=abs).head(20))\n",
        "    \n",
        "    print(\"## WORST Performing Partitions (sorted by MAPE desc)\")\n",
        "    display(table_df.sort_values(\"MAPE\", key=abs, ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345df61d-96ed-4272-a298-e20d61acf0de",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_partition_plots"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# Visualize individual partition actual vs pred on a time series line chart\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "pred_v_actuals_pdf = pred_v_actuals.to_pandas()\n",
        "pred_v_actuals_pdf[TIME_PERIOD_COLUMN] = pd.to_datetime(pred_v_actuals_pdf[TIME_PERIOD_COLUMN])\n",
        "partition_list = sorted(pred_v_actuals_pdf[\"GROUP_IDENTIFIER_STRING\"].unique().tolist())\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    subplot_titles=(\"Line Plot: Actual & Predicted\", \"Scatter Plot: Actual vs. Predicted\"),\n",
        "    vertical_spacing=0.15\n",
        ")\n",
        "\n",
        "for i, partition in enumerate(partition_list):\n",
        "    pdf = pred_v_actuals_pdf[pred_v_actuals_pdf[\"GROUP_IDENTIFIER_STRING\"] == partition].sort_values(TIME_PERIOD_COLUMN)\n",
        "    visible = (i == 0)\n",
        "    \n",
        "    split_date = pdf[pdf[\"DATASET\"] == \"TRAIN\"][TIME_PERIOD_COLUMN].max()\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=pdf[TIME_PERIOD_COLUMN], y=pdf[TARGET_COLUMN],\n",
        "        mode=\"lines\", name=\"Actual\", line=dict(color=\"blue\"),\n",
        "        visible=visible, legendgroup=partition, showlegend=True\n",
        "    ), row=1, col=1)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=pdf[TIME_PERIOD_COLUMN], y=pdf[\"PREDICTED\"],\n",
        "        mode=\"lines\", name=\"Predicted\", line=dict(color=\"red\"),\n",
        "        visible=visible, legendgroup=partition, showlegend=True\n",
        "    ), row=1, col=1)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[split_date, split_date], y=[pdf[TARGET_COLUMN].min(), pdf[TARGET_COLUMN].max()],\n",
        "        mode=\"lines\", name=\"Forecast Start\", line=dict(color=\"green\", dash=\"dash\"),\n",
        "        visible=visible, legendgroup=partition, showlegend=True\n",
        "    ), row=1, col=1)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=pdf[TARGET_COLUMN], y=pdf[\"PREDICTED\"],\n",
        "        mode=\"markers\", name=\"Actual vs Pred\", opacity=0.6,\n",
        "        visible=visible, legendgroup=partition, showlegend=False\n",
        "    ), row=2, col=1)\n",
        "    \n",
        "    min_val, max_val = pdf[TARGET_COLUMN].min(), pdf[TARGET_COLUMN].max()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=[min_val, max_val], y=[min_val, max_val],\n",
        "        mode=\"lines\", name=\"y=x\", line=dict(color=\"black\", dash=\"dash\"),\n",
        "        visible=visible, legendgroup=partition, showlegend=True\n",
        "    ), row=2, col=1)\n",
        "\n",
        "traces_per_partition = 5\n",
        "\n",
        "buttons = []\n",
        "for i, partition in enumerate(partition_list):\n",
        "    visibility = [False] * (len(partition_list) * traces_per_partition)\n",
        "    for j in range(traces_per_partition):\n",
        "        visibility[i * traces_per_partition + j] = True\n",
        "    buttons.append(dict(\n",
        "        label=partition,\n",
        "        method=\"update\",\n",
        "        args=[{\"visible\": visibility}, {\"title\": f\"Partition: {partition}\"}]\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title=f\"Partition: {partition_list[0]}\",\n",
        "    updatemenus=[dict(\n",
        "        active=0,\n",
        "        buttons=buttons,\n",
        "        direction=\"down\",\n",
        "        showactive=True,\n",
        "        x=0.0,\n",
        "        xanchor=\"left\",\n",
        "        y=1.08,\n",
        "        yanchor=\"top\"\n",
        "    )],\n",
        "    annotations=[\n",
        "        dict(text=\"Partition:\", x=0, xref=\"paper\", y=1.06, yref=\"paper\", showarrow=False, xanchor=\"right\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=TIME_PERIOD_COLUMN, row=1, col=1)\n",
        "fig.update_xaxes(title_text=TARGET_COLUMN, row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"PREDICTED\", row=2, col=1)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f597411c-0adb-4298-a4f9-4fb4c78c1db0",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_feature_importance"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ecdf96-13a7-4646-8a61-616cd64f9f19",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_get_feature_importance"
      },
      "outputs": [],
      "source": [
        "# Load model feature important data depending on use of model context or storage table\n",
        "\n",
        "if USE_CONTEXT:\n",
        "    model_obj = mv.load(force=True).context.model_refs\n",
        "    model_data = [(\n",
        "        part, \n",
        "        dict(feature_importance=dict(\n",
        "            zip(ref.model.feature_names_in_, [float(val) for val in ref.model.feature_importances_])\n",
        "        ))\n",
        "    ) for part,ref in model_obj.items()]\n",
        "\n",
        "    model_df = pd.DataFrame(model_data,columns=[\"GROUP_IDENTIFIER_STRING\",\"METADATA\"])\n",
        "    model_df[\"MODEL_NAME\"] = MODEL_NAME\n",
        "    print(\n",
        "        f\"Feature Importances are from model version {model_version_nm} model context.\"\n",
        "    )\n",
        "else:\n",
        "    models_sdf = (\n",
        "        session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\")\n",
        "        .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
        "        .filter(\n",
        "            F.col(\"MODEL_VERSION\")\n",
        "            == reg.get_model(qualified_model_name).default.version_name\n",
        "        )\n",
        "    )\n",
        "    model_df = models_sdf.select(\n",
        "        \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"METADATA\"\n",
        "    ).to_pandas()\n",
        "    print(\n",
        "        f\"Feature Importances are for model version {reg.get_model(qualified_model_name).default.version_name} in table {MODEL_BINARY_STORAGE_TBL_NM}.\"\n",
        "    )\n",
        "\n",
        "# Filter models to given lead if using direct multistep modeling\n",
        "if LEAD > 0:\n",
        "    model_df = model_df[\n",
        "        model_df[\"GROUP_IDENTIFIER_STRING\"].str.endswith(f\"LEAD_{LEAD}\")\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a97be1-2646-4304-8a5b-55b77c13ab9d",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_plot_feature_importance"
      },
      "outputs": [],
      "source": [
        "def preprocess_model_data(df):\n",
        "    \"\"\"Preprocess model data by extracting feature importance from the METADATA column.\"\"\"\n",
        "    df[\"FEATURE_IMPORTANCE\"] = df[\"METADATA\"].apply(\n",
        "        lambda x: (\n",
        "            json.loads(x).get(\"feature_importance\", {})\n",
        "            if isinstance(x, str)\n",
        "            else x.get(\"feature_importance\", {})\n",
        "        )\n",
        "    )\n",
        "\n",
        "    feature_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        for feature, importance in row[\"FEATURE_IMPORTANCE\"].items():\n",
        "            feature_rows.append(\n",
        "                {\n",
        "                    \"MODEL_NAME\": row[\"MODEL_NAME\"],\n",
        "                    \"GROUP_IDENTIFIER_STRING\": row[\"GROUP_IDENTIFIER_STRING\"],\n",
        "                    \"FEATURE\": feature,\n",
        "                    \"IMPORTANCE\": importance,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    feature_df = pd.DataFrame(feature_rows)\n",
        "    return df, feature_df\n",
        "\n",
        "\n",
        "def calculate_average_rank(feature_df):\n",
        "    \"\"\"Calculate the average rank and importance of features across different group partitions.\"\"\"\n",
        "    feature_df = feature_df.copy()\n",
        "    feature_df.loc[:, \"RANK\"] = feature_df.groupby(\"GROUP_IDENTIFIER_STRING\")[\n",
        "        \"IMPORTANCE\"\n",
        "    ].rank(ascending=False)\n",
        "\n",
        "    avg_rank_df = (\n",
        "        feature_df.groupby(\"FEATURE\")\n",
        "        .agg({\"RANK\": \"mean\", \"IMPORTANCE\": \"mean\"})\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    avg_rank_df.rename(\n",
        "        columns={\"RANK\": \"AVERAGE_RANK\", \"IMPORTANCE\": \"AVERAGE_IMPORTANCE\"},\n",
        "        inplace=True,\n",
        "    )\n",
        "    avg_rank_df = avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True)\n",
        "    return feature_df, avg_rank_df\n",
        "\n",
        "\n",
        "model_df, feature_df = preprocess_model_data(model_df)\n",
        "partition_models = sorted(model_df[\"GROUP_IDENTIFIER_STRING\"].unique().tolist())\n",
        "\n",
        "_, avg_rank_df_all = calculate_average_rank(feature_df)\n",
        "\n",
        "top_n = 20\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "agg_df = avg_rank_df_all.head(top_n).sort_values(\"AVERAGE_RANK\", ascending=True)\n",
        "fig.add_trace(go.Bar(\n",
        "    y=agg_df[\"FEATURE\"].tolist(),\n",
        "    x=agg_df[\"AVERAGE_RANK\"].tolist(),\n",
        "    orientation=\"h\",\n",
        "    name=\"Aggregated\",\n",
        "    marker_color=\"steelblue\",\n",
        "    visible=True\n",
        "))\n",
        "\n",
        "buttons = [\n",
        "    dict(\n",
        "        label=\"Aggregated (All)\",\n",
        "        method=\"update\",\n",
        "        args=[\n",
        "            {\"y\": [agg_df[\"FEATURE\"].tolist()], \"x\": [agg_df[\"AVERAGE_RANK\"].tolist()]},\n",
        "            {\"title.text\": f\"Top {top_n} Feature Importance (Aggregated by Average Rank)\", \"xaxis.title.text\": \"Average Rank\"}\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "for partition in partition_models:\n",
        "    part_df = feature_df[feature_df[\"GROUP_IDENTIFIER_STRING\"] == partition]\n",
        "    part_df = part_df.sort_values(\"IMPORTANCE\", ascending=False).head(top_n)\n",
        "    part_df = part_df.sort_values(\"IMPORTANCE\", ascending=True)\n",
        "    \n",
        "    label = partition if len(partition) <= 35 else partition[:32] + \"...\"\n",
        "    buttons.append(dict(\n",
        "        label=label,\n",
        "        method=\"update\",\n",
        "        args=[\n",
        "            {\"y\": [part_df[\"FEATURE\"].tolist()], \"x\": [part_df[\"IMPORTANCE\"].tolist()]},\n",
        "            {\"title.text\": f\"Top {top_n} Feature Importance: {partition}\", \"xaxis.title.text\": \"Importance\"}\n",
        "        ]\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(text=f\"Top {top_n} Feature Importance (Aggregated by Average Rank)\", y=0.98, x=0.5, xanchor=\"center\"),\n",
        "    xaxis_title=\"Average Rank\",\n",
        "    yaxis_title=\"Feature\",\n",
        "    yaxis=dict(categoryorder=\"array\", categoryarray=agg_df[\"FEATURE\"].tolist()),\n",
        "    height=650,\n",
        "    margin=dict(l=200, r=50, t=120, b=50),\n",
        "    template=\"plotly_white\",\n",
        "    updatemenus=[\n",
        "        dict(\n",
        "            active=0,\n",
        "            buttons=buttons,\n",
        "            direction=\"down\",\n",
        "            showactive=True,\n",
        "            x=1.0,\n",
        "            xanchor=\"right\",\n",
        "            y=1.18,\n",
        "            yanchor=\"top\"\n",
        "        )\n",
        "    ],\n",
        "    annotations=[\n",
        "        dict(text=\"Select Partition:\", x=0.99, xref=\"paper\", y=1.15, yref=\"paper\", showarrow=False, xanchor=\"right\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(f\"\\n## Aggregated Feature Importance (Top {top_n})\")\n",
        "display(avg_rank_df_all.head(top_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bdf424e-e4c8-483e-b0e5-0dd004010c2f",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_model_promotion"
      },
      "source": [
        "# Promote Model Version?\n",
        "\n",
        "If set to True, model version evaluated in this notebook will be promoted to default and a new inference table will be created from the test data. A model monitor to track future inference results will also be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a7f372-46c8-48bc-95c1-bc9863660158",
      "metadata": {
        "language": "python",
        "name": "_set_decision_value"
      },
      "outputs": [],
      "source": [
        "PROMOTE_MODEL_VERSION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24ad5f0-3e96-4550-9a45-889c0efba434",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_promote_and_monitor"
      },
      "outputs": [],
      "source": [
        "if PROMOTE_MODEL_VERSION:\n",
        "    m = reg.get_model(qualified_model_name)\n",
        "    m.default = model_version_nm\n",
        "    print(f\"Model version {model_version_nm} promoted.\")\n",
        "    session.use_schema(MODEL_SCHEMA)\n",
        "    source_name = f\"{INFERENCE_RESULT_TBL_NM}_{MODEL_NAME}_{model_version_nm}\"\n",
        "    base_name = source_name + \"_BASELINE\"\n",
        "    table_exist = session.sql(f\"SHOW TABLES LIKE '{source_name}';\").count() > 0\n",
        "    if table_exist:\n",
        "        table_data = session.table(source_name).select(TIME_PERIOD_COLUMN,\"GROUP_IDENTIFIER_STRING\")\n",
        "        data_to_save = test_pred_v_actuals.join(table_data, on = [TIME_PERIOD_COLUMN, \"GROUP_IDENTIFIER_STRING\"], how=\"leftanti\")\n",
        "        data_to_save.drop(\"DATASET\").write.save_as_table(source_name, mode=\"append\")\n",
        "    else:\n",
        "        test_pred_v_actuals.drop(\"DATASET\").write.save_as_table(source_name, mode=\"overwrite\")\n",
        "    session.sql(f\"\"\"\n",
        "        CREATE OR REPLACE MODEL MONITOR {MODEL_NAME}_{model_version_nm}_MONITOR\n",
        "        WITH\n",
        "            MODEL={MODEL_NAME}\n",
        "            VERSION={model_version_nm}\n",
        "            FUNCTION=predict\n",
        "            SOURCE={source_name}\n",
        "            TIMESTAMP_COLUMN={TIME_PERIOD_COLUMN}\n",
        "            PREDICTION_SCORE_COLUMNS=(PREDICTED)  \n",
        "            ACTUAL_SCORE_COLUMNS=(MODEL_TARGET)\n",
        "            SEGMENT_COLUMNS = (GROUP_IDENTIFIER_STRING)\n",
        "            WAREHOUSE={SESSION_WH}\n",
        "            REFRESH_INTERVAL='1 day'\n",
        "            AGGREGATION_WINDOW='1 day';\n",
        "    \"\"\").collect()\n",
        "    print(\"Model monitor created\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "forecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "lastEditStatus": {
      "authorEmail": "allie.feras@snowflake.com",
      "authorId": "8790037420708",
      "authorName": "AFERAS",
      "lastEditTime": 1766443100328,
      "notebookId": "p5makmw26y5bvxul3p5k",
      "sessionId": "f1f8a20a-5c66-44d5-ae69-a66f3b95bd27"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
