{
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "lastEditStatus": {
   "notebookId": "mxbq4r3ie7f7cqxtwog5",
   "authorId": "",
   "authorName": "",
   "sessionId": "370b613b-1a5b-43b5-a9c8-0894c9aaa42f",
   "lastEditTime": 0
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "md_title"
   },
   "source": [
    "# Evaluation Notebook\n",
    "Use the model trained in the modeling notebook to make and evaluate predictions on a test dataset.\n",
    "\n",
    "#### NOTE: The user must have split data into train/test datasets in the modeling notebook before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "collapsed": false,
    "name": "md_instructions"
   },
   "source": [
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
    "## Instructions\n",
    "\n",
    "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
    "    - Adjust the values of the user constants\n",
    "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
    "    - The notebook will perform inference and evaluation. Predictions will be stored in a Snowflake table.\n",
    "    \n",
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_imports"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark import DataFrame as SnowparkDataFrame\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from forecast_model_builder.utils import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_establish_session"
   },
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "250001 (08001): Failed to connect to DB: SFSENORTHAMERICA-AFERAS_AWS1.snowflakecomputing.com:443. Incoming request with IP/Token 65.56.243.156 is not allowed to access Snowflake. Contact your account administrator. For more information about this error, go to https://community.snowflake.com/s/ip-xxxxxxxxxxxx-is-not-allowed-to-access.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSnowparkSessionException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/session.py:523\u001b[39m, in \u001b[36mSession.SessionBuilder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     session = \u001b[43m_get_active_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m session._conn._conn.expired:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/session.py:316\u001b[39m, in \u001b[36m_get_active_session\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_NO_DEFAULT_SESSION()\n",
      "\u001b[31mSnowparkSessionException\u001b[39m: (1403): No default Session is found. Please create a session before you call function 'udf' or use decorator '@udf'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mDatabaseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Establish session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m session = \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m session_db = session.connection.database\n\u001b[32m      4\u001b[39m session_schema = session.connection.schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/emerging-solutions-toolbox/framework-forecast-model-builder/forecast_model_builder/utils.py:57\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(connection_name)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     55\u001b[39m         session = \u001b[43mSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconnection_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_name\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.SnowparkSessionException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     59\u001b[39m         logging.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to connect: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/session.py:530\u001b[39m, in \u001b[36mSession.SessionBuilder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SnowparkClientException \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ex.error_code == \u001b[33m\"\u001b[39m\u001b[33m1403\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# No session, ok lets create one\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/session.py:508\u001b[39m, in \u001b[36mSession.SessionBuilder.create\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m     _add_session(session)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     session = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconnection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._app_name:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_json:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/session.py:550\u001b[39m, in \u001b[36mSession.SessionBuilder._create_internal\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mparamstyle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options:\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m._options[\u001b[33m\"\u001b[39m\u001b[33mparamstyle\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mqmark\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    549\u001b[39m new_session = Session(\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     ServerConnection({}, conn) \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mServerConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m._options,\n\u001b[32m    552\u001b[39m )\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28mself\u001b[39m._options[\u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:170\u001b[39m, in \u001b[36mServerConnection.__init__\u001b[39m\u001b[34m(self, options, conn)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28mself\u001b[39m._lower_case_parameters = {k.lower(): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m options.items()}\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m._add_application_parameters()\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28mself\u001b[39m._conn = conn \u001b[38;5;28;01mif\u001b[39;00m conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lower_case_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.max_string_size = DEFAULT_STRING_SIZE\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conn._session_parameters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/__init__.py:54\u001b[39m, in \u001b[36mConnect\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(SnowflakeConnection.\u001b[34m__init__\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mConnect\u001b[39m(**kwargs) -> SnowflakeConnection:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSnowflakeConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:588\u001b[39m, in \u001b[36mSnowflakeConnection.__init__\u001b[39m\u001b[34m(self, connection_name, connections_file_path, **kwargs)\u001b[39m\n\u001b[32m    586\u001b[39m     kwargs = _get_default_connection_params()\n\u001b[32m    587\u001b[39m \u001b[38;5;28mself\u001b[39m.__set_error_attributes()\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[38;5;28mself\u001b[39m._telemetry = TelemetryClient(\u001b[38;5;28mself\u001b[39m._rest)\n\u001b[32m    590\u001b[39m \u001b[38;5;28mself\u001b[39m.expired = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:972\u001b[39m, in \u001b[36mSnowflakeConnection.connect\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(exceptions_dict))\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:1371\u001b[39m, in \u001b[36mSnowflakeConnection.__open_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1364\u001b[39m     \u001b[38;5;66;03m# okta URL, e.g., https://<account>.okta.com/\u001b[39;00m\n\u001b[32m   1365\u001b[39m     \u001b[38;5;28mself\u001b[39m.auth_class = AuthByOkta(\n\u001b[32m   1366\u001b[39m         application=\u001b[38;5;28mself\u001b[39m.application,\n\u001b[32m   1367\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.login_timeout,\n\u001b[32m   1368\u001b[39m         backoff_generator=\u001b[38;5;28mself\u001b[39m._backoff_generator,\n\u001b[32m   1369\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauthenticate_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28mself\u001b[39m._password = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# ensure password won't persist\u001b[39;00m\n\u001b[32m   1374\u001b[39m \u001b[38;5;28mself\u001b[39m.auth_class.reset_secrets()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:1704\u001b[39m, in \u001b[36mSnowflakeConnection.authenticate_with_retry\u001b[39m\u001b[34m(self, auth_instance)\u001b[39m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauthenticate_with_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, auth_instance) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1702\u001b[39m     \u001b[38;5;66;03m# make some changes if needed before real __authenticate\u001b[39;00m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_authenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   1706\u001b[39m         \u001b[38;5;66;03m# cached id_token expiration error, we have cleaned id_token and try to authenticate again\u001b[39;00m\n\u001b[32m   1707\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mID token expired. Reauthenticating...: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, ex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:1736\u001b[39m, in \u001b[36mSnowflakeConnection._authenticate\u001b[39m\u001b[34m(self, auth_instance)\u001b[39m\n\u001b[32m   1734\u001b[39m auth_instance._retry_ctx.set_start_time()\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_instance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccount\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1739\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrole\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpasscode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_passcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpasscode_in_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_passcode_in_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmfa_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mfa_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassword_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_password_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1748\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1749\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OperationalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1751\u001b[39m     logger.debug(\n\u001b[32m   1752\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOperational Error raised at authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1753\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor authenticator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(auth_instance).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1754\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/auth/_auth.py:401\u001b[39m, in \u001b[36mAuth.authenticate\u001b[39m\u001b[34m(self, auth_instance, account, user, database, schema, warehouse, role, passcode, passcode_in_password, mfa_callback, password_callback, session_parameters, timeout)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(auth_instance, AuthByUsrPwdMfa):\n\u001b[32m    398\u001b[39m         \u001b[38;5;28mself\u001b[39m._delete_temporary_credential(\n\u001b[32m    399\u001b[39m             \u001b[38;5;28mself\u001b[39m._rest._host, user, TokenType.MFA_TOKEN\n\u001b[32m    400\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDatabaseError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmsg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFailed to connect to DB: \u001b[39;49m\u001b[38;5;132;43;01m{host}\u001b[39;49;00m\u001b[33;43m:\u001b[39;49m\u001b[38;5;132;43;01m{port}\u001b[39;49;00m\u001b[33;43m. \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{message}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    408\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rest\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43merrno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mER_FAILED_TO_CONNECT_TO_DB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msqlstate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSQLSTATE_CONNECTION_WAS_NOT_ESTABLISHED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    418\u001b[39m     logger.debug(\n\u001b[32m    419\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtoken = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    420\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    424\u001b[39m         ),\n\u001b[32m    425\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/errors.py:286\u001b[39m, in \u001b[36mError.errorhandler_wrapper\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merrorhandler_wrapper\u001b[39m(\n\u001b[32m    265\u001b[39m     connection: SnowflakeConnection | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    269\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m    272\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m \u001b[33;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     handed_over = \u001b[43mError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[32m    293\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Error.errorhandler_make_exception(\n\u001b[32m    294\u001b[39m             error_class,\n\u001b[32m    295\u001b[39m             error_value,\n\u001b[32m    296\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/errors.py:344\u001b[39m, in \u001b[36mError.hand_to_other_handler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/errors.py:217\u001b[39m, in \u001b[36mError.default_errorhandler\u001b[39m\u001b[34m(connection, cursor, error_class, error_value)\u001b[39m\n\u001b[32m    215\u001b[39m errno = error_value.get(\u001b[33m\"\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m done_format_msg = error_value.get(\u001b[33m\"\u001b[39m\u001b[33mdone_format_msg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[32m    218\u001b[39m     msg=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mmsg\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    219\u001b[39m     errno=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[32m    220\u001b[39m     sqlstate=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msqlstate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    221\u001b[39m     sfqid=error_value.get(\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    222\u001b[39m     query=error_value.get(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    223\u001b[39m     done_format_msg=(\n\u001b[32m    224\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[32m    225\u001b[39m     ),\n\u001b[32m    226\u001b[39m     connection=connection,\n\u001b[32m    227\u001b[39m     cursor=cursor,\n\u001b[32m    228\u001b[39m )\n",
      "\u001b[31mDatabaseError\u001b[39m: 250001 (08001): Failed to connect to DB: SFSENORTHAMERICA-AFERAS_AWS1.snowflakecomputing.com:443. Incoming request with IP/Token 65.56.243.156 is not allowed to access Snowflake. Contact your account administrator. For more information about this error, go to https://community.snowflake.com/s/ip-xxxxxxxxxxxx-is-not-allowed-to-access."
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "session = connect(connection_name=\"default\")\n",
    "session_db = session.connection.database\n",
    "session_schema = session.connection.schema\n",
    "session_wh = session.connection.warehouse\n",
    "print(f\"Session db.schema: {session_db}.{session_schema}\")\n",
    "print(f\"Session warehouse: {session_wh}\")\n",
    "\n",
    "# Query tag\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}'\n",
    "session.query_tag = query_tag\n",
    "\n",
    "# Get the current datetime  (This will be saved in the model storage table)\n",
    "run_dttm = datetime.now()\n",
    "print(f\"Current Datetime: {run_dttm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "md_USER_SETUP"
   },
   "source": [
    "-----\n",
    "# SETUP\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "language": "python",
    "name": "_set_global_variables"
   },
   "outputs": [],
   "source": [
    "# Table name to store the PREDICTION results.\n",
    "# NOTE: If the table name is not fully qualified with DB.SCHEMA, the session's default database and schema will be used.\n",
    "# NOTE: Model version will be appended to the table name to save predictions from a particular run.\n",
    "INFERENCE_RESULT_TBL_NM = \"FORECAST_RESULTS\"\n",
    "\n",
    "# Input data for inference\n",
    "INFERENCE_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "INFERENCE_SCHEMA = \"BASE\"\n",
    "INFERENCE_FV = \"FORECAST_FEATURES\"\n",
    "\n",
    "# Name of the model to use for inference, as well as the Database and Schema of the model registry.\n",
    "# NOTE: The default model version from the registry will be used.\n",
    "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "MODEL_SCHEMA = \"MODELING\"\n",
    "MODEL_NAME = \"TEST_MODEL_1\"\n",
    "\n",
    "# If using direct multistep forecasting, set LEAD to the lead model you wish to evaluate.\n",
    "# Otherwise, set to 0\n",
    "LEAD = 0\n",
    "\n",
    "# Scaling up the warehouse may speed up execution time, especially if there are many partitions.\n",
    "# NOTE: If set to None, then the session warehouse will be used.\n",
    "INFERENCE_WH = \"STANDARD_XL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "collapsed": false,
    "name": "md_objects"
   },
   "source": [
    "-----\n",
    "# Establish objects needed for this run\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_other_objects"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session warehouse:          FORECAST_MODEL_BUILDER_WH\n",
      "WARNING: User does not have access to INFERENCE_WH = 'STANDARD_XL'. Inference will use 'FORECAST_MODEL_BUILDER_WH' instead. \n",
      "\n",
      "Model Version:              YOUNG_LEECH_2\n"
     ]
    }
   ],
   "source": [
    "# Derived Objects\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Notebook Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "SESSION_WH = session.connection.warehouse\n",
    "print(f\"Session warehouse:          {SESSION_WH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Check Inference Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "# Check that the user specified an available warehouse as INFERENCE_WH. If not, use the session warehouse.\n",
    "available_warehouses = [\n",
    "    row[\"NAME\"]\n",
    "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
    "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "if INFERENCE_WH in available_warehouses:\n",
    "    print(f\"Inference warehouse:        {INFERENCE_WH} \\n\")\n",
    "else:\n",
    "    print(\n",
    "        f\"WARNING: User does not have access to INFERENCE_WH = '{INFERENCE_WH}'. Inference will use '{SESSION_WH}' instead. \\n\"\n",
    "    )\n",
    "    INFERENCE_WH = SESSION_WH\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Fully qualified MODEL NAME\n",
    "# -----------------------------------------------------------------------\n",
    "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Get the model and the version name of the default version\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish registry object\n",
    "reg = registry.Registry(\n",
    "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
    ")\n",
    "\n",
    "# Get the model from the registry\n",
    "mv = reg.get_model(qualified_model_name).default\n",
    "\n",
    "# Get the default version name\n",
    "model_version_nm = mv.version_name\n",
    "\n",
    "print(f\"Model Version:              {model_version_nm}\")\n",
    "\n",
    "# --------------------------------\n",
    "# User Constants from Model Setup\n",
    "# --------------------------------\n",
    "stored_constants = mv.show_metrics()[\"user_settings\"]\n",
    "\n",
    "TIME_PERIOD_COLUMN = stored_constants[\"TIME_PERIOD_COLUMN\"]\n",
    "TARGET_COLUMN = stored_constants[\"TARGET_COLUMN\"]\n",
    "PARTITION_COLUMNS = stored_constants[\"PARTITION_COLUMNS\"]\n",
    "EXOGENOUS_COLUMNS = stored_constants[\"EXOGENOUS_COLUMNS\"]\n",
    "ALL_EXOG_COLS_HAVE_FUTURE_VALS = stored_constants[\"ALL_EXOG_COLS_HAVE_FUTURE_VALS\"]\n",
    "CREATE_LAG_FEATURE = stored_constants[\"CREATE_LAG_FEATURE\"]\n",
    "CURRENT_FREQUENCY = stored_constants[\"CURRENT_FREQUENCY\"]\n",
    "ROLLUP_FREQUENCY = stored_constants[\"ROLLUP_FREQUENCY\"]\n",
    "ROLLUP_AGGREGATIONS = stored_constants[\"ROLLUP_AGGREGATIONS\"]\n",
    "FORECAST_HORIZON = stored_constants[\"FORECAST_HORIZON\"]\n",
    "XGB_PARAMS = stored_constants[\"XGB_PARAMS\"]\n",
    "INFERENCE_APPROX_BATCH_SIZE = stored_constants[\"INFERENCE_APPROX_BATCH_SIZE\"]\n",
    "USE_CONTEXT = stored_constants[\"USE_CONTEXT\"]\n",
    "\n",
    "if not USE_CONTEXT:\n",
    "    MODEL_BINARY_STORAGE_TBL_NM = stored_constants[\"MODEL_BINARY_STORAGE_TBL_NM\"]\n",
    "\n",
    "if (not ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD==0):\n",
    "    raise ValueError(\n",
    "        \"\"\"If using direct multistep modeling approach, LEAD must be set to a number \n",
    "        greater than 0 to filter results to a particular lead model\"\"\"\n",
    "    )\n",
    "if (ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD>0):\n",
    "    raise ValueError(\n",
    "        \"\"\"If using global modeling approach, LEAD must be set to a 0\"\"\"\n",
    "    )\n",
    "# --------------------------------\n",
    "# Get datasets\n",
    "# --------------------------------\n",
    "\n",
    "def load_df_from_ds(fully_qualified_name, version):\n",
    "    ds_db, ds_schema, ds_name = fully_qualified_name.split('.')\n",
    "\n",
    "    return Dataset(\n",
    "        session=session,\n",
    "        database=ds_db,\n",
    "        schema=ds_schema,\n",
    "        name=ds_name,\n",
    "        selected_version=version\n",
    "    ).read.to_snowpark_dataframe()\n",
    "\n",
    "train_df = load_df_from_ds(\n",
    "    fully_qualified_name=mv.show_metrics()['train_dataset']['name'],\n",
    "    version=mv.show_metrics()['train_dataset']['version']\n",
    ").drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "test_df = load_df_from_ds(\n",
    "    fully_qualified_name=mv.show_metrics()['test_dataset']['name'],\n",
    "    version=mv.show_metrics()['test_dataset']['version']\n",
    ").drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "# Filter to a particular lead model if performing direct multi step forecasting\n",
    "if LEAD > 0:\n",
    "    train_df = train_df.filter(\n",
    "            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n",
    "        )\n",
    "    test_df = test_df.filter(\n",
    "            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "collapsed": false,
    "name": "md_inference"
   },
   "source": [
    "-----\n",
    "# Inference\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_perform_inference"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "Inference input data row count: 344750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPredictions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m session.use_warehouse(INFERENCE_WH)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m train_result = \u001b[43mperform_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m.with_column(\u001b[33m\"\u001b[39m\u001b[33mDATASET\u001b[39m\u001b[33m\"\u001b[39m,F.lit(\u001b[33m\"\u001b[39m\u001b[33mTRAIN\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     67\u001b[39m test_result = perform_inference(test_df).with_column(\u001b[33m\"\u001b[39m\u001b[33mDATASET\u001b[39m\u001b[33m\"\u001b[39m,F.lit(\u001b[33m\"\u001b[39m\u001b[33mTEST\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     68\u001b[39m test_result.show(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mperform_inference\u001b[39m\u001b[34m(inference_input_df)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Look at a couple rows of the inference input data\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference input data row count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_input_df.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of end partition invocations to expect in the query profile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minference_input_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPARTITION_ID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Use the model to score the input data\u001b[39;00m\n\u001b[32m     54\u001b[39m inference_result = mv.run(inference_input_df, partition_column=\u001b[33m\"\u001b[39m\u001b[33mPARTITION_ID\u001b[39m\u001b[33m\"\u001b[39m).select(\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m_PRED_\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m     F.col(\u001b[33m\"\u001b[39m\u001b[33mGROUP_IDENTIFIER_STRING_OUT_\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mGROUP_IDENTIFIER_STRING\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     57\u001b[39m     F.col(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTIME_PERIOD_COLUMN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_OUT_\u001b[39m\u001b[33m\"\u001b[39m).alias(TIME_PERIOD_COLUMN),\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/utils.py:1120\u001b[39m, in \u001b[36mpublicapi.<locals>.call_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1116\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33m_emit_ast\u001b[39m\u001b[33m\"\u001b[39m] = is_ast_enabled()\n\u001b[32m   1118\u001b[39m \u001b[38;5;66;03m# TODO: Could modify internal docstring to display that users should not modify the _emit_ast parameter.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:4395\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self, statement_params, block, _emit_ast)\u001b[39m\n\u001b[32m   4393\u001b[39m df = \u001b[38;5;28mself\u001b[39m.agg((\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m), _emit_ast=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   4394\u001b[39m adjust_api_subcalls(df, \u001b[33m\"\u001b[39m\u001b[33mDataFrame.count\u001b[39m\u001b[33m\"\u001b[39m, len_subcalls=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4395\u001b[39m result = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_internal_collect_with_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_AsyncResultType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4399\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/telemetry.py:291\u001b[39m, in \u001b[36mdf_collect_api_telemetry.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ResourceUsageCollector() \u001b[38;5;28;01mas\u001b[39;00m resource_usage_collector:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m         result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     resource_usage = resource_usage_collector.get_resource_usage()\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/dataframe.py:828\u001b[39m, in \u001b[36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[39m\u001b[34m(self, statement_params, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[32m    816\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    817\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    826\u001b[39m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[32m    827\u001b[39m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m            \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcollect_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcollect_stacktrace_in_query_tag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    838\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:640\u001b[39m, in \u001b[36mServerConnection.execute\u001b[39m\u001b[34m(self, plan, to_pandas, to_iter, to_arrow, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    631\u001b[39m     is_in_stored_procedure()\n\u001b[32m    632\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block\n\u001b[32m   (...)\u001b[39m\u001b[32m    635\u001b[39m     )\n\u001b[32m    636\u001b[39m ):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    638\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAsync query is not supported in stored procedure yet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    639\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m result_set, result_meta = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[32m    652\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:177\u001b[39m, in \u001b[36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowpark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    172\u001b[39m     _enable_dataframe_trace_on_error,\n\u001b[32m    173\u001b[39m     _enable_trace_sql_errors_to_dataframe,\n\u001b[32m    174\u001b[39m )\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m snowflake.connector.errors.ProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msnowflake\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msnowpark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalyzer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mselect_statement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    180\u001b[39m         Selectable,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:762\u001b[39m, in \u001b[36mServerConnection.get_result_set\u001b[39m\u001b[34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, ignore_results, to_arrow, **kwargs)\u001b[39m\n\u001b[32m    760\u001b[39m     kwargs[DATAFRAME_AST_PARAMETER] = dataframe_ast\n\u001b[32m    761\u001b[39m is_final_query = i == \u001b[38;5;28mlen\u001b[39m(main_queries) - \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_post_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_final_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    778\u001b[39m placeholders[query.query_id_place_holder] = (\n\u001b[32m    779\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33msfqid\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result.query_id\n\u001b[32m    780\u001b[39m )\n\u001b[32m    781\u001b[39m result_meta = get_new_description(\u001b[38;5;28mself\u001b[39m._cursor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:130\u001b[39m, in \u001b[36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages.SERVER_SESSION_EXPIRED(\n\u001b[32m    133\u001b[39m         ex.cause\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:521\u001b[39m, in \u001b[36mServerConnection.run_query\u001b[39m\u001b[34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, to_arrow, **kwargs)\u001b[39m\n\u001b[32m    519\u001b[39m     cached_analyze_attributes.clear_cache()\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     results_cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor.sfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/ml/_internal/telemetry.py:188\u001b[39m, in \u001b[36m_StatementParamsPatchManager._patch_with_statement_params.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m context_params = \u001b[38;5;28mself\u001b[39m._context_var.get(\u001b[38;5;28mdict\u001b[39m())\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context_params:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# Exit early if not in SnowML (decorator) context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# Extract any explicitly provided statement parameters\u001b[39;00m\n\u001b[32m    191\u001b[39m orig_kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/snowpark/_internal/server_connection.py:442\u001b[39m, in \u001b[36mServerConnection.execute_and_notify_query_listener\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m         notify_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdataframe_uuid\u001b[39m\u001b[33m\"\u001b[39m] = statement_params[\u001b[33m\"\u001b[39m\u001b[33m_PLAN_UUID\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     results_cursor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    444\u001b[39m     notify_kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequestId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/cursor.py:1023\u001b[39m, in \u001b[36mSnowflakeCursor.execute\u001b[39m\u001b[34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\u001b[39m\n\u001b[32m   1016\u001b[39m     logger.debug(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquery was rewritten: org=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, new=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m query.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)),\n\u001b[32m   1019\u001b[39m         query1,\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1021\u001b[39m     query = query1\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28mself\u001b[39m._sfqid = (\n\u001b[32m   1025\u001b[39m     ret[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mqueryId\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mqueryId\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ret[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1028\u001b[39m )\n\u001b[32m   1029\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msfqid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.sfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/cursor.py:729\u001b[39m, in \u001b[36mSnowflakeCursor._execute_helper\u001b[39m\u001b[34m(self, query, timeout, statement_params, binding_params, binding_stage, is_internal, describe_only, _no_results, _is_put_get, _no_retry, dataframe_ast)\u001b[39m\n\u001b[32m    727\u001b[39m ret: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: {}}\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcmd_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sequence_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbinding_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinding_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbinding_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinding_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_file_transfer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_is_file_transfer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_internal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_internal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescribe_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescribe_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_no_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreal_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataframe_ast\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataframe_ast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/connection.py:1668\u001b[39m, in \u001b[36mSnowflakeConnection.cmd_query\u001b[39m\u001b[34m(self, sql, sequence_counter, request_id, binding_params, binding_stage, is_file_transfer, statement_params, is_internal, describe_only, _no_results, _update_current_object, _no_retry, timeout, dataframe_ast)\u001b[39m\n\u001b[32m   1659\u001b[39m     logger.debug(\n\u001b[32m   1660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msql=[\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m], sequence_id=[\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m], is_file_transfer=[\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1661\u001b[39m         \u001b[38;5;28mself\u001b[39m._format_query_for_log(data[\u001b[33m\"\u001b[39m\u001b[33msqlText\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m   1662\u001b[39m         data[\u001b[33m\"\u001b[39m\u001b[33msequenceId\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1663\u001b[39m         is_file_transfer,\n\u001b[32m   1664\u001b[39m     )\n\u001b[32m   1666\u001b[39m url_parameters = {REQUEST_ID: request_id}\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/queries/v1/query-request?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43murlencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_parameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_no_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1679\u001b[39m     ret = {\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: {}}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/network.py:461\u001b[39m, in \u001b[36mSnowflakeRestful.request\u001b[39m\u001b[34m(self, url, body, method, client, timeout, _no_results, _include_retry_params, _no_retry)\u001b[39m\n\u001b[32m    459\u001b[39m     headers[HTTP_HEADER_SERVICE_NAME] = \u001b[38;5;28mself\u001b[39m._connection.service_name\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mSnowflakeRestfulJsonEncoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_no_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_request(\n\u001b[32m    474\u001b[39m         url,\n\u001b[32m    475\u001b[39m         headers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m         timeout=timeout,\n\u001b[32m    479\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/network.py:728\u001b[39m, in \u001b[36mSnowflakeRestful._post_request\u001b[39m\u001b[34m(self, url, headers, body, token, external_session_id, timeout, socket_timeout, _no_results, no_retry, _include_retry_params)\u001b[39m\n\u001b[32m    725\u001b[39m     ret = probe_connection(full_url)\n\u001b[32m    726\u001b[39m     pprint(ret)\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_include_retry_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m logger.debug(\n\u001b[32m    741\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mret[code] = \u001b[39m\u001b[38;5;132;01m{code}\u001b[39;00m\u001b[33m, after post request\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    742\u001b[39m         code=(ret.get(\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    743\u001b[39m     )\n\u001b[32m    744\u001b[39m )\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret.get(\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m) == MASTER_TOKEN_EXPIRED_GS_CODE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/network.py:847\u001b[39m, in \u001b[36mSnowflakeRestful.fetch\u001b[39m\u001b[34m(self, method, full_url, headers, data, timeout, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m retry_ctx.set_start_time()\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_exec_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    851\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/network.py:898\u001b[39m, in \u001b[36mSnowflakeRestful._request_exec_wrapper\u001b[39m\u001b[34m(self, session, method, full_url, headers, data, retry_ctx, no_retry, token, external_session_id, **kwargs)\u001b[39m\n\u001b[32m    896\u001b[39m raise_raw_http_failure = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mraise_raw_http_failure\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     return_object = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_exec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexternal_session_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_raw_http_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    910\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m return_object\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/network.py:1083\u001b[39m, in \u001b[36mSnowflakeRestful._request_exec\u001b[39m\u001b[34m(self, session, method, full_url, headers, data, token, external_session_id, catch_okta_unauthorized_error, is_raw_text, is_raw_binary, binary_data_handler, socket_timeout, is_okta_authentication, raise_raw_http_failure)\u001b[39m\n\u001b[32m   1075\u001b[39m \u001b[38;5;66;03m# socket timeout is constant. You should be able to receive\u001b[39;00m\n\u001b[32m   1076\u001b[39m \u001b[38;5;66;03m# the response within the time. If not, ConnectReadTimeout or\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# ReadTimeout is raised.\u001b[39;00m\n\u001b[32m   1078\u001b[39m auth = (\n\u001b[32m   1079\u001b[39m     PATWithExternalSessionAuth(token, external_session_id)\n\u001b[32m   1080\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (external_session_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m SnowflakeAuth(token)\n\u001b[32m   1082\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m raw_ret = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msocket_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_raw_binary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m download_end_time = get_time_millis()\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/requests/adapters.py:485\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    482\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:715\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[39m\n\u001b[32m    712\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_proxy(conn)\n\u001b[32m    714\u001b[39m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m httplib_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[32m    726\u001b[39m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[32m    729\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:467\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    462\u001b[39m             httplib_response = conn.getresponse()\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    464\u001b[39m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    465\u001b[39m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    466\u001b[39m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m             \u001b[43msix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36mraise_from\u001b[39m\u001b[34m(value, from_value)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/connectionpool.py:462\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m         httplib_response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    464\u001b[39m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[32m    465\u001b[39m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[32m    466\u001b[39m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[32m    467\u001b[39m         six.raise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/contrib/pyopenssl.py:330\u001b[39m, in \u001b[36mWrappedSocket.recv_into\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenSSL.SSL.WantReadError:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgettimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    331\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m timeout(\u001b[33m\"\u001b[39m\u001b[33mThe read operation timed out\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/util/wait.py:145\u001b[39m, in \u001b[36mwait_for_read\u001b[39m\u001b[34m(sock, timeout)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_for_read\u001b[39m(sock, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Waits for reading to be available on a given socket.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m    Returns True if the socket is readable, or False if the timeout expired.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_for_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/util/wait.py:106\u001b[39m, in \u001b[36mpoll_wait_for_socket\u001b[39m\u001b[34m(sock, read, write, timeout)\u001b[39m\n\u001b[32m    103\u001b[39m         t *= \u001b[32m1000\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m poll_obj.poll(t)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43m_retry_on_intr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_poll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/util/wait.py:43\u001b[39m, in \u001b[36m_retry_on_intr\u001b[39m\u001b[34m(fn, timeout)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retry_on_intr\u001b[39m(fn, timeout):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/vendored/urllib3/util/wait.py:104\u001b[39m, in \u001b[36mpoll_wait_for_socket.<locals>.do_poll\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     t *= \u001b[32m1000\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpoll_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/connector/cursor.py:718\u001b[39m, in \u001b[36mSnowflakeCursor._execute_helper.<locals>.interrupt_handler\u001b[39m\u001b[34m(*_)\u001b[39m\n\u001b[32m    715\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    716\u001b[39m             \u001b[38;5;66;03m# ignore failures\u001b[39;00m\n\u001b[32m    717\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# INFERENCE\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def perform_inference(inference_input_df):\n",
    "    # If the inference dataset does not have the TARGET column already, add it and fill it with null values\n",
    "    if TARGET_COLUMN not in inference_input_df.columns:\n",
    "        inference_input_df = inference_input_df.with_column(TARGET_COLUMN, F.lit(None).cast(T.FloatType()))\n",
    "\n",
    "    if not USE_CONTEXT:\n",
    "        model_bytes_table = (\n",
    "            session.table(MODEL_BINARY_STORAGE_TBL_NM)\n",
    "            .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "            .filter(F.col(\"MODEL_VERSION\") == model_version_nm)\n",
    "            .select(\"GROUP_IDENTIFIER_STRING\", \"MODEL_BINARY\")\n",
    "        )\n",
    "\n",
    "        # NOTE: We inner joint to the model bytes table to ensure that we only try run inference on partitions that have a model.\n",
    "        inference_input_df = inference_input_df.join(\n",
    "            model_bytes_table, on=[\"GROUP_IDENTIFIER_STRING\"], how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Add a column called BATCH_GROUP,\n",
    "    #   which has the property that for each unique value there are roughly the number of records specified in batch_size.\n",
    "    # Use that to create a PARTITION_ID column that will be used to run inference in batches.\n",
    "    # We do this to avoid running out of memory when performing inference on a large number of records.\n",
    "    largest_partition_record_count = (\n",
    "        inference_input_df.group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "        .agg(F.count(\"*\").alias(\"PARTITION_RECORD_COUNT\"))\n",
    "        .agg(F.max(\"PARTITION_RECORD_COUNT\").alias(\"MAX_PARTITION_RECORD_COUNT\"))\n",
    "        .collect()[0][\"MAX_PARTITION_RECORD_COUNT\"]\n",
    "    )\n",
    "    batch_size = INFERENCE_APPROX_BATCH_SIZE\n",
    "    number_of_batches = math.ceil(largest_partition_record_count / batch_size)\n",
    "    inference_input_df = (\n",
    "        inference_input_df.with_column(\n",
    "            \"BATCH_GROUP\", F.abs(F.random(123)) % F.lit(number_of_batches)\n",
    "        )\n",
    "        .with_column(\n",
    "            \"PARTITION_ID\",\n",
    "            F.concat_ws(\n",
    "                F.lit(\"__\"), F.col(\"GROUP_IDENTIFIER_STRING\"), F.col(\"BATCH_GROUP\")\n",
    "            ),\n",
    "        )\n",
    "        .drop(\"RANDOM_NUMBER\", \"BATCH_GROUP\")\n",
    "    )\n",
    "\n",
    "    # Look at a couple rows of the inference input data\n",
    "    print(f\"Inference input data row count: {inference_input_df.count()}\")\n",
    "    print(\n",
    "        f\"Number of end partition invocations to expect in the query profile: {inference_input_df.select('PARTITION_ID').distinct().count()}\"\n",
    "    )\n",
    "    # Use the model to score the input data\n",
    "    inference_result = mv.run(inference_input_df, partition_column=\"PARTITION_ID\").select(\n",
    "        \"_PRED_\",\n",
    "        F.col(\"GROUP_IDENTIFIER_STRING_OUT_\").alias(\"GROUP_IDENTIFIER_STRING\"),\n",
    "        F.col(f\"{TIME_PERIOD_COLUMN}_OUT_\").alias(TIME_PERIOD_COLUMN),\n",
    "    )\n",
    "\n",
    "    return inference_result\n",
    "\n",
    "\n",
    "print(\"Predictions\")\n",
    "session.use_warehouse(INFERENCE_WH)\n",
    "\n",
    "train_result = perform_inference(train_df).with_column(\"DATASET\",F.lit(\"TRAIN\")).cache_result()\n",
    "test_result = perform_inference(test_df).with_column(\"DATASET\",F.lit(\"TEST\")).cache_result()\n",
    "test_result.show(2)\n",
    "\n",
    "session.use_warehouse(SESSION_WH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_write_predictions_to_table"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to table: FORECAST_MODEL_BUILDER.TEST.FORECAST_RESULTS\n",
      "Sample records:\n",
      "-----------------------------------------------------------------------------------\n",
      "|\"_PRED_\"           |\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"DATASET\"  |\n",
      "-----------------------------------------------------------------------------------\n",
      "|562.4960327148438  |STORE_ID_25_PRODUCT_ID_8   |2023-10-02 00:00:00  |TRAIN      |\n",
      "|588.8887939453125  |STORE_ID_25_PRODUCT_ID_8   |2023-07-25 00:00:00  |TRAIN      |\n",
      "|577.991455078125   |STORE_ID_25_PRODUCT_ID_8   |2023-07-26 00:00:00  |TRAIN      |\n",
      "-----------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write predictions for train and test to a Snowflake table.\n",
    "# Right now this is set up to overwrite the table if it already exists.\n",
    "\n",
    "inference_result = train_result.union_all_by_name(test_result)\n",
    "TABLE_NAME_VERSION = INFERENCE_RESULT_TBL_NM+\"_\"+MODEL_NAME+\"_\"+model_version_nm\n",
    "inference_result.write.save_as_table(\n",
    "    TABLE_NAME_VERSION,\n",
    "    mode=\"overwrite\",\n",
    "    comment='{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}',\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Predictions written to table: {session_db}.{session_schema}.{TABLE_NAME_VERSION}\"\n",
    ")\n",
    "\n",
    "# Look at a few rows of the snowflake table\n",
    "print(\"Sample records:\")\n",
    "inference_result = session.table(TABLE_NAME_VERSION)\n",
    "inference_result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_pred_v_actual",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has 250 partitions\n",
      "-------------------------------------------------------------------------------------------\n",
      "|\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"TARGET\"           |\"PREDICTED\"        |\n",
      "-------------------------------------------------------------------------------------------\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-07-30 00:00:00  |947.8779296875     |941.7086181640625  |\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-05-26 00:00:00  |887.1766967773438  |888.5808715820312  |\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-06-01 00:00:00  |880.4500122070312  |885.3113403320312  |\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predicted vs. actual dataframes for train and test\n",
    "\n",
    "sdf = train_df.union_all_by_name(test_df).select(\"GROUP_IDENTIFIER_STRING\",TIME_PERIOD_COLUMN,TARGET_COLUMN)\n",
    "\n",
    "pred_v_actuals = (\n",
    "    inference_result\n",
    "    .join(sdf, on=[\"GROUP_IDENTIFIER_STRING\", TIME_PERIOD_COLUMN])\n",
    "    .select(\n",
    "        \"GROUP_IDENTIFIER_STRING\", \n",
    "        TIME_PERIOD_COLUMN, \n",
    "        TARGET_COLUMN,\n",
    "        F.col(\"_PRED_\").alias(\"PREDICTED\"),\n",
    "        \"DATASET\"\n",
    "    )\n",
    ")\n",
    "\n",
    "inference_partition_count = pred_v_actuals.select(\"GROUP_IDENTIFIER_STRING\").distinct().count()\n",
    "\n",
    "training_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TRAIN\")\n",
    "\n",
    "test_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TEST\")\n",
    "print(f\"Dataset has {inference_partition_count} partitions\")\n",
    "pred_v_actuals.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_partition_weights",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"GROUP_IDENTIFIER_STRING\"  |\"PARTITION_TARGET_SUM\"  |\"MIN(ORDER_TIMESTAMP)\"  |\"MAX(ORDER_TIMESTAMP)\"  |\"TOTAL_TARGET\"      |\"PARTITION_WEIGHT\"     |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|STORE_ID_17_PRODUCT_ID_1   |1456821.035583496       |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00795719234225625    |\n",
      "|STORE_ID_24_PRODUCT_ID_6   |1438105.7832641602      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007854969173588749   |\n",
      "|STORE_ID_22_PRODUCT_ID_3   |1411616.508605957       |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007710284103622175   |\n",
      "|STORE_ID_21_PRODUCT_ID_9   |1404011.4907226562      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007668745308817853   |\n",
      "|STORE_ID_13_PRODUCT_ID_8   |1399447.1314697266      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00764381466626973    |\n",
      "|STORE_ID_10_PRODUCT_ID_4   |1386199.2810058594      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007571454581065291   |\n",
      "|STORE_ID_15_PRODUCT_ID_9   |1380295.8366699219      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00753920982284484    |\n",
      "|STORE_ID_7_PRODUCT_ID_5    |1377805.5510253906      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007525607821379534   |\n",
      "|STORE_ID_14_PRODUCT_ID_10  |1371382.2724609375      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007490523715812117   |\n",
      "|STORE_ID_9_PRODUCT_ID_5    |1340434.5415039062      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.0073214864478385235  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate weights of each partition for weighted metrics\n",
    "\n",
    "total_window = Window.partition_by()\n",
    "\n",
    "partition_weights = (\n",
    "    pred_v_actuals\n",
    "    .group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "    .agg(F.sum(TARGET_COLUMN).alias(f'PARTITION_{TARGET_COLUMN}_SUM'), F.min(TIME_PERIOD_COLUMN), F.max(TIME_PERIOD_COLUMN))\n",
    "    .with_column(f\"TOTAL_{TARGET_COLUMN}\", F.sum(f'PARTITION_{TARGET_COLUMN}_SUM').over(total_window))\n",
    "    .with_column(\"PARTITION_WEIGHT\", F.col(f'PARTITION_{TARGET_COLUMN}_SUM')/F.col(f\"TOTAL_{TARGET_COLUMN}\"))\n",
    ")\n",
    "\n",
    "partition_weights.sort(F.col(\"PARTITION_WEIGHT\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6211445-d2e9-432b-9608-c642609efe72",
   "metadata": {
    "collapsed": false,
    "name": "md_overall_performance"
   },
   "source": [
    "# Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_produce_metrics",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:05.523 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:05.581 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/forecast/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-13 15:44:05.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:05.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.224 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-10-13 15:44:17.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.378 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.479 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.480 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-10-13 15:44:28.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:28.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:28.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "def produce_metrics(sdf: SnowparkDataFrame) -> SnowparkDataFrame:\n",
    "    # Row-level metrics\n",
    "    row_actual_v_fcst = (\n",
    "        sdf\n",
    "        .with_column(\"PRED_ERROR\", F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
    "        .with_column(\n",
    "            \"ABS_ERROR\", F.abs(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
    "        )\n",
    "        .with_column(\n",
    "            \"APE\",\n",
    "            F.when(F.col(TARGET_COLUMN) == 0, F.lit(None)).otherwise(\n",
    "                F.abs(F.col(\"ABS_ERROR\") / F.col(TARGET_COLUMN))\n",
    "            ),\n",
    "        )\n",
    "        .with_column(\"SQ_ERROR\", F.pow(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"), 2))\n",
    "    )\n",
    "    \n",
    "    # Metrics per partition\n",
    "    partition_metrics = row_actual_v_fcst.group_by(\"GROUP_IDENTIFIER_STRING\").agg(\n",
    "        F.avg(\"APE\").alias(\"MAPE\"),\n",
    "        F.avg(\"ABS_ERROR\").alias(\"MAE\"),\n",
    "        F.sqrt(F.avg(\"SQ_ERROR\")).alias(\"RMSE\"),\n",
    "        F.count(\"*\").alias(\"TOTAL_PRED_COUNT\"),\n",
    "    )\n",
    "    \n",
    "    # Overall modeling process across all partitions\n",
    "    overall_avg_metrics = partition_metrics.agg(\n",
    "        F.avg(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "        F.avg(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "        F.avg(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    "    ).with_column(\"AGGREGATION\", F.lit(\"AVG\"))\n",
    "    \n",
    "    overall_weighted_avg_metrics = (\n",
    "        partition_metrics\n",
    "            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
    "            .agg(\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAPE\")).alias(\"OVERALL_MAPE\"),\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAE\")).alias(\"OVERALL_MAE\"),\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"RMSE\")).alias(\"OVERALL_RMSE\"),\n",
    "                 )\n",
    "            .with_column(\"AGGREGATION\", F.lit(\"WEIGHTED_AVG\"))\n",
    "    )\n",
    "    \n",
    "    overall_median_metrics = partition_metrics.agg(\n",
    "        F.median(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "        F.median(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "        F.median(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    "    ).with_column(\"AGGREGATION\", F.lit(\"MEDIAN\"))\n",
    "    \n",
    "    overall_metrics = (\n",
    "        overall_avg_metrics\n",
    "            .union(overall_median_metrics)\n",
    "            .union(overall_weighted_avg_metrics)\n",
    "            .select(\"AGGREGATION\", \"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n",
    "            .sort(\"AGGREGATION\")\n",
    "    )\n",
    "    \n",
    "    # Show the metrics\n",
    "    if inference_partition_count == 1:\n",
    "        st.write(\n",
    "            \"There is only 1 partition, so these values are the metrics for that single model:\"\n",
    "        )\n",
    "        st.dataframe(\n",
    "            overall_median_metrics.select(\"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\"),\n",
    "            use_container_width=True,\n",
    "        )\n",
    "    else:\n",
    "        st.write(\"Avg and Median of each metric over all the partitions:\")\n",
    "        st.dataframe(overall_metrics, use_container_width=True)\n",
    "\n",
    "    return row_actual_v_fcst, partition_metrics\n",
    "\n",
    "st.write(\"TRAINING SET\")\n",
    "train_metric_sdf, train_partition_metrics = produce_metrics(training_pred_v_actuals)\n",
    "st.write(\"VALIDATION SET\")\n",
    "test_metric_sdf, test_partition_metrics = produce_metrics(test_pred_v_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464115e1-dacb-4fbd-8638-6595bc802457",
   "metadata": {
    "collapsed": false,
    "name": "md_partition_performance"
   },
   "source": [
    "# Partition Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dd9d6-9a83-440e-a41c-cf58961ce3c7",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_metric_distribution",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (len(PARTITION_COLUMNS) > 0) & (inference_partition_count > 1):\n",
    "    # Metric Distribution plot with dynamic filtering\n",
    "    metric = st.selectbox(\"Metric\", [\"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\", \"GROUP_IDENTIFIER_STRING\"], key=\"metric_select_box_mnth\")\n",
    "    \n",
    "    distribution_df = test_partition_metrics.to_pandas()\n",
    "    \n",
    "    if metric not in [\"WGHT_PCT\", \"GROUP_IDENTIFIER_STRING\"]:\n",
    "        st.subheader(f\"{metric} Distribution\")\n",
    "        \n",
    "        # Add a slider to filter outliers\n",
    "        value_min, value_max = st.slider(\n",
    "            f\"Filter {metric} range in plot:\",\n",
    "            float(distribution_df[metric].min()),\n",
    "            float(distribution_df[metric].max()),\n",
    "            (float(distribution_df[metric].min()), float(distribution_df[metric].max())),\n",
    "        )\n",
    "    \n",
    "        # Filter the DataFrame based on the slider values\n",
    "        filtered_df = distribution_df[\n",
    "            (distribution_df[metric] >= value_min) & (distribution_df[metric] <= value_max)\n",
    "        ]\n",
    "    \n",
    "        fig = px.box(\n",
    "            filtered_df,\n",
    "            x=metric,  # Horizontal orientation\n",
    "            points=\"all\",  # Show individual data points as dots\n",
    "            title=f\"{metric} Distribution ({value_min:.2f} - {value_max:.2f})\",\n",
    "            labels={metric: metric, \"GROUP_IDENTIFIER_STRING\": \"Partition\"},\n",
    "            hover_data=[\"GROUP_IDENTIFIER_STRING\"],  # Add this for hover info\n",
    "        )\n",
    "\n",
    "        fig.update_layout(template=\"plotly_white\", showlegend=True)\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        # Layout with two columns\n",
    "        col1, col2 = st.columns(2)\n",
    "    \n",
    "        # Column 1: Tables\n",
    "        table_to_show = (\n",
    "            test_partition_metrics\n",
    "            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
    "            .with_column(\"WGHT_PCT\", F.col(\"PARTITION_WEIGHT\")*100)\n",
    "            .select(\"GROUP_IDENTIFIER_STRING\", \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\")\n",
    "        )\n",
    "        with col1:\n",
    "            # Look at the best performing partitions\n",
    "            st.subheader(\"BEST Performing Partitions\")\n",
    "            st.dataframe(table_to_show.sort(F.abs(metric)))\n",
    "        with col2:\n",
    "            # Look at the worst performing partitions\n",
    "            st.subheader(\"WORST Performing Partitions\")\n",
    "            st.dataframe(table_to_show.sort(F.abs(metric).desc()))\n",
    "\n",
    "    else:\n",
    "        table_to_show = (\n",
    "            test_partition_metrics\n",
    "            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
    "            .with_column(\"WGHT_PCT\", F.col(\"PARTITION_WEIGHT\")*100)\n",
    "            .select(\"GROUP_IDENTIFIER_STRING\", \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\")\n",
    "        )\n",
    "        \n",
    "        st.subheader(f\"Sorted by {metric}\")\n",
    "        st.dataframe(table_to_show.sort(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345df61d-96ed-4272-a298-e20d61acf0de",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_partition_plots",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Visualize individual partition actual vs pred on a time series line chart\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Enter partition manually instead of using selectbox\n",
    "partition_input = st.text_input(\"Enter Partition Name\", key=\"partition_selector_1\")\n",
    "load_partition = st.button(\"Load Partition\")\n",
    "\n",
    "if load_partition and partition_input.strip():\n",
    "    partition_choice = partition_input.strip()\n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    partition_choice_df = (\n",
    "        pred_v_actuals.filter(F.col(\"GROUP_IDENTIFIER_STRING\") == partition_choice)\n",
    "        .sort(TIME_PERIOD_COLUMN)\n",
    "        .to_pandas()\n",
    "    )\n",
    "    partition_choice_df[TIME_PERIOD_COLUMN] = pd.to_datetime(partition_choice_df[TIME_PERIOD_COLUMN])\n",
    "\n",
    "    tabs = st.tabs(\n",
    "        [\n",
    "            \"Line Plot: Validation Actual & Predicted\",\n",
    "            \"Scatter Plot: Validation Actual vs. Predicted\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- EDITED CODE FOR LINE PLOT ---\n",
    "    # Create a Plotly line chart\n",
    "    fig_line = px.line(\n",
    "        partition_choice_df,\n",
    "        x=TIME_PERIOD_COLUMN,\n",
    "        y=[TARGET_COLUMN, \"PREDICTED\"],\n",
    "        title=\"Validation Actual vs. Predicted\"\n",
    "    )\n",
    "\n",
    "    split_date = partition_choice_df[\n",
    "        partition_choice_df[\"DATASET\"]==\"TRAIN\"\n",
    "    ][TIME_PERIOD_COLUMN].max()\n",
    "\n",
    "    # Add a dashed vertical line at the specified date\n",
    "    fig_line.add_vline(\n",
    "        x=split_date.timestamp() * 1000,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=\"Forecast Start\",\n",
    "        annotation_position=\"top left\"\n",
    "    )\n",
    "\n",
    "    # Render the Plotly figure in Streamlit\n",
    "    tabs[0].plotly_chart(fig_line, use_container_width=True)\n",
    "   \n",
    "    # ----------------------\n",
    "    # Validation Actuals vs. Predictions Scatter Plot\n",
    "    fig_scatter = px.scatter(\n",
    "        partition_choice_df,\n",
    "        x=TARGET_COLUMN,\n",
    "        y=\"PREDICTED\",\n",
    "        title=\"Predicted vs. Actual\",\n",
    "        opacity=0.6,\n",
    "        trendline=\"ols\",\n",
    "        hover_data=[\"PREDICTED\", TARGET_COLUMN, TIME_PERIOD_COLUMN],\n",
    "    )\n",
    "\n",
    "    # Add expected trendline (y = x)\n",
    "    min_visits = min(partition_choice_df[TARGET_COLUMN])\n",
    "    max_visits = max(partition_choice_df[TARGET_COLUMN])\n",
    "\n",
    "    fig_scatter.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_visits, max_visits],\n",
    "            y=[min_visits, max_visits],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", dash=\"dash\"),\n",
    "            name=\"Expected Trend (y = x)\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tabs[1].plotly_chart(fig_scatter, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597411c-0adb-4298-a4f9-4fb4c78c1db0",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_importance"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ecdf96-13a7-4646-8a61-616cd64f9f19",
   "metadata": {
    "language": "python",
    "name": "_get_feature_importance",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances are from model version YOUNG_LEECH_2 in table MODEL_STORAGE_TEST_MODEL_1.\n"
     ]
    }
   ],
   "source": [
    "# Load model feature important data depending on use of model context or storage table\n",
    "\n",
    "if USE_CONTEXT:\n",
    "    model_obj = mv.load().context.model_refs\n",
    "    model_data = [(\n",
    "        part, \n",
    "        dict(feature_importance=dict(\n",
    "            zip(ref.model.feature_names_in_, [float(val) for val in ref.model.feature_importances_])\n",
    "        ))\n",
    "    ) for part,ref in model_obj.items()]\n",
    "\n",
    "    model_df = pd.DataFrame(model_data,columns=[\"GROUP_IDENTIFIER_STRING\",\"METADATA\"])\n",
    "    model_df[\"MODEL_NAME\"] = MODEL_NAME\n",
    "    print(\n",
    "        f\"Feature Importances are from model version {model_version_nm} model context.\"\n",
    "    )\n",
    "else:\n",
    "    models_sdf = (\n",
    "        session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\")\n",
    "        .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "        .filter(\n",
    "            F.col(\"MODEL_VERSION\")\n",
    "            == reg.get_model(qualified_model_name).default.version_name\n",
    "        )\n",
    "    )\n",
    "    model_df = models_sdf.select(\n",
    "        \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"METADATA\"\n",
    "    ).to_pandas()\n",
    "    print(\n",
    "        f\"Feature Importances are for model version {reg.get_model(qualified_model_name).default.version_name} in table {MODEL_BINARY_STORAGE_TBL_NM}.\"\n",
    "    )\n",
    "\n",
    "# Filter models to given lead if using direct multistep modeling\n",
    "if LEAD > 0:\n",
    "    model_df = model_df[\n",
    "        model_df[\"GROUP_IDENTIFIER_STRING\"].str.endswith(f\"LEAD_{LEAD}\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a97be1-2646-4304-8a5b-55b77c13ab9d",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_plot_feature_importance",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_model_data(df):\n",
    "    \"\"\"Preprocess model data by extracting feature importance from the METADATA column.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts the \"feature_importance\" dictionary from the \"METADATA\" column.\n",
    "    2. Converts the extracted feature importance data into a new DataFrame where each row\n",
    "       represents a feature and its corresponding importance for a specific model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing model data with at least\n",
    "                           the columns \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\",\n",
    "                           and \"METADATA\".\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The original DataFrame with an additional \"FEATURE_IMPORTANCE\" column.\n",
    "            - pd.DataFrame: A new DataFrame containing the extracted features and their importance,\n",
    "              with columns [\"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"FEATURE\", \"IMPORTANCE\"].\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract feature importance from METADATA\n",
    "    df[\"FEATURE_IMPORTANCE\"] = df[\"METADATA\"].apply(\n",
    "        lambda x: (\n",
    "            json.loads(x).get(\"feature_importance\", {})\n",
    "            if isinstance(x, str)\n",
    "            else x.get(\"feature_importance\", {})\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Explode feature importance into rows\n",
    "    feature_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        for feature, importance in row[\"FEATURE_IMPORTANCE\"].items():\n",
    "            feature_rows.append(\n",
    "                {\n",
    "                    \"MODEL_NAME\": row[\"MODEL_NAME\"],\n",
    "                    \"GROUP_IDENTIFIER_STRING\": row[\"GROUP_IDENTIFIER_STRING\"],\n",
    "                    \"FEATURE\": feature,\n",
    "                    \"IMPORTANCE\": importance,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    feature_df = pd.DataFrame(feature_rows)\n",
    "    return df, feature_df\n",
    "\n",
    "\n",
    "def calculate_average_rank(feature_df):\n",
    "    \"\"\"Calculate the average rank and importance of features across different group partitions.\n",
    "\n",
    "    This function:\n",
    "    1. Computes the rank of each feature within its \"GROUP_IDENTIFIER_STRING\" based on\n",
    "       feature importance in descending order.\n",
    "    2. Aggregates the average rank and average importance for each feature across all groups.\n",
    "    3. Returns the feature DataFrame with calculated ranks and a summarized DataFrame\n",
    "       sorted by average rank.\n",
    "\n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): Input DataFrame containing extracted feature importance\n",
    "                                   with at least the columns [\"GROUP_IDENTIFIER_STRING\",\n",
    "                                   \"FEATURE\", \"IMPORTANCE\"].\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The input DataFrame with an additional \"RANK\" column.\n",
    "            - pd.DataFrame: A new DataFrame containing features and their average rank and\n",
    "              importance, with columns [\"FEATURE\", \"AVERAGE_RANK\", \"AVERAGE_IMPORTANCE\"].\n",
    "\n",
    "    \"\"\"\n",
    "    feature_df = feature_df.copy()\n",
    "    feature_df.loc[:, \"RANK\"] = feature_df.groupby(\"GROUP_IDENTIFIER_STRING\")[\n",
    "        \"IMPORTANCE\"\n",
    "    ].rank(ascending=False)\n",
    "\n",
    "    avg_rank_df = (\n",
    "        feature_df.groupby(\"FEATURE\")\n",
    "        .agg({\"RANK\": \"mean\", \"IMPORTANCE\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    avg_rank_df.rename(\n",
    "        columns={\"RANK\": \"AVERAGE_RANK\", \"IMPORTANCE\": \"AVERAGE_IMPORTANCE\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    avg_rank_df = avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True)\n",
    "    return feature_df, avg_rank_df\n",
    "\n",
    "\n",
    "def plot_feature_importance(df, is_aggregated=True, top_n=20):\n",
    "    \"\"\"Create a horizontal bar plot to visualize feature importance.\n",
    "\n",
    "    This function generates a feature importance plot based on whether the data\n",
    "    is aggregated (showing average ranks across groups) or unaggregated (showing\n",
    "    importance for a selected partition).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing feature importance data.\n",
    "                           Expected columns:\n",
    "                           - If `is_aggregated=True`: [\"FEATURE\", \"AVERAGE_RANK\"]\n",
    "                           - If `is_aggregated=False`: [\"FEATURE\", \"IMPORTANCE\"]\n",
    "        is_aggregated (bool, optional): If True, plots average rank of features\n",
    "                                        across groups. If False, plots raw importance\n",
    "                                        for a single partition. Default is True.\n",
    "        top_n (int, optional): Number of top features to display in the plot.\n",
    "                               Default is 20.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: A bar plot visualizing the top feature importance.\n",
    "\n",
    "    \"\"\"\n",
    "    if is_aggregated:\n",
    "        df = df.sort_values(\"AVERAGE_RANK\", ascending=True).head(top_n)\n",
    "        x_col = \"AVERAGE_RANK\"\n",
    "        title = \"Top Feature Importance (Aggregated by Average Rank)\"\n",
    "        fig = px.bar(\n",
    "            df,\n",
    "            x=x_col,\n",
    "            y=\"FEATURE\",\n",
    "            orientation=\"h\",\n",
    "            title=title,\n",
    "            labels={\"FEATURE\": \"Feature\", x_col: \"Average Rank\"},\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(categoryorder=\"total descending\"),\n",
    "            xaxis_title=\"Average Rank\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            margin=dict(l=50, r=50, t=50, b=50),\n",
    "        )\n",
    "    else:\n",
    "        df = df.sort_values(\"IMPORTANCE\", ascending=False).head(top_n)\n",
    "        x_col = \"IMPORTANCE\"\n",
    "        title = \"Top Feature Importance for Selected Partition\"\n",
    "\n",
    "        fig = px.bar(\n",
    "            df,\n",
    "            x=x_col,\n",
    "            y=\"FEATURE\",\n",
    "            orientation=\"h\",\n",
    "            title=title,\n",
    "            labels={\"FEATURE\": \"Feature\", x_col: \"Importance\"},\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(categoryorder=\"total ascending\"),\n",
    "            xaxis_title=\"Importance\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            margin=dict(l=50, r=50, t=50, b=50),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "model_df, feature_df = preprocess_model_data(model_df)\n",
    "\n",
    "# Select Partition Model ID\n",
    "partition_models = model_df[\"GROUP_IDENTIFIER_STRING\"].unique()\n",
    "selected_partition_model = st.selectbox(\n",
    "    \"Select Partition\", [None] + sorted(partition_models)\n",
    ")\n",
    "\n",
    "# Filter data based on selections\n",
    "filtered_feature_df = feature_df\n",
    "if selected_partition_model:\n",
    "    filtered_feature_df = filtered_feature_df[\n",
    "        filtered_feature_df[\"GROUP_IDENTIFIER_STRING\"] == selected_partition_model\n",
    "    ]\n",
    "\n",
    "# Select Top N Features\n",
    "top_n = st.slider(\"Number of Top Features to Show\", min_value=5, max_value=50, value=20)\n",
    "\n",
    "\n",
    "# Display Feature Importance\n",
    "st.subheader(\"Feature Importance\")\n",
    "\n",
    "if selected_partition_model:\n",
    "    fig = plot_feature_importance(filtered_feature_df, is_aggregated=False, top_n=top_n)\n",
    "else:\n",
    "    filtered_feature_df, avg_rank_df = calculate_average_rank(filtered_feature_df)\n",
    "    fig = plot_feature_importance(avg_rank_df, is_aggregated=True, top_n=top_n)\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Expander for Underlying Data\n",
    "with st.expander(\"Show Underlying Data\"):\n",
    "    if selected_partition_model:\n",
    "        st.dataframe(filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False))\n",
    "    else:\n",
    "        tabs = st.tabs([\"Average Importance\", \"Individual Importance\"])\n",
    "        tabs[0].dataframe(avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True))\n",
    "        tabs[1].dataframe(\n",
    "            filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False)\n",
    "        )"
   ]
  }
 ]
}
