{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000000",
      "metadata": {
        "collapsed": false,
        "name": "md_title",
        "codeCollapsed": true
      },
      "source": "# Evaluation Notebook\nUse the model trained in the modeling notebook to make and evaluate predictions on a test dataset.\n\n#### NOTE: The user must have split data into train/test datasets in the modeling notebook before running this notebook."
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000001",
      "metadata": {
        "collapsed": false,
        "name": "md_instructions",
        "codeCollapsed": true
      },
      "source": "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n## Instructions\n\n1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n    - Adjust the values of the user constants\n2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n    - The notebook will perform inference and evaluation. \n    - PROMOTE_MODEL is set to false. If evaluation is satisfactory, change the value at the end of the notebook and run the last 2 cells. If model is promoted, predictions on test data will be stored in a Snowflake table and a model monitor will be create to track future inference values.\n    \n❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce110000-1111-2222-3333-ffffff000002",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_imports"
      },
      "outputs": [],
      "source": "# Imports\nimport math\nimport json\nfrom datetime import datetime\nfrom snowflake.ml.registry import registry\nfrom snowflake.ml.dataset import Dataset\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.window import Window\nfrom snowflake.snowpark import DataFrame as SnowparkDataFrame\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom forecast_model_builder.utils import connect, perform_inference"
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000004",
      "metadata": {
        "collapsed": false,
        "name": "md_USER_SETUP",
        "codeCollapsed": true
      },
      "source": "-----\n# SETUP\n-----"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce110000-1111-2222-3333-ffffff000005",
      "metadata": {
        "language": "python",
        "name": "_set_global_variables"
      },
      "outputs": [],
      "source": "# Name of project\nPROJECT_SCHEMA = \"TEST_PROJECT\"\n\n# Set warehouse\nSESSION_WH = \"FORECAST_MODEL_BUILDER_WH\"\n\n# Table name to store the PREDICTION results.\n# NOTE: If the table name is not fully qualified with DB.SCHEMA, the session's default database and schema will be used.\n# NOTE: Model version will be appended to the table name to save predictions from a particular run.\nINFERENCE_RESULT_TBL_NM = \"FORECAST_RESULTS\"\n\n# Input data for inference\nINFERENCE_DB = \"FORECAST_MODEL_BUILDER\"\nINFERENCE_SCHEMA = \"BASE\"\nINFERENCE_FV = \"FORECAST_FEATURES\"\n\n# Name of the model to use for inference, as well as the Database and Schema of the model registry.\n# NOTE: The default model version from the registry will be used.\nMODEL_DB = \"FORECAST_MODEL_BUILDER\"\nMODEL_SCHEMA = \"MODELING\"\nMODEL_NAME = \"TEST_MODEL_1\"\n\n# If using direct multistep forecasting, set LEAD to the lead model you wish to evaluate.\n# Otherwise, set to 0\nLEAD = 0\n\n# Scaling up the warehouse may speed up execution time, especially if there are many partitions.\n# NOTE: If set to None, then the session warehouse will be used.\nINFERENCE_WH = \"STANDARD_XL\""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce110000-1111-2222-3333-ffffff000003",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_establish_session"
      },
      "outputs": [],
      "source": "# Establish session\nsession = connect(connection_name=\"default\")\nsession.use_database(MODEL_DB)\nsession_db = MODEL_DB\nsession.use_schema(PROJECT_SCHEMA)\nsession_schema = PROJECT_SCHEMA\nprint(f\"Session db.schema: {session_db}.{session_schema}\")\nprint(f\"Session warehouse: {session_wh}\")\n\n# Query tag\nquery_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}'\nsession.query_tag = query_tag\n\n# Get the current datetime  (This will be saved in the model storage table)\nrun_dttm = datetime.now()\nprint(f\"Current Datetime: {run_dttm}\")"
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000006",
      "metadata": {
        "collapsed": false,
        "name": "md_objects",
        "codeCollapsed": true
      },
      "source": "-----\n# Establish objects needed for this run\n-----"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce110000-1111-2222-3333-ffffff000007",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_set_other_objects"
      },
      "outputs": [],
      "source": "# Derived Objects\n\n# -----------------------------------------------------------------------\n# Notebook Warehouse\n# -----------------------------------------------------------------------\nsession.use_warehouse(SESSION_WH)\nprint(f\"Session warehouse:          {SESSION_WH}\")\n\n# -----------------------------------------------------------------------\n# Check Inference Warehouse\n# -----------------------------------------------------------------------\n# Check that the user specified an available warehouse as INFERENCE_WH. If not, use the session warehouse.\navailable_warehouses = [\n    row[\"NAME\"]\n    for row in session.sql(\"SHOW WAREHOUSES\")\n    .select(F.col('\"name\"').alias(\"NAME\"))\n    .collect()\n]\n\nif INFERENCE_WH in available_warehouses:\n    print(f\"Inference warehouse:        {INFERENCE_WH} \\n\")\nelse:\n    print(\n        f\"WARNING: User does not have access to INFERENCE_WH = '{INFERENCE_WH}'. Inference will use '{SESSION_WH}' instead. \\n\"\n    )\n    INFERENCE_WH = SESSION_WH\n\n# -----------------------------------------------------------------------\n# Fully qualified MODEL NAME\n# -----------------------------------------------------------------------\nqualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n\n# -----------------------------------------------------------------------\n# Get the model and the version name of the default version\n# -----------------------------------------------------------------------\n# Establish registry object\nreg = registry.Registry(\n    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n)\n\n# Get the model from the registry\nmv = reg.get_model(qualified_model_name).last()\n\n# Get the default version name\nmodel_version_nm = mv.version_name\n\nprint(f\"Model Version:              {model_version_nm}\")\n\n# --------------------------------\n# User Constants from Model Setup\n# --------------------------------\nstored_constants = mv.show_metrics()[\"user_settings\"]\n\nTIME_PERIOD_COLUMN = stored_constants[\"TIME_PERIOD_COLUMN\"]\nTARGET_COLUMN = stored_constants[\"TARGET_COLUMN\"]\nPARTITION_COLUMNS = stored_constants[\"PARTITION_COLUMNS\"]\nALL_EXOG_COLS_HAVE_FUTURE_VALS = stored_constants[\"ALL_EXOG_COLS_HAVE_FUTURE_VALS\"]\nUSE_CONTEXT = stored_constants[\"USE_CONTEXT\"]\n\nif not USE_CONTEXT:\n    MODEL_BINARY_STORAGE_TBL_NM = stored_constants[\"MODEL_BINARY_STORAGE_TBL_NM\"]\n\nif (not ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD==0):\n    raise ValueError(\n        \"\"\"If using direct multistep modeling approach, LEAD must be set to a number \n        greater than 0 to filter results to a particular lead model\"\"\"\n    )\nif (ALL_EXOG_COLS_HAVE_FUTURE_VALS) & (LEAD>0):\n    raise ValueError(\n        \"\"\"If using global modeling approach, LEAD must be set to a 0\"\"\"\n    )\n# --------------------------------\n# Get datasets\n# --------------------------------\n\ndef load_df_from_ds(fully_qualified_name, version):\n    ds_db, ds_schema, ds_name = fully_qualified_name.split('.')\n\n    return Dataset(\n        session=session,\n        database=ds_db,\n        schema=ds_schema,\n        name=ds_name,\n        selected_version=version\n    ).read.to_snowpark_dataframe()\n\ntrain_df = load_df_from_ds(\n    fully_qualified_name=mv.show_metrics()['train_dataset']['name'],\n    version=mv.show_metrics()['train_dataset']['version']\n).drop(\"GROUP_IDENTIFIER\")\n\ntest_df = load_df_from_ds(\n    fully_qualified_name=mv.show_metrics()['test_dataset']['name'],\n    version=mv.show_metrics()['test_dataset']['version']\n).drop(\"GROUP_IDENTIFIER\")\n\n# Filter to a particular lead model if performing direct multi step forecasting\nif LEAD > 0:\n    train_df = train_df.filter(\n            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n        )\n    test_df = test_df.filter(\n            F.col(\"GROUP_IDENTIFIER_STRING\").endswith(f\"LEAD_{LEAD}\")\n        )"
    },
    {
      "cell_type": "markdown",
      "id": "ce110000-1111-2222-3333-ffffff000012",
      "metadata": {
        "collapsed": false,
        "name": "md_inference",
        "codeCollapsed": true
      },
      "source": "-----\n# Inference\n-----"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ce110000-1111-2222-3333-ffffff000013",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_perform_inference"
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------\n# INFERENCE\n# ------------------------------------------------------------------------\n\nprint(\"Predictions\")\nsession.use_warehouse(INFERENCE_WH)\n\ntrain_result = perform_inference(session, train_df, mv).with_column(\"DATASET\",F.lit(\"TRAIN\")).cache_result()\ntest_result = perform_inference(session, test_df, mv).with_column(\"DATASET\",F.lit(\"TEST\")).cache_result()\ntest_result.show(2)\n\n\ninference_result = train_result.union_all_by_name(test_result)\nsession.use_warehouse(SESSION_WH)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce110000-1111-2222-3333-ffffff000008",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_pred_v_actual"
      },
      "outputs": [],
      "source": "# Get predicted vs. actual dataframes for train and test\n\nsdf = train_df.union_all_by_name(test_df).select(\"GROUP_IDENTIFIER_STRING\",TIME_PERIOD_COLUMN,TARGET_COLUMN)\n\npred_v_actuals = (\n    inference_result\n    .join(sdf, on=[\"GROUP_IDENTIFIER_STRING\", TIME_PERIOD_COLUMN])\n    .select(\n        \"GROUP_IDENTIFIER_STRING\", \n        TIME_PERIOD_COLUMN, \n        TARGET_COLUMN,\n        F.col(\"_PRED_\").alias(\"PREDICTED\"),\n        \"DATASET\"\n    )\n)\n\ninference_partition_count = pred_v_actuals.select(\"GROUP_IDENTIFIER_STRING\").distinct().count()\n\ntraining_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TRAIN\")\n\ntest_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TEST\")\nprint(f\"Dataset has {inference_partition_count} partitions\")\npred_v_actuals.show(3)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ce110000-1111-2222-3333-ffffff000009",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_partition_weights"
      },
      "outputs": [],
      "source": "# Calculate weights of each partition for weighted metrics\n\ntotal_window = Window.partition_by()\n\npartition_weights = (\n    pred_v_actuals\n    .group_by(\"GROUP_IDENTIFIER_STRING\")\n    .agg(F.sum(TARGET_COLUMN).alias(f'PARTITION_{TARGET_COLUMN}_SUM'), F.min(TIME_PERIOD_COLUMN), F.max(TIME_PERIOD_COLUMN))\n    .with_column(f\"TOTAL_{TARGET_COLUMN}\", F.sum(f'PARTITION_{TARGET_COLUMN}_SUM').over(total_window))\n    .with_column(\"PARTITION_WEIGHT\", F.col(f'PARTITION_{TARGET_COLUMN}_SUM')/F.col(f\"TOTAL_{TARGET_COLUMN}\"))\n)\n\npartition_weights.sort(F.col(\"PARTITION_WEIGHT\").desc()).show()"
    },
    {
      "cell_type": "markdown",
      "id": "b6211445-d2e9-432b-9608-c642609efe72",
      "metadata": {
        "collapsed": false,
        "name": "md_overall_performance",
        "codeCollapsed": true
      },
      "source": "# Overall Performance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce110000-1111-2222-3333-ffffff000010",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_produce_metrics"
      },
      "outputs": [],
      "source": "def produce_metrics(sdf: SnowparkDataFrame) -> SnowparkDataFrame:\n    # Row-level metrics\n    row_actual_v_fcst = (\n        sdf\n        .with_column(\"PRED_ERROR\", F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n        .with_column(\n            \"ABS_ERROR\", F.abs(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n        )\n        .with_column(\n            \"APE\",\n            F.when(F.col(TARGET_COLUMN) == 0, F.lit(None)).otherwise(\n                F.abs(F.col(\"ABS_ERROR\") / F.col(TARGET_COLUMN))\n            ),\n        )\n        .with_column(\"SQ_ERROR\", F.pow(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"), 2))\n    )\n    \n    # Metrics per partition\n    partition_metrics = row_actual_v_fcst.group_by(\"GROUP_IDENTIFIER_STRING\").agg(\n        F.avg(\"APE\").alias(\"MAPE\"),\n        F.avg(\"ABS_ERROR\").alias(\"MAE\"),\n        F.sqrt(F.avg(\"SQ_ERROR\")).alias(\"RMSE\"),\n        F.count(\"*\").alias(\"TOTAL_PRED_COUNT\"),\n    )\n    \n    # Overall modeling process across all partitions\n    overall_avg_metrics = partition_metrics.agg(\n        F.avg(\"MAPE\").alias(\"OVERALL_MAPE\"),\n        F.avg(\"MAE\").alias(\"OVERALL_MAE\"),\n        F.avg(\"RMSE\").alias(\"OVERALL_RMSE\"),\n    ).with_column(\"AGGREGATION\", F.lit(\"AVG\"))\n    \n    overall_weighted_avg_metrics = (\n        partition_metrics\n            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n            .agg(\n                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAPE\")).alias(\"OVERALL_MAPE\"),\n                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAE\")).alias(\"OVERALL_MAE\"),\n                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"RMSE\")).alias(\"OVERALL_RMSE\"),\n                 )\n            .with_column(\"AGGREGATION\", F.lit(\"WEIGHTED_AVG\"))\n    )\n    \n    overall_median_metrics = partition_metrics.agg(\n        F.median(\"MAPE\").alias(\"OVERALL_MAPE\"),\n        F.median(\"MAE\").alias(\"OVERALL_MAE\"),\n        F.median(\"RMSE\").alias(\"OVERALL_RMSE\"),\n    ).with_column(\"AGGREGATION\", F.lit(\"MEDIAN\"))\n    \n    overall_metrics = (\n        overall_avg_metrics\n            .union(overall_median_metrics)\n            .union(overall_weighted_avg_metrics)\n            .select(\"AGGREGATION\", \"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n            .sort(\"AGGREGATION\")\n    )\n    \n    # Show the metrics\n    if inference_partition_count == 1:\n        print(\n            \"There is only 1 partition, so these values are the metrics for that single model:\"\n        )\n        display(\n            overall_median_metrics.select(\"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n        )\n    else:\n        print(\"Avg and Median of each metric over all the partitions:\")\n        display(overall_metrics)\n\n    return row_actual_v_fcst, partition_metrics\n\nprint(\"TRAINING SET\")\ntrain_metric_sdf, train_partition_metrics = produce_metrics(training_pred_v_actuals)\nprint(\"VALIDATION SET\")\ntest_metric_sdf, test_partition_metrics = produce_metrics(test_pred_v_actuals)"
    },
    {
      "cell_type": "markdown",
      "id": "464115e1-dacb-4fbd-8638-6595bc802457",
      "metadata": {
        "collapsed": false,
        "name": "md_partition_performance",
        "codeCollapsed": true
      },
      "source": "# Partition Performance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63dd9d6-9a83-440e-a41c-cf58961ce3c7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_metric_distribution"
      },
      "outputs": [],
      "source": "if (len(PARTITION_COLUMNS) > 0) & (inference_partition_count > 1):\n    # Metric Distribution plot - set metric parameter here\n    metric = \"MAPE\"  # Options: \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\", \"GROUP_IDENTIFIER_STRING\"\n    \n    distribution_df = test_partition_metrics.to_pandas()\n    \n    if metric not in [\"WGHT_PCT\", \"GROUP_IDENTIFIER_STRING\"]:\n        print(f\"## {metric} Distribution\")\n        \n        value_min = float(distribution_df[metric].min())\n        value_max = float(distribution_df[metric].max())\n    \n        # Filter the DataFrame based on the range values\n        filtered_df = distribution_df[\n            (distribution_df[metric] >= value_min) & (distribution_df[metric] <= value_max)\n        ]\n    \n        fig = px.box(\n            filtered_df,\n            x=metric,  # Horizontal orientation\n            points=\"all\",  # Show individual data points as dots\n            title=f\"{metric} Distribution ({value_min:.2f} - {value_max:.2f})\",\n            labels={metric: metric, \"GROUP_IDENTIFIER_STRING\": \"Partition\"},\n            hover_data=[\"GROUP_IDENTIFIER_STRING\"],  # Add this for hover info\n        )\n\n        fig.update_layout(template=\"plotly_white\", showlegend=True)\n        fig.show()\n\n        # Tables\n        table_to_show = (\n            test_partition_metrics\n            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n            .with_column(\"WGHT_PCT\", F.col(\"PARTITION_WEIGHT\")*100)\n            .select(\"GROUP_IDENTIFIER_STRING\", \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\")\n        )\n        \n        # Look at the best performing partitions\n        print(\"## BEST Performing Partitions\")\n        display(table_to_show.sort(F.abs(metric)))\n        \n        # Look at the worst performing partitions\n        print(\"## WORST Performing Partitions\")\n        display(table_to_show.sort(F.abs(metric).desc()))\n\n    else:\n        table_to_show = (\n            test_partition_metrics\n            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n            .with_column(\"WGHT_PCT\", F.col(\"PARTITION_WEIGHT\")*100)\n            .select(\"GROUP_IDENTIFIER_STRING\", \"MAPE\", \"MAE\", \"RMSE\", \"WGHT_PCT\")\n        )\n        \n        print(f\"## Sorted by {metric}\")\n        display(table_to_show.sort(metric))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345df61d-96ed-4272-a298-e20d61acf0de",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_partition_plots"
      },
      "outputs": [],
      "source": "# ------------------------------------------------------------------------------\n# Visualize individual partition actual vs pred on a time series line chart\n# ------------------------------------------------------------------------------\n\n# Enter partition manually - set partition name here\npartition_input = \"\"  # Set partition name to analyze, e.g., \"PARTITION_1\"\n\nif partition_input.strip():\n    partition_choice = partition_input.strip()\n\n    # Create a pandas dataframe\n    partition_choice_df = (\n        pred_v_actuals.filter(F.col(\"GROUP_IDENTIFIER_STRING\") == partition_choice)\n        .sort(TIME_PERIOD_COLUMN)\n        .to_pandas()\n    )\n    partition_choice_df[TIME_PERIOD_COLUMN] = pd.to_datetime(partition_choice_df[TIME_PERIOD_COLUMN])\n\n    # --- LINE PLOT ---\n    # Create a Plotly line chart\n    fig_line = px.line(\n        partition_choice_df,\n        x=TIME_PERIOD_COLUMN,\n        y=[TARGET_COLUMN, \"PREDICTED\"],\n        title=\"Validation Actual vs. Predicted\"\n    )\n\n    split_date = partition_choice_df[\n        partition_choice_df[\"DATASET\"]==\"TRAIN\"\n    ][TIME_PERIOD_COLUMN].max()\n\n    # Add a dashed vertical line at the specified date\n    fig_line.add_vline(\n        x=split_date.timestamp() * 1000,\n        line_dash=\"dash\",\n        line_color=\"red\",\n        annotation_text=\"Forecast Start\",\n        annotation_position=\"top left\"\n    )\n\n    print(\"## Line Plot: Validation Actual & Predicted\")\n    fig_line.show()\n   \n    # ----------------------\n    # Validation Actuals vs. Predictions Scatter Plot\n    fig_scatter = px.scatter(\n        partition_choice_df,\n        x=TARGET_COLUMN,\n        y=\"PREDICTED\",\n        title=\"Predicted vs. Actual\",\n        opacity=0.6,\n        trendline=\"ols\",\n        hover_data=[\"PREDICTED\", TARGET_COLUMN, TIME_PERIOD_COLUMN],\n    )\n\n    # Add expected trendline (y = x)\n    min_visits = min(partition_choice_df[TARGET_COLUMN])\n    max_visits = max(partition_choice_df[TARGET_COLUMN])\n\n    fig_scatter.add_trace(\n        go.Scatter(\n            x=[min_visits, max_visits],\n            y=[min_visits, max_visits],\n            mode=\"lines\",\n            line=dict(color=\"black\", dash=\"dash\"),\n            name=\"Expected Trend (y = x)\",\n            showlegend=True,\n        )\n    )\n\n    print(\"## Scatter Plot: Validation Actual vs. Predicted\")\n    fig_scatter.show()"
    },
    {
      "cell_type": "markdown",
      "id": "f597411c-0adb-4298-a4f9-4fb4c78c1db0",
      "metadata": {
        "collapsed": false,
        "name": "md_feature_importance",
        "codeCollapsed": true
      },
      "source": "# Feature Importance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ecdf96-13a7-4646-8a61-616cd64f9f19",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_get_feature_importance"
      },
      "outputs": [],
      "source": "# Load model feature important data depending on use of model context or storage table\n\nif USE_CONTEXT:\n    model_obj = mv.load(force=True).context.model_refs\n    model_data = [(\n        part, \n        dict(feature_importance=dict(\n            zip(ref.model.feature_names_in_, [float(val) for val in ref.model.feature_importances_])\n        ))\n    ) for part,ref in model_obj.items()]\n\n    model_df = pd.DataFrame(model_data,columns=[\"GROUP_IDENTIFIER_STRING\",\"METADATA\"])\n    model_df[\"MODEL_NAME\"] = MODEL_NAME\n    print(\n        f\"Feature Importances are from model version {model_version_nm} model context.\"\n    )\nelse:\n    models_sdf = (\n        session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\")\n        .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n        .filter(\n            F.col(\"MODEL_VERSION\")\n            == reg.get_model(qualified_model_name).default.version_name\n        )\n    )\n    model_df = models_sdf.select(\n        \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"METADATA\"\n    ).to_pandas()\n    print(\n        f\"Feature Importances are for model version {reg.get_model(qualified_model_name).default.version_name} in table {MODEL_BINARY_STORAGE_TBL_NM}.\"\n    )\n\n# Filter models to given lead if using direct multistep modeling\nif LEAD > 0:\n    model_df = model_df[\n        model_df[\"GROUP_IDENTIFIER_STRING\"].str.endswith(f\"LEAD_{LEAD}\")\n    ]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a97be1-2646-4304-8a5b-55b77c13ab9d",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_plot_feature_importance"
      },
      "outputs": [],
      "source": "def preprocess_model_data(df):\n    \"\"\"Preprocess model data by extracting feature importance from the METADATA column.\n\n    This function performs the following steps:\n    1. Extracts the \"feature_importance\" dictionary from the \"METADATA\" column.\n    2. Converts the extracted feature importance data into a new DataFrame where each row\n       represents a feature and its corresponding importance for a specific model.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing model data with at least\n                           the columns \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\",\n                           and \"METADATA\".\n\n    Returns:\n        tuple:\n            - pd.DataFrame: The original DataFrame with an additional \"FEATURE_IMPORTANCE\" column.\n            - pd.DataFrame: A new DataFrame containing the extracted features and their importance,\n              with columns [\"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"FEATURE\", \"IMPORTANCE\"].\n\n    \"\"\"\n    # Extract feature importance from METADATA\n    df[\"FEATURE_IMPORTANCE\"] = df[\"METADATA\"].apply(\n        lambda x: (\n            json.loads(x).get(\"feature_importance\", {})\n            if isinstance(x, str)\n            else x.get(\"feature_importance\", {})\n        )\n    )\n\n    # Explode feature importance into rows\n    feature_rows = []\n    for _, row in df.iterrows():\n        for feature, importance in row[\"FEATURE_IMPORTANCE\"].items():\n            feature_rows.append(\n                {\n                    \"MODEL_NAME\": row[\"MODEL_NAME\"],\n                    \"GROUP_IDENTIFIER_STRING\": row[\"GROUP_IDENTIFIER_STRING\"],\n                    \"FEATURE\": feature,\n                    \"IMPORTANCE\": importance,\n                }\n            )\n\n    feature_df = pd.DataFrame(feature_rows)\n    return df, feature_df\n\n\ndef calculate_average_rank(feature_df):\n    \"\"\"Calculate the average rank and importance of features across different group partitions.\n\n    This function:\n    1. Computes the rank of each feature within its \"GROUP_IDENTIFIER_STRING\" based on\n       feature importance in descending order.\n    2. Aggregates the average rank and average importance for each feature across all groups.\n    3. Returns the feature DataFrame with calculated ranks and a summarized DataFrame\n       sorted by average rank.\n\n    Args:\n        feature_df (pd.DataFrame): Input DataFrame containing extracted feature importance\n                                   with at least the columns [\"GROUP_IDENTIFIER_STRING\",\n                                   \"FEATURE\", \"IMPORTANCE\"].\n\n    Returns:\n        tuple:\n            - pd.DataFrame: The input DataFrame with an additional \"RANK\" column.\n            - pd.DataFrame: A new DataFrame containing features and their average rank and\n              importance, with columns [\"FEATURE\", \"AVERAGE_RANK\", \"AVERAGE_IMPORTANCE\"].\n\n    \"\"\"\n    feature_df = feature_df.copy()\n    feature_df.loc[:, \"RANK\"] = feature_df.groupby(\"GROUP_IDENTIFIER_STRING\")[\n        \"IMPORTANCE\"\n    ].rank(ascending=False)\n\n    avg_rank_df = (\n        feature_df.groupby(\"FEATURE\")\n        .agg({\"RANK\": \"mean\", \"IMPORTANCE\": \"mean\"})\n        .reset_index()\n    )\n\n    avg_rank_df.rename(\n        columns={\"RANK\": \"AVERAGE_RANK\", \"IMPORTANCE\": \"AVERAGE_IMPORTANCE\"},\n        inplace=True,\n    )\n    avg_rank_df = avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True)\n    return feature_df, avg_rank_df\n\n\ndef plot_feature_importance(df, is_aggregated=True, top_n=20):\n    \"\"\"Create a horizontal bar plot to visualize feature importance.\n\n    This function generates a feature importance plot based on whether the data\n    is aggregated (showing average ranks across groups) or unaggregated (showing\n    importance for a selected partition).\n\n    Args:\n        df (pd.DataFrame): DataFrame containing feature importance data.\n                           Expected columns:\n                           - If `is_aggregated=True`: [\"FEATURE\", \"AVERAGE_RANK\"]\n                           - If `is_aggregated=False`: [\"FEATURE\", \"IMPORTANCE\"]\n        is_aggregated (bool, optional): If True, plots average rank of features\n                                        across groups. If False, plots raw importance\n                                        for a single partition. Default is True.\n        top_n (int, optional): Number of top features to display in the plot.\n                               Default is 20.\n\n    Returns:\n        plotly.graph_objects.Figure: A bar plot visualizing the top feature importance.\n\n    \"\"\"\n    if is_aggregated:\n        df = df.sort_values(\"AVERAGE_RANK\", ascending=True).head(top_n)\n        x_col = \"AVERAGE_RANK\"\n        title = \"Top Feature Importance (Aggregated by Average Rank)\"\n        fig = px.bar(\n            df,\n            x=x_col,\n            y=\"FEATURE\",\n            orientation=\"h\",\n            title=title,\n            labels={\"FEATURE\": \"Feature\", x_col: \"Average Rank\"},\n        )\n\n        fig.update_layout(\n            yaxis=dict(categoryorder=\"total descending\"),\n            xaxis_title=\"Average Rank\",\n            yaxis_title=\"Feature\",\n            margin=dict(l=50, r=50, t=50, b=50),\n        )\n    else:\n        df = df.sort_values(\"IMPORTANCE\", ascending=False).head(top_n)\n        x_col = \"IMPORTANCE\"\n        title = \"Top Feature Importance for Selected Partition\"\n\n        fig = px.bar(\n            df,\n            x=x_col,\n            y=\"FEATURE\",\n            orientation=\"h\",\n            title=title,\n            labels={\"FEATURE\": \"Feature\", x_col: \"Importance\"},\n        )\n\n        fig.update_layout(\n            yaxis=dict(categoryorder=\"total ascending\"),\n            xaxis_title=\"Importance\",\n            yaxis_title=\"Feature\",\n            margin=dict(l=50, r=50, t=50, b=50),\n        )\n\n    return fig\n\n\n# Load and preprocess the data\nmodel_df, feature_df = preprocess_model_data(model_df)\n\n# Select Partition Model ID - set partition here or leave as None for aggregated view\npartition_models = model_df[\"GROUP_IDENTIFIER_STRING\"].unique()\nselected_partition_model = None  # Set to partition name to view individual, or None for aggregated\n\n# Filter data based on selections\nfiltered_feature_df = feature_df\nif selected_partition_model:\n    filtered_feature_df = filtered_feature_df[\n        filtered_feature_df[\"GROUP_IDENTIFIER_STRING\"] == selected_partition_model\n    ]\n\n# Select Top N Features\ntop_n = 20  # Number of top features to show (5-50)\n\n\n# Display Feature Importance\nprint(\"## Feature Importance\")\n\nif selected_partition_model:\n    fig = plot_feature_importance(filtered_feature_df, is_aggregated=False, top_n=top_n)\nelse:\n    filtered_feature_df, avg_rank_df = calculate_average_rank(filtered_feature_df)\n    fig = plot_feature_importance(avg_rank_df, is_aggregated=True, top_n=top_n)\n\nfig.show()\n\n# Show Underlying Data\nprint(\"## Underlying Data\")\nif selected_partition_model:\n    display(filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False))\nelse:\n    print(\"### Average Importance\")\n    display(avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True))\n    print(\"### Individual Importance\")\n    display(filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False))"
    },
    {
      "cell_type": "markdown",
      "id": "5bdf424e-e4c8-483e-b0e5-0dd004010c2f",
      "metadata": {
        "collapsed": false,
        "name": "md_model_promotion",
        "codeCollapsed": true
      },
      "source": "# Promote Model Version?\n\nIf set to True, model version evaluated in this notebook will be promoted to default and a new inference table will be created from the test data. A model monitor to track future inference results will also be created."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a7f372-46c8-48bc-95c1-bc9863660158",
      "metadata": {
        "language": "python",
        "name": "_set_decision_value"
      },
      "outputs": [],
      "source": "PROMOTE_MODEL_VERSION = False"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24ad5f0-3e96-4550-9a45-889c0efba434",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "language": "python",
        "name": "_promote_and_monitor"
      },
      "outputs": [],
      "source": "if PROMOTE_MODEL_VERSION:\n    m = reg.get_model(qualified_model_name)\n    m.default = model_version_nm\n    print(f\"Model version {model_version_nm} promoted.\")\n    session.use_schema(MODEL_SCHEMA)\n    source_name = f\"{INFERENCE_RESULT_TBL_NM}_{MODEL_NAME}_{model_version_nm}\"\n    base_name = source_name + \"_BASELINE\"\n    table_exist = session.sql(f\"SHOW TABLES LIKE '{source_name}';\").count() > 0\n    if table_exist:\n        table_data = session.table(source_name).select(TIME_PERIOD_COLUMN,\"GROUP_IDENTIFIER_STRING\")\n        data_to_save = test_pred_v_actuals.join(table_data, on = [TIME_PERIOD_COLUMN, \"GROUP_IDENTIFIER_STRING\"], how=\"leftanti\")\n        data_to_save.drop(\"DATASET\").write.save_as_table(source_name, mode=\"append\")\n    else:\n        test_pred_v_actuals.drop(\"DATASET\").write.save_as_table(source_name, mode=\"overwrite\")\n    session.sql(f\"\"\"\n        CREATE OR REPLACE MODEL MONITOR {MODEL_NAME}_{model_version_nm}_MONITOR\n        WITH\n            MODEL={MODEL_NAME}\n            VERSION={model_version_nm}\n            FUNCTION=predict\n            SOURCE={source_name}\n            TIMESTAMP_COLUMN={TIME_PERIOD_COLUMN}\n            PREDICTION_SCORE_COLUMNS=(PREDICTED)  \n            ACTUAL_SCORE_COLUMNS=(MODEL_TARGET)\n            SEGMENT_COLUMNS = (GROUP_IDENTIFIER_STRING)\n            WAREHOUSE={SESSION_WH}\n            REFRESH_INTERVAL='1 day'\n            AGGREGATION_WINDOW='1 day';\n    \"\"\").collect()\n    print(\"Model monitor created\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "forecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "lastEditStatus": {
      "authorEmail": "allie.feras@snowflake.com",
      "authorId": "8790037420708",
      "authorName": "AFERAS",
      "lastEditTime": 1766443100328,
      "notebookId": "p5makmw26y5bvxul3p5k",
      "sessionId": "f1f8a20a-5c66-44d5-ae69-a66f3b95bd27"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
