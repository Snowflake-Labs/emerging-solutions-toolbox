{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "md_title"
   },
   "source": [
    "# Inference Notebook\n",
    "Use the model trained in the modeling notebook to make predictions on an inference dataset.\n",
    "\n",
    "#### NOTE: The user must have an inference dataset available as a table or view in Snowflake before running this notebook.\n",
    "- If using a __direct multi-step forecasting__ pattern, the inference dataset does not need to contain records for the future datetime points.\n",
    "- If using a __global modeling__ pattern, the inference dataset must contain records for each future datetime to be forecasted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "md_instructions"
   },
   "source": [
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
    "## Instructions\n",
    "\n",
    "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
    "    - Adjust the values of the user constants\n",
    "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
    "    - The notebook will perform feature engineering steps and inference. Predictions will be stored in a Snowflake table.\n",
    "    \n",
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.window import Window\n",
    "from snowflake.snowpark import DataFrame as SnowparkDataFrame\n",
    "import streamlit as st\n",
    "\n",
    "from forecast_model_builder.feature_engineering import (\n",
    "    apply_functions_in_a_loop,\n",
    "    expand_datetime,\n",
    "    recent_rolling_avg,\n",
    "    roll_up,\n",
    "    verify_current_frequency,\n",
    "    verify_valid_rollup_spec,\n",
    ")\n",
    "from forecast_model_builder.utils import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_establish_session"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session db.schema: FORECAST_MODEL_BUILDER.TEST\n",
      "Session warehouse: FORECAST_MODEL_BUILDER_WH\n",
      "Current Datetime: 2025-10-13 14:41:15.751743\n"
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "session = connect(connection_name=\"default\")\n",
    "session_db = session.connection.database\n",
    "session_schema = session.connection.schema\n",
    "session_wh = session.connection.warehouse\n",
    "print(f\"Session db.schema: {session_db}.{session_schema}\")\n",
    "print(f\"Session warehouse: {session_wh}\")\n",
    "\n",
    "# Query tag\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}'\n",
    "session.query_tag = query_tag\n",
    "\n",
    "# Get the current datetime  (This will be saved in the model storage table)\n",
    "run_dttm = datetime.now()\n",
    "print(f\"Current Datetime: {run_dttm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "md_USER_SETUP"
   },
   "source": [
    "-----\n",
    "# SETUP\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "language": "python",
    "name": "_set_global_variables"
   },
   "outputs": [],
   "source": [
    "# Establish cutoff datetime for the records that will be scored using the forecast model.\n",
    "# NOTE: For Direct Multi-Step Forecasting, this value will likely be the most recent date, since this pattern does not take in records for future dates as input.\n",
    "#       For Global Modeling, this value will be the first future datetime value in the records that will be scored.\n",
    "# NOTE: This cutoff will be applied AFTER feature engineering, so that lag features can be calculated.\n",
    "\n",
    "# Table name to store the PREDICTION results.\n",
    "# NOTE: If the table name is not fully qualified with DB.SCHEMA, the session's default database and schema will be used.\n",
    "# NOTE: Currently the code will overwrite the existing predictions table with the predictions from this run.\n",
    "INFERENCE_RESULT_TBL_NM = \"FORECAST_RESULTS\"\n",
    "\n",
    "# Input data for inference\n",
    "INFERENCE_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "INFERENCE_SCHEMA = \"BASE\"\n",
    "INFERENCE_FV = \"FORECAST_FEATURES\"\n",
    "\n",
    "# Name of the model to use for inference, as well as the Database and Schema of the model registry.\n",
    "# NOTE: The default model version from the registry will be used.\n",
    "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "MODEL_SCHEMA = \"MODELING\"\n",
    "MODEL_NAME = \"TEST_MODEL_1\"\n",
    "\n",
    "# Scaling up the warehouse may speed up execution time, especially if there are many partitions.\n",
    "# NOTE: If set to None, then the session warehouse will be used.\n",
    "INFERENCE_WH = \"STANDARD_XL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "name": "md_objects"
   },
   "source": [
    "-----\n",
    "# Establish objects needed for this run\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_other_objects"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session warehouse:          FORECAST_MODEL_BUILDER_WH\n",
      "Inference warehouse:        FORECAST_MODEL_BUILDER_WH \n",
      "\n",
      "Model Version:              LIGHT_HOUND_1\n"
     ]
    }
   ],
   "source": [
    "# Derived Objects\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Notebook Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "SESSION_WH = session.connection.warehouse\n",
    "print(f\"Session warehouse:          {SESSION_WH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Check Inference Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "# Check that the user specified an available warehouse as INFERENCE_WH. If not, use the session warehouse.\n",
    "available_warehouses = [\n",
    "    row[\"NAME\"]\n",
    "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
    "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "if INFERENCE_WH in available_warehouses:\n",
    "    print(f\"Inference warehouse:        {INFERENCE_WH} \\n\")\n",
    "else:\n",
    "    print(\n",
    "        f\"WARNING: User does not have access to INFERENCE_WH = '{INFERENCE_WH}'. Inference will use '{SESSION_WH}' instead. \\n\"\n",
    "    )\n",
    "    INFERENCE_WH = SESSION_WH\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Fully qualified MODEL NAME\n",
    "# -----------------------------------------------------------------------\n",
    "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Get the model and the version name of the default version\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish registry object\n",
    "reg = registry.Registry(\n",
    "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
    ")\n",
    "\n",
    "# Get the model from the registry\n",
    "mv = reg.get_model(qualified_model_name).default\n",
    "\n",
    "# Get the default version name\n",
    "model_version_nm = mv.version_name\n",
    "\n",
    "print(f\"Model Version:              {model_version_nm}\")\n",
    "\n",
    "# --------------------------------\n",
    "# User Constants from Model Setup\n",
    "# --------------------------------\n",
    "stored_constants = mv.show_metrics()[\"user_settings\"]\n",
    "\n",
    "TIME_PERIOD_COLUMN = stored_constants[\"TIME_PERIOD_COLUMN\"]\n",
    "TARGET_COLUMN = stored_constants[\"TARGET_COLUMN\"]\n",
    "PARTITION_COLUMNS = stored_constants[\"PARTITION_COLUMNS\"]\n",
    "EXOGENOUS_COLUMNS = stored_constants[\"EXOGENOUS_COLUMNS\"]\n",
    "ALL_EXOG_COLS_HAVE_FUTURE_VALS = stored_constants[\"ALL_EXOG_COLS_HAVE_FUTURE_VALS\"]\n",
    "CREATE_LAG_FEATURE = stored_constants[\"CREATE_LAG_FEATURE\"]\n",
    "CURRENT_FREQUENCY = stored_constants[\"CURRENT_FREQUENCY\"]\n",
    "ROLLUP_FREQUENCY = stored_constants[\"ROLLUP_FREQUENCY\"]\n",
    "ROLLUP_AGGREGATIONS = stored_constants[\"ROLLUP_AGGREGATIONS\"]\n",
    "FORECAST_HORIZON = stored_constants[\"FORECAST_HORIZON\"]\n",
    "INFERENCE_APPROX_BATCH_SIZE = stored_constants[\"INFERENCE_APPROX_BATCH_SIZE\"]\n",
    "USE_CONTEXT = stored_constants[\"USE_CONTEXT\"]\n",
    "\n",
    "# --------------------------------\n",
    "# Get datasets\n",
    "# --------------------------------\n",
    "\n",
    "def load_df_from_ds(fully_qualified_name, version):\n",
    "    ds_db, ds_schema, ds_name = fully_qualified_name.split('.')\n",
    "\n",
    "    return Dataset(\n",
    "        session=session,\n",
    "        database=ds_db,\n",
    "        schema=ds_schema,\n",
    "        name=ds_name,\n",
    "        selected_version=version\n",
    "    ).read.to_snowpark_dataframe()\n",
    "\n",
    "train_df = load_df_from_ds(\n",
    "    fully_qualified_name=mv.show_metrics()['train_dataset']['name'],\n",
    "    version=mv.show_metrics()['train_dataset']['version']\n",
    ").drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "test_df = load_df_from_ds(\n",
    "    fully_qualified_name=mv.show_metrics()['test_dataset']['name'],\n",
    "    version=mv.show_metrics()['test_dataset']['version']\n",
    ").drop(\"GROUP_IDENTIFIER\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "name": "md_inference"
   },
   "source": [
    "-----\n",
    "# Inference\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_perform_inference"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "Inference input data row count: 344750\n",
      "Number of end partition invocations to expect in the query profile: 1750\n",
      "Inference input data row count: 22750\n",
      "Number of end partition invocations to expect in the query profile: 250\n",
      "------------------------------------------------------------------------------------\n",
      "|\"_PRED_\"            |\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"DATASET\"  |\n",
      "------------------------------------------------------------------------------------\n",
      "|145.43472290039062  |STORE_ID_6_PRODUCT_ID_6    |2024-12-06 00:00:00  |TEST       |\n",
      "|141.02732849121094  |STORE_ID_6_PRODUCT_ID_6    |2024-12-17 00:00:00  |TEST       |\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# INFERENCE\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def perform_inference(inference_input_df):\n",
    "    # If the inference dataset does not have the TARGET column already, add it and fill it with null values\n",
    "    if TARGET_COLUMN not in inference_input_df.columns:\n",
    "        inference_input_df = inference_input_df.with_column(TARGET_COLUMN, F.lit(None).cast(T.FloatType()))\n",
    "\n",
    "    if not USE_CONTEXT:\n",
    "        MODEL_BINARY_STORAGE_TBL_NM = stored_constants[\"MODEL_BINARY_STORAGE_TBL_NM\"]\n",
    "        model_bytes_table = (\n",
    "            session.table(MODEL_BINARY_STORAGE_TBL_NM)\n",
    "            .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "            .filter(F.col(\"MODEL_VERSION\") == model_version_nm)\n",
    "            .select(\"GROUP_IDENTIFIER_STRING\", \"MODEL_BINARY\")\n",
    "        )\n",
    "\n",
    "        # NOTE: We inner joint to the model bytes table to ensure that we only try run inference on partitions that have a model.\n",
    "        inference_input_df = inference_input_df.join(\n",
    "            model_bytes_table, on=[\"GROUP_IDENTIFIER_STRING\"], how=\"inner\"\n",
    "        )\n",
    "\n",
    "    # Add a column called BATCH_GROUP,\n",
    "    #   which has the property that for each unique value there are roughly the number of records specified in batch_size.\n",
    "    # Use that to create a PARTITION_ID column that will be used to run inference in batches.\n",
    "    # We do this to avoid running out of memory when performing inference on a large number of records.\n",
    "    largest_partition_record_count = (\n",
    "        inference_input_df.group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "        .agg(F.count(\"*\").alias(\"PARTITION_RECORD_COUNT\"))\n",
    "        .agg(F.max(\"PARTITION_RECORD_COUNT\").alias(\"MAX_PARTITION_RECORD_COUNT\"))\n",
    "        .collect()[0][\"MAX_PARTITION_RECORD_COUNT\"]\n",
    "    )\n",
    "    batch_size = INFERENCE_APPROX_BATCH_SIZE\n",
    "    number_of_batches = math.ceil(largest_partition_record_count / batch_size)\n",
    "    inference_input_df = (\n",
    "        inference_input_df.with_column(\n",
    "            \"BATCH_GROUP\", F.abs(F.random(123)) % F.lit(number_of_batches)\n",
    "        )\n",
    "        .with_column(\n",
    "            \"PARTITION_ID\",\n",
    "            F.concat_ws(\n",
    "                F.lit(\"__\"), F.col(\"GROUP_IDENTIFIER_STRING\"), F.col(\"BATCH_GROUP\")\n",
    "            ),\n",
    "        )\n",
    "        .drop(\"RANDOM_NUMBER\", \"BATCH_GROUP\")\n",
    "    )\n",
    "\n",
    "    # Look at a couple rows of the inference input data\n",
    "    print(f\"Inference input data row count: {inference_input_df.count()}\")\n",
    "    print(\n",
    "        f\"Number of end partition invocations to expect in the query profile: {inference_input_df.select('PARTITION_ID').distinct().count()}\"\n",
    "    )\n",
    "    # Use the model to score the input data\n",
    "    inference_result = mv.run(inference_input_df, partition_column=\"PARTITION_ID\").select(\n",
    "        \"_PRED_\",\n",
    "        F.col(\"GROUP_IDENTIFIER_STRING_OUT_\").alias(\"GROUP_IDENTIFIER_STRING\"),\n",
    "        F.col(f\"{TIME_PERIOD_COLUMN}_OUT_\").alias(TIME_PERIOD_COLUMN),\n",
    "    )\n",
    "\n",
    "    return inference_result\n",
    "\n",
    "\n",
    "print(\"Predictions\")\n",
    "session.use_warehouse(INFERENCE_WH)\n",
    "\n",
    "train_result = perform_inference(train_df).with_column(\"DATASET\",F.lit(\"TRAIN\"))\n",
    "test_result = perform_inference(test_df).with_column(\"DATASET\",F.lit(\"TEST\"))\n",
    "test_result.show(2)\n",
    "\n",
    "session.use_warehouse(SESSION_WH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_write_predictions_to_table"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions written to table: FORECAST_MODEL_BUILDER.TEST.FORECAST_RESULTS\n",
      "Sample records:\n",
      "-----------------------------------------------------------------------------------\n",
      "|\"_PRED_\"           |\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"DATASET\"  |\n",
      "-----------------------------------------------------------------------------------\n",
      "|562.4960327148438  |STORE_ID_25_PRODUCT_ID_8   |2023-10-02 00:00:00  |TRAIN      |\n",
      "|588.8887939453125  |STORE_ID_25_PRODUCT_ID_8   |2023-07-25 00:00:00  |TRAIN      |\n",
      "|577.991455078125   |STORE_ID_25_PRODUCT_ID_8   |2023-07-26 00:00:00  |TRAIN      |\n",
      "-----------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write predictions to a Snowflake table.\n",
    "# Right now this is set up to overwrite the table if it already exists.\n",
    "\n",
    "inference_result = train_result.union_all_by_name(test_result)\n",
    "inference_result.write.save_as_table(\n",
    "    INFERENCE_RESULT_TBL_NM,\n",
    "    mode=\"overwrite\",\n",
    "    comment='{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"inference\"}}',\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Predictions written to table: {session_db}.{session_schema}.{INFERENCE_RESULT_TBL_NM}\"\n",
    ")\n",
    "\n",
    "# Look at a few rows of the snowflake table\n",
    "print(\"Sample records:\")\n",
    "inference_result = session.table(INFERENCE_RESULT_TBL_NM)\n",
    "inference_result.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset has 250 partitions\n",
      "-------------------------------------------------------------------------------------------\n",
      "|\"GROUP_IDENTIFIER_STRING\"  |\"ORDER_TIMESTAMP\"    |\"TARGET\"           |\"PREDICTED\"        |\n",
      "-------------------------------------------------------------------------------------------\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-07-30 00:00:00  |947.8779296875     |941.7086181640625  |\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-05-26 00:00:00  |887.1766967773438  |888.5808715820312  |\n",
      "|STORE_ID_9_PRODUCT_ID_8    |2023-06-01 00:00:00  |880.4500122070312  |885.3113403320312  |\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = train_df.union_all_by_name(test_df).select(\"GROUP_IDENTIFIER_STRING\",TIME_PERIOD_COLUMN,TARGET_COLUMN)\n",
    "\n",
    "pred_v_actuals = (\n",
    "    inference_result\n",
    "    .join(sdf, on=[\"GROUP_IDENTIFIER_STRING\", TIME_PERIOD_COLUMN])\n",
    "    .select(\n",
    "        \"GROUP_IDENTIFIER_STRING\", \n",
    "        TIME_PERIOD_COLUMN, \n",
    "        TARGET_COLUMN,\n",
    "        F.col(\"_PRED_\").alias(\"PREDICTED\")\n",
    "    )\n",
    ")\n",
    "\n",
    "inference_partition_count = pred_v_actuals.select(\"GROUP_IDENTIFIER_STRING\").distinct().count()\n",
    "\n",
    "training_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TRAIN\")\n",
    "\n",
    "test_pred_v_actuals = pred_v_actuals.filter(F.col(\"DATASET\")==\"TEST\")\n",
    "print(f\"Dataset has {inference_partition_count} partitions\")\n",
    "pred_v_actuals.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"GROUP_IDENTIFIER_STRING\"  |\"PARTITION_TARGET_SUM\"  |\"MIN(ORDER_TIMESTAMP)\"  |\"MAX(ORDER_TIMESTAMP)\"  |\"TOTAL_TARGET\"      |\"PARTITION_WEIGHT\"     |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|STORE_ID_17_PRODUCT_ID_1   |1456821.035583496       |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00795719234225625    |\n",
      "|STORE_ID_24_PRODUCT_ID_6   |1438105.7832641602      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007854969173588749   |\n",
      "|STORE_ID_22_PRODUCT_ID_3   |1411616.508605957       |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007710284103622175   |\n",
      "|STORE_ID_21_PRODUCT_ID_9   |1404011.4907226562      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007668745308817853   |\n",
      "|STORE_ID_13_PRODUCT_ID_8   |1399447.1314697266      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00764381466626973    |\n",
      "|STORE_ID_10_PRODUCT_ID_4   |1386199.2810058594      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007571454581065291   |\n",
      "|STORE_ID_15_PRODUCT_ID_9   |1380295.8366699219      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.00753920982284484    |\n",
      "|STORE_ID_7_PRODUCT_ID_5    |1377805.5510253906      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007525607821379534   |\n",
      "|STORE_ID_14_PRODUCT_ID_10  |1371382.2724609375      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.007490523715812117   |\n",
      "|STORE_ID_9_PRODUCT_ID_5    |1340434.5415039062      |2021-01-01 00:00:00     |2025-01-09 00:00:00     |183082294.97571963  |0.0073214864478385235  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_window = Window.partition_by()\n",
    "\n",
    "partition_weights = (\n",
    "    pred_v_actuals\n",
    "    .group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "    .agg(F.sum(TARGET_COLUMN).alias(f'PARTITION_{TARGET_COLUMN}_SUM'), F.min(TIME_PERIOD_COLUMN), F.max(TIME_PERIOD_COLUMN))\n",
    "    .with_column(f\"TOTAL_{TARGET_COLUMN}\", F.sum(f'PARTITION_{TARGET_COLUMN}_SUM').over(total_window))\n",
    "    .with_column(\"PARTITION_WEIGHT\", F.col(f'PARTITION_{TARGET_COLUMN}_SUM')/F.col(f\"TOTAL_{TARGET_COLUMN}\"))\n",
    ")\n",
    "\n",
    "partition_weights.sort(F.col(\"PARTITION_WEIGHT\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 15:44:05.523 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:05.581 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/forecast/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-13 15:44:05.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:05.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.222 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.223 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:07.224 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-10-13 15:44:17.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:17.378 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.479 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:18.480 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2025-10-13 15:44:28.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:28.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-13 15:44:28.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "def produce_metrics(sdf: SnowparkDataFrame) -> SnowparkDataFrame:\n",
    "    # Row-level metrics\n",
    "    row_actual_v_fcst = (\n",
    "        sdf\n",
    "        .with_column(\"PRED_ERROR\", F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
    "        .with_column(\n",
    "            \"ABS_ERROR\", F.abs(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"))\n",
    "        )\n",
    "        .with_column(\n",
    "            \"APE\",\n",
    "            F.when(F.col(TARGET_COLUMN) == 0, F.lit(None)).otherwise(\n",
    "                F.abs(F.col(\"ABS_ERROR\") / F.col(TARGET_COLUMN))\n",
    "            ),\n",
    "        )\n",
    "        .with_column(\"SQ_ERROR\", F.pow(F.col(TARGET_COLUMN) - F.col(\"PREDICTED\"), 2))\n",
    "    )\n",
    "    \n",
    "    # Metrics per partition\n",
    "    partition_metrics = row_actual_v_fcst.group_by(\"GROUP_IDENTIFIER_STRING\").agg(\n",
    "        F.avg(\"APE\").alias(\"MAPE\"),\n",
    "        F.avg(\"ABS_ERROR\").alias(\"MAE\"),\n",
    "        F.sqrt(F.avg(\"SQ_ERROR\")).alias(\"RMSE\"),\n",
    "        F.count(\"*\").alias(\"TOTAL_PRED_COUNT\"),\n",
    "    )\n",
    "    \n",
    "    # Overall modeling process across all partitions\n",
    "    overall_avg_metrics = partition_metrics.agg(\n",
    "        F.avg(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "        F.avg(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "        F.avg(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    "    ).with_column(\"AGGREGATION\", F.lit(\"AVG\"))\n",
    "    \n",
    "    overall_weighted_avg_metrics = (\n",
    "        partition_metrics\n",
    "            .join(partition_weights.select(\"GROUP_IDENTIFIER_STRING\", \"PARTITION_WEIGHT\"), on=[\"GROUP_IDENTIFIER_STRING\"])\n",
    "            .agg(\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAPE\")).alias(\"OVERALL_MAPE\"),\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"MAE\")).alias(\"OVERALL_MAE\"),\n",
    "                F.sum(F.col(\"PARTITION_WEIGHT\")*F.col(\"RMSE\")).alias(\"OVERALL_RMSE\"),\n",
    "                 )\n",
    "            .with_column(\"AGGREGATION\", F.lit(\"WEIGHTED_AVG\"))\n",
    "    )\n",
    "    \n",
    "    overall_median_metrics = partition_metrics.agg(\n",
    "        F.median(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "        F.median(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "        F.median(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    "    ).with_column(\"AGGREGATION\", F.lit(\"MEDIAN\"))\n",
    "    \n",
    "    overall_metrics = (\n",
    "        overall_avg_metrics\n",
    "            .union(overall_median_metrics)\n",
    "            .union(overall_weighted_avg_metrics)\n",
    "            .select(\"AGGREGATION\", \"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\")\n",
    "            .sort(\"AGGREGATION\")\n",
    "    )\n",
    "    \n",
    "    # Show the metrics\n",
    "    if inference_partition_count == 1:\n",
    "        st.write(\n",
    "            \"There is only 1 partition, so these values are the metrics for that single model:\"\n",
    "        )\n",
    "        st.dataframe(\n",
    "            overall_median_metrics.select(\"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\"),\n",
    "            use_container_width=True,\n",
    "        )\n",
    "    else:\n",
    "        st.write(\"Avg and Median of each metric over all the partitions:\")\n",
    "        st.dataframe(overall_metrics, use_container_width=True)\n",
    "\n",
    "    return row_actual_v_fcst, partition_metrics\n",
    "\n",
    "st.write(\"TRAINING SET\")\n",
    "train_metric_sdf, train_partition_metrics = produce_metrics(training_pred_v_actuals)\n",
    "st.write(\"VALIDATION SET\")\n",
    "test_metric_sdf, test_partition_metrics = produce_metrics(test_pred_v_actuals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "lastEditStatus": {
   "authorEmail": "kirk.mason@snowflake.com",
   "authorId": "340032177737",
   "authorName": "KMASON",
   "lastEditTime": 1745508327584,
   "notebookId": "gks23kx33gb5rjz54brg",
   "sessionId": "92cfcca6-675e-421b-a33e-6e491cd3179a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
