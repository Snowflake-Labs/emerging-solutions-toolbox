{
 "metadata": {
  "kernelspec": {
   "display_name": "forecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "lastEditStatus": {
   "notebookId": "amae6zxododd2zh3iny2",
   "authorId": "",
   "authorName": "",
   "sessionId": "2b7499ee-7822-4160-a6a6-43706cc3772e",
   "lastEditTime": 0
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "md_title"
   },
   "source": [
    "# Partitioned Time Series Modeling\n",
    "\n",
    "This notebook can be used to train a time series forecasting model. \n",
    "\n",
    "It is especially useful for use cases in which multiple series need to be trained in parallel. For example, if a retailer needs to build a separate model for each individual store location, this code will train those models in parallel. This greatly improves run time, especially in cases involving a large number of partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2c7aa-6f5d-4afb-a9f4-ad3e55ad983b",
   "metadata": {
    "collapsed": false,
    "name": "md_instructions"
   },
   "source": [
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
    "\n",
    "__Prerequisites before running this notebook:__ \n",
    "\n",
    "- The time series data in Snowflake must have at least the following columns: __date or timestamp__ column, __target__ column, and __partition__ column(s) if multi-series. \n",
    "- The date column name MUST be in [unquoted identifier](https://docs.snowflake.com/en/sql-reference/identifiers-syntax#label-unquoted-identifier) format, i.e. contains only __upper case letters, underscores, and decimal digits__. It is __recommended__ that all other column names also be in that format so that [double-quoted identifiers](https://docs.snowflake.com/en/sql-reference/identifiers-syntax#label-delimited-identifier) are not needed.\n",
    "- The target column (and any exogenous feature columns if they exist) should have values in a numeric format like FLOAT, DOUBLE, or INT. \n",
    "- Any null values in the data should already be imputed. \n",
    "\n",
    "## Instructions\n",
    "\n",
    "\n",
    "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
    "    - Change the values of the user constants to match the specifications of the use case.\n",
    "    - Descriptions of each value are written in that cell.\n",
    "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
    "    - The notebook will perform feature engineering and will train models. \n",
    "    - If ___SAVE_MODEL_VERSION_THIS_RUN=True___, then the models will be saved to the model registry for later inference. \n",
    "    \n",
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_imports"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import importlib.metadata\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "import pkgutil\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.dataset import Dataset\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode,\n",
    ")\n",
    "\n",
    "from forecast_model_builder.feature_engineering import (\n",
    "    apply_functions_in_a_loop,\n",
    "    expand_datetime,\n",
    "    recent_rolling_avg,\n",
    "    roll_up,\n",
    "    verify_current_frequency,\n",
    "    verify_valid_rollup_spec,\n",
    ")\n",
    "from forecast_model_builder.utils import (\n",
    "    connect,\n",
    "    version_featureview,\n",
    "    version_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_establish_session"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session db.schema: FORECAST_MODEL_BUILDER.TEST\n",
      "Current Datetime: 2025-10-15 14:40:50.100966\n"
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "session = connect(connection_name=\"default\")\n",
    "session_db = session.connection.database\n",
    "session_schema = session.connection.schema\n",
    "session_db_schema = f\"{session_db}.{session_schema}\"\n",
    "print(f\"Session db.schema: {session_db_schema}\")\n",
    "\n",
    "# Query tag\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"modeling\"}}'\n",
    "session.query_tag = query_tag\n",
    "\n",
    "# Get the current datetime  (This will be saved in the model storage table)\n",
    "run_dttm = datetime.now()\n",
    "print(f\"Current Datetime: {run_dttm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "collapsed": false,
    "name": "md_USER_SETUP"
   },
   "source": [
    "-----\n",
    "# SETUP\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_set_global_variables"
   },
   "outputs": [],
   "source": [
    "# SET GLOBAL VARIABLES FOR THIS RUN\n",
    "\n",
    "# Name the model (if model already exists, a new version will be created)\n",
    "MODEL_NAME = \"TEST_MODEL_1\"\n",
    "\n",
    "# Boolean that is True if we want to save the model version in the current run.\n",
    "# Users may want to set it to false while they experiment with different specificiations, and then set to True when they develop the model they want to save.\n",
    "SAVE_MODEL_VERSION_THIS_RUN = True\n",
    "\n",
    "# --------------------------------\n",
    "# Input Time Series Data\n",
    "# --------------------------------\n",
    "# Establish the Snowflake database, schema, and table containing the time series data\n",
    "TS_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "TS_SCHEMA = \"BASE\"\n",
    "TS_TABLE_NM = \"DAILY_PARTITIONED_SAMPLE_DATA\"\n",
    "\n",
    "# --------------------------------\n",
    "# Modeling setup\n",
    "# --------------------------------\n",
    "# Establish the Database and Schema that will be used to store the models\n",
    "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "MODEL_SCHEMA = \"MODELING\"\n",
    "\n",
    "# --------------------------------\n",
    "# Virtual Warehouse\n",
    "# --------------------------------\n",
    "# For modeling and inference, a larger warehouse may speed up execution time depending on the number of partitions.\n",
    "# Scale up if there are a lot of partitions.\n",
    "# NOTE: If set to None, then the session warehouse will be used.\n",
    "MODELING_WH = \"STANDARD_XL\"\n",
    "\n",
    "# --------------------------------\n",
    "# Modeling\n",
    "# --------------------------------\n",
    "# From the time series data (TS_TABLE_NM), specify the name of column containing the datetime information\n",
    "TIME_PERIOD_COLUMN = \"ORDER_TIMESTAMP\"\n",
    "\n",
    "# NOTE: For the next 3 constants, if column names require a double-quoted identifier, include double quotes within the single quotes.\n",
    "#       Examples: '\"Target\"', ['\"store id\"', '\"product id\"'], ['\"Feature 1\"'].\n",
    "\n",
    "# Name of column containing the target variable (i.e. the value we are trying to predict)\n",
    "TARGET_COLUMN = \"TARGET\"\n",
    "\n",
    "# List of column names to use as partition columns. This is how you define each individual series to be modeled.\n",
    "# If modeling a single series (i.e. no partitions) set this as an EMPTY LIST [].\n",
    "PARTITION_COLUMNS = [\"STORE_ID\", \"PRODUCT_ID\"]\n",
    "\n",
    "# List of column names in the time series table to use as EXOGENOUS FEATURES.\n",
    "# Exogenous features are variables outside the main time series that can impact future values of the target variable.\n",
    "#     Examples: weather features, promotions, holidays, economic indicators (like inflation), inventory on hand, etc.\n",
    "# If there are no exogenous features in the data set, set this as an EMPTY LIST [].\n",
    "# NOTE: This notebook will create several features (like YEAR, MONTH, DAY_OF_YEAR, etc). You do NOT need to list those.\n",
    "#       Only list features that are already in the Snowflake table (TS_TABLE_NM).\n",
    "EXOGENOUS_COLUMNS = [\"FEATURE_1\"]\n",
    "\n",
    "# ALL_EXOG_COLS_HAVE_FUTURE_VALS is a boolean that is True if all exogenous features have future values present in the inference data.\n",
    "#     For example, if you are predicting 56 days into the future (i.e. FORECAST_HORIZON=56),\n",
    "#                  but you only know promotions for the next 4 weeks, you would set ALL_EXOG_COLS_HAVE_FUTURE_VALS = False.\n",
    "# NOTE: There are two modeling patterns in this notebook:\n",
    "#       1. Direct Multi-Step Forecasting - If ALL_EXOG_COLS_HAVE_FUTURE_VALS = False,\n",
    "#                                           the code will create separate models for each lead/step (from step = 1 to step = FORECAST_HORIZON) within each partition.\n",
    "#                                           In this pattern, inference is done using the most current date's information to predict each future step.\n",
    "#       2. Global Modeling               - If ALL_EXOG_COLS_HAVE_FUTURE_VALS = True (or EXOGENOUS_COLUMNS is empty),\n",
    "#                                           the code will train a single model within each partition.\n",
    "#                                           In this pattern, inference is done using the information for each future step,\n",
    "#                                               so the inference dataset will need a separate record for each future date to be predicted.\n",
    "# This variable will determine which pattern is used.\n",
    "ALL_EXOG_COLS_HAVE_FUTURE_VALS = True\n",
    "\n",
    "# Specify if we should create lag features for the target variable (including avgs of previous periods). This will affect the lag_and_target_prep & recent_rolling_avg functions.\n",
    "# NOTE: If we are using the Global Modeling pattern, we will not create recent rolling avg features.\n",
    "CREATE_LAG_FEATURE = False\n",
    "\n",
    "# Frequency of the data (choose from: \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"other\")\n",
    "# This is the frequency of the data as it currently exists in the Snowflake table (TS_TABLE_NM).\n",
    "# If it is not a standard frequency, select \"other\"\n",
    "CURRENT_FREQUENCY = \"day\"\n",
    "\n",
    "# Frequency to roll up to (choose from: \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", or None)\n",
    "# If you do not wish to roll up to a higher level, set ROLLUP_FREQUENCY=None.\n",
    "ROLLUP_FREQUENCY = None\n",
    "\n",
    "# Specify how each column should agg on roll-up (choose from: \"sum\", \"avg\", \"min\", or \"max\")\n",
    "# NOTE: If rollup_frequency is not None, then this can be an empty dictionary {}.\n",
    "#       Otherwise, you must specify an aggregation for the TARGET column AND for each of the EXOGENOUS_COLUMNS.\n",
    "ROLLUP_AGGREGATIONS = {\n",
    "    TARGET_COLUMN: \"sum\",\n",
    "    \"FEATURE_1\": \"sum\",\n",
    "}\n",
    "\n",
    "# Forecast Horizon. Number of time periods to forecast into the future (UNITS will be that of the ROLLUP_FREQUENCY if specified, otherwise CURRENT_FREQUENCY).\n",
    "# NOTE: Keep this number as small as possible if doing Direct Multi-Step Forecasting (in which a separate model gets built for each future time period).\n",
    "FORECAST_HORIZON = 7\n",
    "\n",
    "# Specify how many days to set aside for validation.\n",
    "# NOTE: If this is set to 0, then the model will be trained on all historic data.\n",
    "# Setting aside testing data is required to run the subsequent evaluation notebook.\n",
    "VALIDATION_DAYS = 90\n",
    "\n",
    "# XGBRegressor hyperparameter selections.\n",
    "# NOTE: This notebook does not perform hyperparameter tuning, so you can set these parameters here if you know which values you would like to use.\n",
    "XGB_PARAMS = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.80,\n",
    "    \"colsample_bytree\": 0.80,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# --------------------------------\n",
    "# Inference\n",
    "# --------------------------------\n",
    "# When distributing the inference records, we can set the batch size here.\n",
    "# If the number is too high, inference on a large number of records might use up all available memory.\n",
    "INFERENCE_APPROX_BATCH_SIZE = 200\n",
    "\n",
    "# --------------------------------\n",
    "# Calculated Constants\n",
    "# --------------------------------\n",
    "# Model context can currently only accept a maximum of 1000 models. Solutions with > 1000 models will use a model storage table instead.\n",
    "MODELCONTEXT_MAX = 1000\n",
    "# Establish the name of the table that will hold model binaries.\n",
    "# This will be a Snowflake table in your project schema if the number of models is > MODELCONTEXT_MAX\n",
    "MODEL_BINARY_STORAGE_TBL_NM = f\"MODEL_STORAGE_{MODEL_NAME}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "collapsed": false,
    "name": "md_objects"
   },
   "source": [
    "-----\n",
    "# Establish objects needed for this run\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_other_objects"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session warehouse:          FORECAST_MODEL_BUILDER_WH\n",
      "WARNING: User does not have access to MODELING_WH = 'STANDARD_XL'. Model training will use 'FORECAST_MODEL_BUILDER_WH' instead. \n",
      "\n",
      "Modeling Frequency:         day\n",
      "Train Separate Lead Models: False\n",
      "Model FORECAST_MODEL_BUILDER.MODELING.TEST_MODEL_1 already exists. This notebook will build a new version.\n"
     ]
    }
   ],
   "source": [
    "# DERIVED OBJECTS\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Notebook Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "SESSION_WH = session.connection.warehouse\n",
    "print(f\"Session warehouse:          {SESSION_WH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Check Modeling Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "# Check that the user specified an available warehouse as MODELING_WH. If not, use the session warehouse.\n",
    "available_warehouses = [\n",
    "    row[\"NAME\"]\n",
    "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
    "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "if MODELING_WH in available_warehouses:\n",
    "    print(f\"Modeling warehouse:         {MODELING_WH} \\n\")\n",
    "else:\n",
    "    print(\n",
    "        f\"WARNING: User does not have access to MODELING_WH = '{MODELING_WH}'. Model training will use '{SESSION_WH}' instead. \\n\"\n",
    "    )\n",
    "    MODELING_WH = SESSION_WH\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Fully qualified MODEL NAME\n",
    "# -----------------------------------------------------------------------\n",
    "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create dictionary of user settings to log with the model\n",
    "# -----------------------------------------------------------------------\n",
    "user_settings_dict = {\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"SAVE_MODEL_VERSION_THIS_RUN\": SAVE_MODEL_VERSION_THIS_RUN,\n",
    "    \"TS_DB\": TS_DB,\n",
    "    \"TS_SCHEMA\": TS_SCHEMA,\n",
    "    \"TS_TABLE_NM\": TS_TABLE_NM,\n",
    "    \"MODEL_DB\": MODEL_DB,\n",
    "    \"MODEL_SCHEMA\": MODEL_SCHEMA,\n",
    "    \"SESSION_WH\": SESSION_WH,\n",
    "    \"MODELING_WH\": MODELING_WH,\n",
    "    \"TIME_PERIOD_COLUMN\": TIME_PERIOD_COLUMN,\n",
    "    \"PARTITION_COLUMNS\": PARTITION_COLUMNS,\n",
    "    \"EXOGENOUS_COLUMNS\": EXOGENOUS_COLUMNS,\n",
    "    \"ALL_EXOG_COLS_HAVE_FUTURE_VALS\": ALL_EXOG_COLS_HAVE_FUTURE_VALS,\n",
    "    \"CREATE_LAG_FEATURE\": CREATE_LAG_FEATURE,\n",
    "    \"CURRENT_FREQUENCY\": CURRENT_FREQUENCY,\n",
    "    \"ROLLUP_FREQUENCY\": ROLLUP_FREQUENCY,\n",
    "    \"ROLLUP_AGGREGATIONS\": ROLLUP_AGGREGATIONS,\n",
    "    \"FORECAST_HORIZON\": FORECAST_HORIZON,\n",
    "    \"VALIDATION_DAYS\": VALIDATION_DAYS,\n",
    "    \"XGB_PARAMS\": XGB_PARAMS,\n",
    "    \"INFERENCE_APPROX_BATCH_SIZE\": INFERENCE_APPROX_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# BACKEND SETUP: Create Model Schema\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a schema to hold our models if it does not already exist\n",
    "schema_exists = (\n",
    "    session.table(f\"{MODEL_DB}.INFORMATION_SCHEMA.SCHEMATA\")\n",
    "    .filter(F.upper(F.col(\"SCHEMA_NAME\")) == F.upper(F.lit(MODEL_SCHEMA)))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if schema_exists == 0:\n",
    "    try:\n",
    "        session.sql(f\"create schema if not exists {MODEL_DB}.{MODEL_SCHEMA}\").collect()\n",
    "    except Exception as e:\n",
    "        if \"insufficient privileges\" in str(e).lower():\n",
    "            raise PermissionError(f\"\"\"Schema {MODEL_SCHEMA} does not already exist in {MODEL_DB}, and user does not have sufficient privileges to CREATE SCHEMA. \n",
    "            Please specify an existing schema for MODEL_SCHEMA constant.\"\"\") from e\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"An error occurred while attempting to create schema {MODEL_DB}.{MODEL_SCHEMA}: {e}\"\n",
    "            ) from e\n",
    "\n",
    "# Reset the schema to the original session schema. (If we created a new schema, the session schema was set to the new schema)\n",
    "session.use_schema(session_db_schema)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a window spec\n",
    "# -----------------------------------------------------------------------\n",
    "window_spec = Window.partitionBy(PARTITION_COLUMNS).orderBy(TIME_PERIOD_COLUMN)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a variable for the frequency at which we will be modeling\n",
    "# -----------------------------------------------------------------------\n",
    "CURRENT_FREQUENCY = CURRENT_FREQUENCY.lower()\n",
    "\n",
    "if ROLLUP_FREQUENCY is not None:\n",
    "    ROLLUP_FREQUENCY = ROLLUP_FREQUENCY.lower()\n",
    "    if ROLLUP_FREQUENCY.lower() == \"none\":\n",
    "        ROLLUP_FREQUENCY = None\n",
    "\n",
    "modeling_frequency = CURRENT_FREQUENCY if ROLLUP_FREQUENCY is None else ROLLUP_FREQUENCY\n",
    "print(f\"Modeling Frequency:         {modeling_frequency}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Varible for modeling pattern\n",
    "# -----------------------------------------------------------------------\n",
    "# Either (1) train_separate_lead_models = False : all features have future values in the inference data, so we don't need a separate model for each lead\n",
    "# or (2) train_separate_lead_models = True : data contains exogenous variables that the inference data won't have future values for, requiring direct multi-step (lead) modeling\n",
    "train_separate_lead_models = (\n",
    "    False\n",
    "    if ALL_EXOG_COLS_HAVE_FUTURE_VALS is True or len(EXOGENOUS_COLUMNS) == 0\n",
    "    else True\n",
    ")\n",
    "print(f\"Train Separate Lead Models: {train_separate_lead_models}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish model registry object\n",
    "# -----------------------------------------------------------------------\n",
    "reg = registry.Registry(\n",
    "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Does model already exist in the registry?\n",
    "# -----------------------------------------------------------------------\n",
    "try:\n",
    "    number_of_versions = len(reg.get_model(qualified_model_name).show_versions())\n",
    "    if number_of_versions > 0:\n",
    "        print(\n",
    "            f\"Model {qualified_model_name} already exists. This notebook will build a new version.\"\n",
    "        )\n",
    "except Exception:\n",
    "    print(f\"This will be the first version of model {qualified_model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a880469-60ae-4ea6-977d-3e24cdc84678",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_get_data",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Snowpark DataFrame from table in Snowflake\n",
    "sdf = session.table(f\"{TS_DB}.{TS_SCHEMA}.{TS_TABLE_NM}\")\n",
    "\n",
    "# Only keep the columns specified in the config\n",
    "sdf = sdf.select(\n",
    "    TIME_PERIOD_COLUMN, TARGET_COLUMN, *PARTITION_COLUMNS, *EXOGENOUS_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_prelim_checks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common time between consecutive records (frequency): 1.0 day(s)\n",
      "    The current frequency appears to be in DAY granularity.\n",
      "    The range of values is 1.0 - 1.0 day (s)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Preliminary checks\n",
    "# -----------------------------------------\n",
    "# Verify valid rollup specification.\n",
    "# Raise an error if the user specifies a rollup frequency that is finer grain than the current frequency\n",
    "# Raise an error if the user does not specify a rollup aggregation for the target and all exogenous columns\n",
    "verify_valid_rollup_spec(\n",
    "    CURRENT_FREQUENCY, ROLLUP_FREQUENCY, ROLLUP_AGGREGATIONS, EXOGENOUS_COLUMNS\n",
    ")\n",
    "\n",
    "# Roughly verify the current frequency (datetime difference between consecutive records) of the time series data\n",
    "# Note the existence of gaps if range is anything other than 1 - 1\n",
    "verify_current_frequency(sdf, TIME_PERIOD_COLUMN, window_spec, CURRENT_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "_handling_gaps_sample",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: while the sample data in this repo has no gaps, real data may require inserting missing dates\n",
    "# and filling with zero (or other relevant imputed value, depending on the business problem).\n",
    "# To maintain compatibility with incremental feature view refresh, consider using a calendar table cross join\n",
    "\n",
    "# Sample code:\n",
    "\n",
    "\"\"\"\n",
    "START_DATE = (\n",
    "    sdf.select(F.min(TIME_PERIOD_COLUMN).alias(TIME_PERIOD_COLUMN))\n",
    "    .collect()[0][TIME_PERIOD_COLUMN]\n",
    ")\n",
    "\n",
    "cal = (\n",
    "    session.table(f\"{TS_DB}.{TS_SCHEMA}.{CAL_TABLE_NM}\")\n",
    "    .select(F.col(CAL_DATE_COLUMN).alias(TIME_PERIOD_COLUMN))\n",
    ")\n",
    "\n",
    "time_series = (\n",
    "    cal.filter(F.col(TIME_PERIOD_COLUMN)>=START_DATE)\n",
    "    .join(sdf.select(PARTITION_COLUMNS).distinct(), how='cross')\n",
    ")\n",
    "\n",
    "sdf = (\n",
    "    time_series.join(sdf, on=PARTITION_COLUMNS+[TIME_PERIOD_COLUMN], how='left')\n",
    "    .fillna({TARGET_COLUMN:0})\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_engineering"
   },
   "source": [
    "-----\n",
    "# Feature Engineering\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_feature_engineering"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total record count after rolling up:   367500\n",
      "Total record count of final data:      367500\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_TIMESTAMP\"    |\"TARGET\"       |\"FEATURE_1\"    |\"YEAR\"  |\"MONTH_SIN\"         |\"MONTH_COS\"          |\"WEEK_OF_YEAR_SIN\"  |\"WEEK_OF_YEAR_COS\"   |\"DAY_OF_WEEK_SUN\"  |\"DAY_OF_WEEK_MON\"  |\"DAY_OF_WEEK_TUE\"  |\"DAY_OF_WEEK_WED\"  |\"DAY_OF_WEEK_THU\"  |\"DAY_OF_WEEK_FRI\"  |\"DAY_OF_WEEK_SAT\"  |\"DAY_OF_YEAR_SIN\"   |\"DAY_OF_YEAR_COS\"    |\"DAYS_SINCE_JAN2020\"  |\"MODEL_TARGET\"  |\"GROUP_IDENTIFIER\"  |\"GROUP_IDENTIFIER_STRING\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2022-05-14 00:00:00  |306.213257512  |320.129119915  |2022    |0.5000000000000003  |-0.8660254037844385  |0.748510748171101   |-0.6631226582407953  |0                  |0                  |0                  |0                  |0                  |0                  |1                  |0.7412220108485956  |-0.6712599575675317  |864                   |306.213257512   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "|2022-05-15 00:00:00  |325.224079529  |452.846319917  |2022    |0.5000000000000003  |-0.8660254037844385  |0.748510748171101   |-0.6631226582407953  |1                  |0                  |0                  |0                  |0                  |0                  |0                  |0.7295575540864875  |-0.6839194216246106  |865                   |325.224079529   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First Convert Decimal data types to Floats (because DecimalType doesn't work in modeling algorithms)\n",
    "sdf_converted = sdf.select(\n",
    "    [\n",
    "        (\n",
    "            F.col(field.name).cast(T.FloatType()).alias(field.name)\n",
    "            if isinstance(field.datatype, T.DecimalType)\n",
    "            else F.col(field.name)\n",
    "        )\n",
    "        for field in sdf.schema\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ROLL UP to specified frequency\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_rollup = roll_up(\n",
    "    sdf_converted,\n",
    "    TIME_PERIOD_COLUMN,\n",
    "    PARTITION_COLUMNS,\n",
    "    TARGET_COLUMN,\n",
    "    EXOGENOUS_COLUMNS,\n",
    "    ROLLUP_FREQUENCY,\n",
    "    ROLLUP_AGGREGATIONS,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create time-derived features\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_engineered = expand_datetime(sdf_rollup, TIME_PERIOD_COLUMN, modeling_frequency)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create rolling average of most recent time periods\n",
    "# ------------------------------------------------------------------------\n",
    "# NOTE: We can only generate recent rolling average features if we are training separate lead models (direct multi-step forecasting).\n",
    "if CREATE_LAG_FEATURE & train_separate_lead_models:\n",
    "    sdf_engineered = recent_rolling_avg(\n",
    "        sdf_engineered, [TARGET_COLUMN], window_spec, modeling_frequency\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create LAG features (and possibly LEAD feature) of the TARGET variable\n",
    "# ------------------------------------------------------------------------\n",
    "final_sdf = apply_functions_in_a_loop(\n",
    "    train_separate_lead_models=train_separate_lead_models,\n",
    "    partition_column_list=PARTITION_COLUMNS,\n",
    "    input_sdf=sdf_engineered,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    time_step_frequency=modeling_frequency,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    w_spec=window_spec,\n",
    "    create_lag_feature=CREATE_LAG_FEATURE,\n",
    ")\n",
    "\n",
    "# Inspect data\n",
    "print(f\"Total record count after rolling up:   {sdf_rollup.count()}\")\n",
    "print(f\"Total record count of final data:      {final_sdf.count()}\")\n",
    "final_sdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b42d9-5c84-49ad-b545-8cdd0908d8ac",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_store"
   },
   "source": [
    "-----\n",
    "# Feature Store\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c2df3c-9a0f-4a0c-ac9e-1846a42c79f4",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_feature_store_entities",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/ml/feature_store/feature_store.py:189: UserWarning: Entity TS_PARTITION_ENTITY already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Entity(name=TS_PARTITION_ENTITY, join_keys=['GROUP_IDENTIFIER_STRING'], owner=None, desc=)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create or retrieve feature store based on session database and schema\n",
    "fs = FeatureStore(\n",
    "    session,\n",
    "    database=session_db,\n",
    "    name=session_schema,\n",
    "    default_warehouse=SESSION_WH,\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")\n",
    "\n",
    "# Create and register entity based on partition column.\n",
    "# If entity already exists, registration will be skipped.\n",
    "entity = Entity(\n",
    "    name=\"TS_PARTITION_ENTITY\",\n",
    "    join_keys=[\"GROUP_IDENTIFIER_STRING\"],\n",
    ")\n",
    "\n",
    "fs.register_entity(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa20fb6-1a2a-4d83-aa60-a5f70f4cea47",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_feature_view",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'online_config' is in private preview since 1.12.0. Do not use it in production.\n",
      "/opt/anaconda3/envs/forecast/lib/python3.12/site-packages/snowflake/ml/feature_store/feature_store.py:546: UserWarning: FeatureView FORECAST_FEATURES/7F60099EF9D475EAD08DA791CDF493AD already exists. Skip registration. Set `overwrite` to True if you want to replace existing FeatureView.\n",
      "  return self._get_feature_view_if_exists(feature_view.name, str(version))\n"
     ]
    }
   ],
   "source": [
    "# Create feature view based on dataframe engineered above\n",
    "# This allows the same logic to be applied to training and testing data,\n",
    "# keeping features up to date on an incrementally refreshed schedule\n",
    "fv = FeatureView(\n",
    "    name=\"FORECAST_FEATURES\",\n",
    "    entities=[entity],\n",
    "    feature_df=final_sdf,\n",
    "    timestamp_col=TIME_PERIOD_COLUMN,\n",
    "    refresh_freq=\"1 days\",\n",
    "    refresh_mode=\"INCREMENTAL\",\n",
    ")\n",
    "\n",
    "# Automatically versions the feature view definition (not the data itself)\n",
    "# If there are changes to the feature view (ex. a different transformation of the dataframe)\n",
    "# the version will change. If there are no changes, the same version will be returned, and \n",
    "# feature view registration will be skipped, returning the existing feature view.\n",
    "version = version_featureview(fv)\n",
    "\n",
    "fv_reg = fs.register_feature_view(fv, version=version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "collapsed": false,
    "name": "md_train_test_split"
   },
   "source": [
    "-----\n",
    "# TRAIN/TEST SPLIT\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_train_split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 344750\n",
      "First time period in training set: 2021-01-01 00:00:00\n",
      "Last time period in training set:  2024-10-10 00:00:00\n",
      "Total Partition Count: 250\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_TIMESTAMP\"    |\"TARGET\"       |\"FEATURE_1\"    |\"YEAR\"  |\"MONTH_SIN\"  |\"MONTH_COS\"              |\"WEEK_OF_YEAR_SIN\"  |\"WEEK_OF_YEAR_COS\"   |\"DAY_OF_WEEK_SUN\"  |\"DAY_OF_WEEK_MON\"  |\"DAY_OF_WEEK_TUE\"  |\"DAY_OF_WEEK_WED\"  |\"DAY_OF_WEEK_THU\"  |\"DAY_OF_WEEK_FRI\"  |\"DAY_OF_WEEK_SAT\"  |\"DAY_OF_YEAR_SIN\"   |\"DAY_OF_YEAR_COS\"    |\"DAYS_SINCE_JAN2020\"  |\"MODEL_TARGET\"  |\"GROUP_IDENTIFIER\"  |\"GROUP_IDENTIFIER_STRING\"  |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2021-03-24 00:00:00  |371.575949     |176.58401403   |2021    |1.0          |-3.8285686989269494e-16  |0.992708874098054   |0.12053668025532323  |0                  |0                  |0                  |1                  |0                  |0                  |0                  |0.989932495087353   |0.141540295217043    |448                   |371.575949      |{                   |STORE_ID_3_PRODUCT_ID_2    |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 2,  |                           |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 3     |                           |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "|2021-03-25 00:00:00  |357.405921565  |685.394387188  |2021    |1.0          |-3.8285686989269494e-16  |0.992708874098054   |0.12053668025532323  |0                  |0                  |0                  |0                  |1                  |0                  |0                  |0.9922222094179323  |0.12447926388678915  |449                   |357.405921565   |{                   |STORE_ID_3_PRODUCT_ID_2    |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 2,  |                           |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 3     |                           |\n",
      "|                     |               |               |        |             |                         |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN/TEST SPLIT\n",
    "\n",
    "sdf_fv = fs.read_feature_view(fv_reg).cache_result()\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "if VALIDATION_DAYS == 0:\n",
    "    sdf_train = sdf_fv\n",
    "elif VALIDATION_DAYS > 0:\n",
    "    # Get the last time period in the dataset\n",
    "    last_time_period = sdf_fv.select(\n",
    "        F.max(TIME_PERIOD_COLUMN).alias(\"MAX_DTTM\")\n",
    "    ).collect()[0][\"MAX_DTTM\"]\n",
    "    # Remove the validation records from the training set\n",
    "    sdf_train = sdf_fv.filter(\n",
    "        F.date_trunc(\"day\", TIME_PERIOD_COLUMN)\n",
    "        < F.dateadd(\"day\", F.lit(-VALIDATION_DAYS), F.lit(last_time_period))\n",
    "    )\n",
    "    sdf_test = sdf_fv.filter(\n",
    "        F.date_trunc(\"day\", TIME_PERIOD_COLUMN)\n",
    "        >= F.dateadd(\"day\", F.lit(-VALIDATION_DAYS), F.lit(last_time_period))\n",
    "    )\n",
    "\n",
    "# Inspect the data\n",
    "training_dttm_boundaries = sdf_train.select(\n",
    "    F.min(TIME_PERIOD_COLUMN).alias(\"MIN_DTTM\"),\n",
    "    F.max(TIME_PERIOD_COLUMN).alias(\"MAX_DTTM\"),\n",
    ").collect()[0]\n",
    "print(f\"Training set row count: {sdf_train.count()}\")\n",
    "print(f\"First time period in training set: {training_dttm_boundaries['MIN_DTTM']}\")\n",
    "print(f\"Last time period in training set:  {training_dttm_boundaries['MAX_DTTM']}\")\n",
    "if len(PARTITION_COLUMNS) > 0:\n",
    "    print(\n",
    "        f\"Total Partition Count: {sdf_train.select(F.get(F.split('GROUP_IDENTIFIER_STRING',F.lit('_LEAD')),0)).distinct().count()}\"\n",
    "    )\n",
    "    model_count = sdf_train.select('GROUP_IDENTIFIER_STRING').distinct().count()\n",
    "    USE_CONTEXT = model_count <= MODELCONTEXT_MAX\n",
    "else:\n",
    "    print(\"No partitions specified.\")\n",
    "    USE_CONTEXT = False\n",
    "sdf_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29822a-5404-453f-a58f-038d25444abb",
   "metadata": {
    "codeCollapsed": true,
    "language": "python",
    "name": "_save_datasets",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A Column with semi-structured data type (variant, array or object) was detected. It might not be able to get converted to tensors. Please consider handling it in feature engineering.\n",
      "WARNING:root:A Column with semi-structured data type (variant, array or object) was detected. It might not be able to get converted to tensors. Please consider handling it in feature engineering.\n"
     ]
    }
   ],
   "source": [
    "# Save training and testing datasets for future use\n",
    "\n",
    "dataset_name = \"FORECAST_FEATURES\"\n",
    "dataset = Dataset.create(session, name=dataset_name+\"_TRAIN\", exist_ok=True)\n",
    "\n",
    "# Version the data itself; If the data changes on a subsequent run, a new dataset will be created.\n",
    "# If the data does not change, the previously saved dataset will be returned.\n",
    "ds_train_version = version_data(sdf_train)\n",
    "\n",
    "if ds_train_version in dataset.list_versions():\n",
    "    ds_train = dataset.select_version(ds_train_version)\n",
    "else:\n",
    "    ds_train = dataset.create_version(\n",
    "        version=ds_train_version,\n",
    "        input_dataframe=sdf_train,\n",
    "        label_cols=[\"MODEL_TARGET\"],        \n",
    "    )\n",
    "\n",
    "if VALIDATION_DAYS > 0:\n",
    "    dataset = Dataset.create(session, name=dataset_name+\"_TEST\", exist_ok=True)\n",
    "\n",
    "    ds_test_version = version_data(sdf_test)\n",
    "    \n",
    "    if ds_test_version in dataset.list_versions():\n",
    "        ds_test = dataset.select_version(ds_test_version)\n",
    "    else:\n",
    "        ds_test = dataset.create_version(\n",
    "            version=ds_test_version,\n",
    "            input_dataframe=sdf_test,\n",
    "            label_cols=[\"MODEL_TARGET\"],        \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "collapsed": false,
    "name": "md_model_training"
   },
   "source": [
    "-----\n",
    "# Model Training\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_create_udtf_for_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registration complete\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Define and register a UDTF to perform model training\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# # Get all of the column names except the partition columns and the column LEAD\n",
    "training_udtf_input_col_nms = [\n",
    "    colnm\n",
    "    for colnm in sdf_train.columns\n",
    "    if colnm not in [\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"]\n",
    "]\n",
    "\n",
    "\n",
    "def train_model(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Trains a forecasting model and returns the model binary and metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the model binary and metadata.\n",
    "\n",
    "    \"\"\"\n",
    "    # NOTE: In a vectorized UDTF we need to RENAME the columns to match the input dataset\n",
    "    df.columns = training_udtf_input_col_nms\n",
    "\n",
    "    # Set the index\n",
    "    df = df.set_index(pd.to_datetime(df.pop(TIME_PERIOD_COLUMN)))\n",
    "\n",
    "    # Create X and y dataframes.\n",
    "    X = df.drop(columns=[TARGET_COLUMN, \"MODEL_TARGET\"])\n",
    "    y = df[\"MODEL_TARGET\"]\n",
    "\n",
    "    # train a model\n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    model.fit(X, y)\n",
    "    # Save the model\n",
    "    raw_model = json.loads(model.get_booster().save_raw(raw_format='json'))\n",
    "    model_binary = pickle.dumps(raw_model)\n",
    "\n",
    "    # Obtain feature importances\n",
    "    feature_importance_dict = dict(\n",
    "        zip(X.columns, [float(val) for val in model.feature_importances_])\n",
    "    )\n",
    "    metadata = {\n",
    "        \"feature_importance\": feature_importance_dict,\n",
    "    }\n",
    "\n",
    "    # Save the environment specs\n",
    "    module_dict = {}\n",
    "    for finder, module_name, is_pkg in pkgutil.iter_modules():\n",
    "        try:\n",
    "            distribution = importlib.metadata.distribution(module_name)\n",
    "            version = distribution.version\n",
    "            module_dict[module_name] = version\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            continue\n",
    "    model_df = pd.DataFrame(\n",
    "        [[model.__class__.__name__, model_binary, metadata, module_dict]],\n",
    "        columns=[\"ALGORITHM\", \"MODEL_BINARY\", \"METADATA\",\"ENVIRONMENT_SPECS\"],\n",
    "    )\n",
    "\n",
    "    return model_df\n",
    "\n",
    "\n",
    "# Define UDTF class\n",
    "class ModelTrainingUDTF:\n",
    "    \"\"\"Class which is registered as a UDTF to train forecasting models.\"\"\"\n",
    "\n",
    "    def end_partition(self, df):\n",
    "        \"\"\"End partition method which utilizes the train model function.\"\"\"\n",
    "        forecast_df = train_model(df)\n",
    "        yield forecast_df\n",
    "\n",
    "\n",
    "# Get the data types for the input dataframe\n",
    "vect_udtf_input_dtypes = [\n",
    "    T.PandasDataFrameType(\n",
    "        [\n",
    "            field.datatype\n",
    "            for field in sdf_train.schema.fields\n",
    "            if field.name not in [\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"]\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Register the class as a temporary UDTF\n",
    "# Give the UDTF a unique name so that it doesn't conflict with anyone else running the same notebook\n",
    "udtf_name = f\"MODEL_TRAINER_{MODEL_NAME}_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}__{random.randint(1, 999)}\"\n",
    "session.udtf.register(\n",
    "    ModelTrainingUDTF,\n",
    "    name=udtf_name,\n",
    "    input_types=vect_udtf_input_dtypes,\n",
    "    output_schema=T.PandasDataFrameType(\n",
    "        [T.StringType(), T.BinaryType(), T.VariantType(), T.VariantType()],\n",
    "        [\"ALGORITHM\", \"MODEL_BINARY\", \"METADATA\",\"ENVIRONMENT_SPECS\"],\n",
    "    ),\n",
    "    packages=[\n",
    "        \"snowflake-snowpark-python\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"xgboost\",\n",
    "        \"scikit-learn\",\n",
    "    ],\n",
    "    replace=True,\n",
    "    is_permanent=False,\n",
    "    comment=query_tag,\n",
    ")\n",
    "\n",
    "print(\"Registration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_train_models"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "session.use_warehouse(MODELING_WH)\n",
    "\n",
    "# Before model training, remove records where MODEL_TARGET is null\n",
    "sdf_train = sdf_train.filter(F.col(\"MODEL_TARGET\").isNotNull())\n",
    "\n",
    "# Run the UDTF\n",
    "udtf_models = sdf_train.select(\n",
    "    \"GROUP_IDENTIFIER\",\n",
    "    \"GROUP_IDENTIFIER_STRING\",\n",
    "    F.call_table_function(udtf_name, *training_udtf_input_col_nms).over(\n",
    "        partition_by=[\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"],\n",
    "        order_by=TIME_PERIOD_COLUMN,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Add additional columns to the output\n",
    "if train_separate_lead_models:\n",
    "    total_leads_modeled_this_run = FORECAST_HORIZON\n",
    "elif not train_separate_lead_models:\n",
    "    total_leads_modeled_this_run = None\n",
    "\n",
    "udtf_models = udtf_models.select(\n",
    "    \"GROUP_IDENTIFIER\",\n",
    "    \"GROUP_IDENTIFIER_STRING\",\n",
    "    F.lit(MODEL_NAME).alias(\"MODEL_NAME\"),\n",
    "    \"ALGORITHM\",\n",
    "    F.lit(run_dttm).alias(\"MODEL_TRAINED_DTTM\"),\n",
    "    \"MODEL_BINARY\",\n",
    "    \"METADATA\",\n",
    "    \"ENVIRONMENT_SPECS\",\n",
    ")\n",
    "\n",
    "# Cache results for faster downstream usage of the udtf_models DataFrame\n",
    "udtf_models = udtf_models.cache_result()\n",
    "\n",
    "# Switch back to the original warehouse\n",
    "session.use_warehouse(SESSION_WH)\n",
    "\n",
    "# Function to load xgb model from raw json\n",
    "def get_xgb_model(raw_model):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\", mode='w', delete=False) as f:\n",
    "            json.dump(raw_model,f)\n",
    "            model_filename = f.name\n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    model.load_model(model_filename)\n",
    "    os.remove(model_filename)\n",
    "    return model\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "collapsed": false,
    "name": "md_model_registry"
   },
   "source": [
    "-----\n",
    "# Model Registry\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_log_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged successfully.: 100%|██████████| 6/6 [03:11<00:00, 31.96s/it]                          \n",
      "Model version name: LIGHT_HOUND_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>default_version_name</th>\n",
       "      <th>versions</th>\n",
       "      <th>aliases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-09 13:46:18.962000-07:00</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>USER_MODEL</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>ML_DEV_ROLE</td>\n",
       "      <td>ROTTEN_GOOSE_2</td>\n",
       "      <td>[\"LIGHT_HOUND_1\",\"ROTTEN_GOOSE_2\",\"SLIPPERY_SN...</td>\n",
       "      <td>{\"DEFAULT\":\"ROTTEN_GOOSE_2\",\"FIRST\":\"SLIPPERY_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on          name  model_type  \\\n",
       "0 2025-10-09 13:46:18.962000-07:00  TEST_MODEL_1  USER_MODEL   \n",
       "\n",
       "            database_name schema_name  \\\n",
       "0  FORECAST_MODEL_BUILDER    MODELING   \n",
       "\n",
       "                                             comment        owner  \\\n",
       "0  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  ML_DEV_ROLE   \n",
       "\n",
       "  default_version_name                                           versions  \\\n",
       "0       ROTTEN_GOOSE_2  [\"LIGHT_HOUND_1\",\"ROTTEN_GOOSE_2\",\"SLIPPERY_SN...   \n",
       "\n",
       "                                             aliases  \n",
       "0  {\"DEFAULT\":\"ROTTEN_GOOSE_2\",\"FIRST\":\"SLIPPERY_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Define the Partitioned Custom Model\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Sample input (based on feature view table) to track lineage\n",
    "sample_input = sdf_train.limit(100).drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "# Input features\n",
    "model_input_predictor_features = [\n",
    "    colnm\n",
    "    for colnm in sdf_train.columns\n",
    "    if colnm\n",
    "    not in [\n",
    "        \"GROUP_IDENTIFIER\",\n",
    "        \"GROUP_IDENTIFIER_STRING\",\n",
    "        TIME_PERIOD_COLUMN,\n",
    "        TARGET_COLUMN,\n",
    "        \"MODEL_TARGET\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Custom model uses model context if provided, otherwise uses model storage table binaries\n",
    "class ForecastingModel(custom_model.CustomModel):\n",
    "    \"\"\"Custom model class.\"\"\"\n",
    "\n",
    "    def __init__(self, context: Optional[custom_model.ModelContext] = None) -> None:\n",
    "        \"\"\"Initialize object.\"\"\"\n",
    "        super().__init__(context)\n",
    "        self.partition_id = None\n",
    "        self.model = None\n",
    "\n",
    "    @custom_model.partitioned_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Make predictions using unpickled model.\"\"\"\n",
    "        if self.partition_id != input_df[\"GROUP_IDENTIFIER_STRING\"][0]:\n",
    "            self.partition_id = input_df[\"GROUP_IDENTIFIER_STRING\"][0]\n",
    "\n",
    "            # Use model context if it exists\n",
    "            if len(self.context.model_refs):\n",
    "                self.model = self.context.model_ref(self.partition_id)\n",
    "            else:\n",
    "                # Get the model binary from the first row of the input DataFrame where the column is not null\n",
    "                raw_model = pickle.loads(\n",
    "                    input_df.loc[\n",
    "                        input_df[\"MODEL_BINARY\"].first_valid_index(), \"MODEL_BINARY\"\n",
    "                    ]\n",
    "                )\n",
    "                self.model = get_xgb_model(raw_model)\n",
    "\n",
    "        model_output = self.model.predict(input_df[model_input_predictor_features])\n",
    "        res = pd.DataFrame(model_output, columns=[\"_PRED_\"])\n",
    "        res[\"GROUP_IDENTIFIER_STRING_OUT_\"] = input_df[\"GROUP_IDENTIFIER_STRING\"]\n",
    "        res[TIME_PERIOD_COLUMN+\"_OUT_\"] = input_df[TIME_PERIOD_COLUMN]\n",
    "        return res\n",
    "\n",
    "if USE_CONTEXT:\n",
    "    # Create model context based on json from dataframe\n",
    "    model_dict = {row.GROUP_IDENTIFIER_STRING:get_xgb_model(pickle.loads(row.MODEL_BINARY)) for row in udtf_models.collect()}\n",
    "    context = custom_model.ModelContext(models=model_dict)\n",
    "else:\n",
    "    # Create Model Storage table if it does not already exist\n",
    "    # It will be created in the schema associated with the notebook (which is the schema that was created for this project).\n",
    "    session.sql(\n",
    "        f\"\"\"\n",
    "            create table if not exists {MODEL_BINARY_STORAGE_TBL_NM} (\n",
    "                GROUP_IDENTIFIER VARIANT,\n",
    "                GROUP_IDENTIFIER_STRING VARCHAR,\n",
    "                MODEL_NAME VARCHAR(100),\n",
    "                MODEL_VERSION VARCHAR(100),\n",
    "                ALGORITHM VARCHAR(100),\n",
    "                MODEL_TRAINED_DTTM TIMESTAMP,\n",
    "                MODEL_BINARY BINARY,\n",
    "                METADATA VARIANT,\n",
    "                ENVIRONMENT_SPECS VARIANT\n",
    "                )\n",
    "            comment = '{query_tag}'\n",
    "    \"\"\"\n",
    "    ).collect()\n",
    "    context = None\n",
    "    user_settings_dict[\"MODEL_BINARY_STORAGE_TBL_NM\"] = MODEL_BINARY_STORAGE_TBL_NM\n",
    "    # Add model storage to sample input\n",
    "    sample_input = sample_input.join(\n",
    "        udtf_models.select(\"GROUP_IDENTIFIER_STRING\",\"MODEL_BINARY\"),\n",
    "        on = \"GROUP_IDENTIFIER_STRING\",\n",
    "    )\n",
    "    print(f\"Number of models ({model_count}), greater than model context max. Model using storage table approach\")\n",
    "\n",
    "\n",
    "m = ForecastingModel(context)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Log Model to Model Registry\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "# Log the model to the model registry\n",
    "options = {\"function_type\": \"TABLE_FUNCTION\", \"relax_version\": False}\n",
    "user_settings_dict[\"USE_CONTEXT\"] = USE_CONTEXT\n",
    "user_settings_dict[\"TARGET_COLUMN\"] = \"MODEL_TARGET\"\n",
    "metrics_to_log = {\n",
    "    \"direct_multi_step_forecasting\": train_separate_lead_models,\n",
    "    \"frequency\": modeling_frequency,\n",
    "    \"training_data_start\": training_dttm_boundaries[\"MIN_DTTM\"].strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S\"\n",
    "    ),\n",
    "    \"training_data_end\": training_dttm_boundaries[\"MAX_DTTM\"].strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S\"\n",
    "    ),\n",
    "    \"user_settings\": user_settings_dict,\n",
    "    \"train_dataset\": {\"name\":ds_train.fully_qualified_name, \"version\":ds_train.selected_version.name},\n",
    "    \"test_dataset\": {\"name\":ds_test.fully_qualified_name, \"version\":ds_test.selected_version.name},\n",
    "}\n",
    "mv = reg.log_model(\n",
    "    m,\n",
    "    model_name=qualified_model_name,\n",
    "    options=options,\n",
    "    metrics=metrics_to_log,\n",
    "    conda_dependencies=[\"pandas\", \"xgboost\"],\n",
    "    sample_input_data=sample_input,\n",
    "    #signatures={\"predict\": signature},\n",
    "    comment=query_tag,\n",
    ")\n",
    "\n",
    "# In addition to setting the query tag for the model version, we also set it for the model itself\n",
    "reg.get_model(qualified_model_name).comment = query_tag\n",
    "\n",
    "print(f\"Model version name: {mv.version_name}\")\n",
    "\n",
    "# Confirm that the new model/version is in the registry\n",
    "reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_save_version_if_desired"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version 'LIGHT_HOUND_1' set as the default version in the registry.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>comment</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>is_default_version</th>\n",
       "      <th>functions</th>\n",
       "      <th>metadata</th>\n",
       "      <th>user_data</th>\n",
       "      <th>model_attributes</th>\n",
       "      <th>size</th>\n",
       "      <th>environment</th>\n",
       "      <th>runnable_in</th>\n",
       "      <th>inference_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-09 13:46:19.008000-07:00</td>\n",
       "      <td>SLIPPERY_SNAKE_2</td>\n",
       "      <td>[\"FIRST\"]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"client\":\"snowflake-ml-p...</td>\n",
       "      <td>112164418</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.12\",\"cuda_vers...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-09 13:57:36.875000-07:00</td>\n",
       "      <td>ROTTEN_GOOSE_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"client\":\"snowflake-ml-p...</td>\n",
       "      <td>4275486</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.12\",\"snowflake...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-13 11:40:42.696000-07:00</td>\n",
       "      <td>LIGHT_HOUND_1</td>\n",
       "      <td>[\"DEFAULT\",\"LAST\"]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>true</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"client\":\"snowflake-ml-p...</td>\n",
       "      <td>112164510</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.12\",\"cuda_vers...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on              name             aliases  \\\n",
       "0 2025-10-09 13:46:19.008000-07:00  SLIPPERY_SNAKE_2           [\"FIRST\"]   \n",
       "1 2025-10-09 13:57:36.875000-07:00    ROTTEN_GOOSE_2                  []   \n",
       "2 2025-10-13 11:40:42.696000-07:00     LIGHT_HOUND_1  [\"DEFAULT\",\"LAST\"]   \n",
       "\n",
       "                                             comment           database_name  \\\n",
       "0  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "1  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "2  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "\n",
       "  schema_name    model_name is_default_version    functions  \\\n",
       "0    MODELING  TEST_MODEL_1              false  [\"PREDICT\"]   \n",
       "1    MODELING  TEST_MODEL_1              false  [\"PREDICT\"]   \n",
       "2    MODELING  TEST_MODEL_1               true  [\"PREDICT\"]   \n",
       "\n",
       "                                            metadata user_data  \\\n",
       "0  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "1  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "2  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "\n",
       "                                    model_attributes       size  \\\n",
       "0  {\"framework\":\"custom\",\"client\":\"snowflake-ml-p...  112164418   \n",
       "1  {\"framework\":\"custom\",\"client\":\"snowflake-ml-p...    4275486   \n",
       "2  {\"framework\":\"custom\",\"client\":\"snowflake-ml-p...  112164510   \n",
       "\n",
       "                                         environment    runnable_in  \\\n",
       "0  {\"default\":{\"python_version\":\"3.12\",\"cuda_vers...  [\"WAREHOUSE\"]   \n",
       "1  {\"default\":{\"python_version\":\"3.12\",\"snowflake...  [\"WAREHOUSE\"]   \n",
       "2  {\"default\":{\"python_version\":\"3.12\",\"cuda_vers...  [\"WAREHOUSE\"]   \n",
       "\n",
       "  inference_services  \n",
       "0                 []  \n",
       "1                 []  \n",
       "2                 []  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Set the model as the default version in the registry\n",
    "# If using model storage table, save models there\n",
    "# --------------------------------------------------------\n",
    "\n",
    "if SAVE_MODEL_VERSION_THIS_RUN:\n",
    "        # Set default version of the model to this version name\n",
    "    reg.get_model(qualified_model_name).default = mv.version_name\n",
    "\n",
    "    if not USE_CONTEXT:\n",
    "        # Append model binaries and metadata to the model binary storage table in Snowflake\n",
    "        udtf_models_w_version = udtf_models.with_column(\n",
    "            \"MODEL_VERSION\", F.lit(mv.version_name)\n",
    "        ).select(session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\").columns)\n",
    "\n",
    "        udtf_models_w_version.write.save_as_table(\n",
    "            f\"{MODEL_BINARY_STORAGE_TBL_NM}\", mode=\"append\"\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Model version '{mv.version_name}' set as the default version in the registry.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"\"\"Model version '{mv.version_name}' will be deleted from the registry at the end of this notebook.\n",
    "    If you wish to save this version, set SAVE_MODEL_VERSION_THIS_RUN = True.\"\"\"\n",
    "    )\n",
    "\n",
    "# Look at the most recent 3 versions of the model\n",
    "reg.get_model(qualified_model_name).show_versions().tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000032",
   "metadata": {
    "collapsed": false,
    "name": "md_clean_up"
   },
   "source": [
    "-----\n",
    "# Clean up\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce110000-1111-2222-3333-ffffff000034",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_delete_version"
   },
   "outputs": [],
   "source": [
    "# If we don't want to keep the version we just built, we can remove it from the registry\n",
    "\n",
    "# NOTE: Comment this code out if you do not want to delete the model version from the model registry\n",
    "# If the user does not want to save the current version, delete this version of the model from the registry.\n",
    "current_model = reg.get_model(qualified_model_name)\n",
    "\n",
    "if not SAVE_MODEL_VERSION_THIS_RUN:\n",
    "    deletion_message = \"\"\n",
    "    try:\n",
    "        current_model.version(mv.version_name)\n",
    "    except Exception:\n",
    "        deletion_message = f\"WARNING: Model version '{mv.version_name}' does not exist in the registry.\"\n",
    "        print(deletion_message)\n",
    "\n",
    "    if len(deletion_message) == 0:\n",
    "        try:\n",
    "            if len(current_model.versions()) == 0:\n",
    "                print(\n",
    "                    f\"WARNING: There are no versions for model '{MODEL_NAME}' in the registry.\"\n",
    "                )\n",
    "            elif (len(current_model.versions()) == 1) & (\n",
    "                current_model.default.version_name == mv.version_name\n",
    "            ):\n",
    "                reg.delete_model(MODEL_NAME)\n",
    "                print(\n",
    "                    f\" Model '{MODEL_NAME}' (which only had one version: '{mv.version_name}') was deleted from the registry.\"\n",
    "                )\n",
    "            else:\n",
    "                current_model.delete_version(mv.version_name)\n",
    "                print(\n",
    "                    f\"Model version '{mv.version_name}' was deleted from the registry.\"\n",
    "                )\n",
    "        except Exception:\n",
    "            print(\n",
    "                f\"WARNING: Model version '{mv.version_name}' was not able to be deleted from the registry.\"\n",
    "            )\n",
    "\n",
    "    reg.show_models()"
   ]
  }
 ]
}
