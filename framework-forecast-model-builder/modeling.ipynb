{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "name": "md_title"
   },
   "source": [
    "# Partitioned Time Series Modeling\n",
    "\n",
    "This notebook can be used to train a time series forecasting model. \n",
    "\n",
    "It is especially useful for use cases in which multiple series need to be trained in parallel. For example, if a retailer needs to build a separate model for each individual store location, this code will train those models in parallel. This greatly improves run time, especially in cases involving a large number of partitions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2c7aa-6f5d-4afb-a9f4-ad3e55ad983b",
   "metadata": {
    "collapsed": false,
    "name": "md_instructions"
   },
   "source": [
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ \n",
    "\n",
    "__Prerequisites before running this notebook:__ \n",
    "\n",
    "- The time series data in Snowflake must have at least the following columns: __date or timestamp__ column, __target__ column, and __partition__ column(s) if multi-series. \n",
    "- The date column name MUST be in [unquoted identifier](https://docs.snowflake.com/en/sql-reference/identifiers-syntax#label-unquoted-identifier) format, i.e. contains only __upper case letters, underscores, and decimal digits__. It is __recommended__ that all other column names also be in that format so that [double-quoted identifiers](https://docs.snowflake.com/en/sql-reference/identifiers-syntax#label-delimited-identifier) are not needed.\n",
    "- The target column (and any exogenous feature columns if they exist) should have values in a numeric format like FLOAT, DOUBLE, or INT. \n",
    "- Any null values in the data should already be imputed. \n",
    "\n",
    "## Instructions\n",
    "\n",
    "\n",
    "1. Go to the ____set_global_variables___ cell in the __SETUP__ section below. \n",
    "    - Change the values of the user constants to match the specifications of the use case.\n",
    "    - Descriptions of each value are written in that cell.\n",
    "2. Click ___Run all___ in the upper right corner of the notebook to run the entire notebook. \n",
    "    - The notebook will perform feature engineering and will train models. \n",
    "    - If ___SAVE_MODEL_VERSION_THIS_RUN=True___, then the models will be saved to the model registry for later inference. \n",
    "    - If ___VALIDATION_DAYS___ are specified, interactive cells near the end of the notebook can be used to evaluate model performance.\n",
    "    \n",
    "❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ ❄️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import importlib.metadata\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import pkgutil\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "import xgboost as xgb\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model.model_signature import DataType, FeatureSpec, ModelSignature\n",
    "from snowflake.ml.registry import registry\n",
    "from snowflake.snowpark import Window\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "from forecast_model_builder.feature_engineering import (\n",
    "    apply_functions_in_a_loop,\n",
    "    expand_datetime,\n",
    "    recent_rolling_avg,\n",
    "    roll_up,\n",
    "    verify_current_frequency,\n",
    "    verify_valid_rollup_spec,\n",
    ")\n",
    "from forecast_model_builder.utils import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_establish_session"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session db.schema: FORECAST_MODEL_BUILDER.SAMPLE_PROJECT\n",
      "Current Datetime: 2025-03-24 23:09:33.498618\n"
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "session = connect(connection_name=\"default\")\n",
    "session_db = session.connection.database\n",
    "session_schema = session.connection.schema\n",
    "session_db_schema = f\"{session_db}.{session_schema}\"\n",
    "print(f\"Session db.schema: {session_db_schema}\")\n",
    "\n",
    "# Query tag\n",
    "query_tag = '{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"modeling\"}}'\n",
    "session.query_tag = query_tag\n",
    "\n",
    "# Get the current datetime  (This will be saved in the model storage table)\n",
    "run_dttm = datetime.now()\n",
    "print(f\"Current Datetime: {run_dttm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "collapsed": false,
    "name": "md_USER_SETUP"
   },
   "source": [
    "-----\n",
    "# SETUP\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_set_global_variables"
   },
   "outputs": [],
   "source": [
    "# SET GLOBAL VARIABLES FOR THIS RUN\n",
    "\n",
    "# Name the model (if model already exists, a new version will be created)\n",
    "MODEL_NAME = \"TEST_MODEL_1\"\n",
    "\n",
    "# Boolean that is True if we want to save the model version in the current run.\n",
    "# Users may want to set it to false while they experiment with different specificiations, and then set to True when they develop the model they want to save.\n",
    "SAVE_MODEL_VERSION_THIS_RUN = True\n",
    "\n",
    "# --------------------------------\n",
    "# Input Time Series Data\n",
    "# --------------------------------\n",
    "# Establish the Snowflake database, schema, and table containing the time series data\n",
    "TS_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "TS_SCHEMA = \"BASE\"\n",
    "TS_TABLE_NM = \"DAILY_PARTITIONED_SAMPLE_DATA\"\n",
    "\n",
    "# --------------------------------\n",
    "# Modeling setup\n",
    "# --------------------------------\n",
    "# Establish the Database and Schema that will be used to store the models\n",
    "MODEL_DB = \"FORECAST_MODEL_BUILDER\"\n",
    "MODEL_SCHEMA = \"MODELING\"\n",
    "\n",
    "# --------------------------------\n",
    "# Virtual Warehouse\n",
    "# --------------------------------\n",
    "# For modeling and inference, a larger warehouse may speed up execution time depending on the number of partitions.\n",
    "# Scale up if there are a lot of partitions.\n",
    "# NOTE: If set to None, then the session warehouse will be used.\n",
    "MODELING_WH = \"STANDARD_XL\"\n",
    "\n",
    "# --------------------------------\n",
    "# Modeling\n",
    "# --------------------------------\n",
    "# From the time series data (TS_TABLE_NM), specify the name of column containing the datetime information\n",
    "TIME_PERIOD_COLUMN = \"ORDER_TIMESTAMP\"\n",
    "\n",
    "# NOTE: For the next 3 constants, if column names require a double-quoted identifier, include double quotes within the single quotes.\n",
    "#       Examples: '\"Target\"', ['\"store id\"', '\"product id\"'], ['\"Feature 1\"'].\n",
    "\n",
    "# Name of column containing the target variable (i.e. the value we are trying to predict)\n",
    "TARGET_COLUMN = \"TARGET\"\n",
    "\n",
    "# List of column names to use as partition columns. This is how you define each individual series to be modeled.\n",
    "# If modeling a single series (i.e. no partitions) set this as an EMPTY LIST [].\n",
    "PARTITION_COLUMNS = [\"STORE_ID\", \"PRODUCT_ID\"]\n",
    "\n",
    "# List of column names in the time series table to use as EXOGENOUS FEATURES.\n",
    "# Exogenous features are variables outside the main time series that can impact future values of the target variable.\n",
    "#     Examples: weather features, promotions, holidays, economic indicators (like inflation), inventory on hand, etc.\n",
    "# If there are no exogenous features in the data set, set this as an EMPTY LIST [].\n",
    "# NOTE: This notebook will create several features (like YEAR, MONTH, DAY_OF_YEAR, etc). You do NOT need to list those.\n",
    "#       Only list features that are already in the Snowflake table (TS_TABLE_NM).\n",
    "EXOGENOUS_COLUMNS = [\"FEATURE_1\"]\n",
    "\n",
    "# ALL_EXOG_COLS_HAVE_FUTURE_VALS is a boolean that is True if all exogenous features have future values present in the inference data.\n",
    "#     For example, if you are predicting 56 days into the future (i.e. FORECAST_HORIZON=56),\n",
    "#                  but you only know promotions for the next 4 weeks, you would set ALL_EXOG_COLS_HAVE_FUTURE_VALS = False.\n",
    "# NOTE: There are two modeling patterns in this notebook:\n",
    "#       1. Direct Multi-Step Forecasting - If ALL_EXOG_COLS_HAVE_FUTURE_VALS = False,\n",
    "#                                           the code will create separate models for each lead/step (from step = 1 to step = FORECAST_HORIZON) within each partition.\n",
    "#                                           In this pattern, inference is done using the most current date's information to predict each future step.\n",
    "#       2. Global Modeling               - If ALL_EXOG_COLS_HAVE_FUTURE_VALS = True (or EXOGENOUS_COLUMNS is empty),\n",
    "#                                           the code will train a single model within each partition.\n",
    "#                                           In this pattern, inference is done using the information for each future step,\n",
    "#                                               so the inference dataset will need a separate record for each future date to be predicted.\n",
    "# This variable will determine which pattern is used.\n",
    "ALL_EXOG_COLS_HAVE_FUTURE_VALS = True\n",
    "\n",
    "# Specify if we should create lag features for the target variable (including avgs of previous periods). This will affect the lag_and_target_prep & recent_rolling_avg functions.\n",
    "# NOTE: If we are using the Global Modeling pattern, we will not create recent rolling avg features.\n",
    "CREATE_LAG_FEATURE = False\n",
    "\n",
    "# Frequency of the data (choose from: \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"other\")\n",
    "# This is the frequency of the data as it currently exists in the Snowflake table (TS_TABLE_NM).\n",
    "# If it is not a standard frequency, select \"other\"\n",
    "CURRENT_FREQUENCY = \"day\"\n",
    "\n",
    "# Frequency to roll up to (choose from: \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", or None)\n",
    "# If you do not wish to roll up to a higher level, set ROLLUP_FREQUENCY=None.\n",
    "ROLLUP_FREQUENCY = None\n",
    "\n",
    "# Specify how each column should agg on roll-up (choose from: \"sum\", \"avg\", \"min\", or \"max\")\n",
    "# NOTE: If rollup_frequency is not None, then this can be an empty dictionary {}.\n",
    "#       Otherwise, you must specify an aggregation for the TARGET column AND for each of the EXOGENOUS_COLUMNS.\n",
    "ROLLUP_AGGREGATIONS = {\n",
    "    TARGET_COLUMN: \"sum\",\n",
    "    \"FEATURE_1\": \"sum\",\n",
    "}\n",
    "\n",
    "# Forecast Horizon. Number of time periods to forecast into the future (UNITS will be that of the ROLLUP_FREQUENCY if specified, otherwise CURRENT_FREQUENCY).\n",
    "# NOTE: Keep this number as small as possible if doing Direct Multi-Step Forecasting (in which a separate model gets built for each future time period).\n",
    "FORECAST_HORIZON = 7\n",
    "\n",
    "# Specify how many days to set aside for validation.\n",
    "# NOTE: If this is set to 0, then the model will be trained on all historic data.\n",
    "VALIDATION_DAYS = 90\n",
    "\n",
    "# XGBRegressor hyperparameter selections.\n",
    "# NOTE: This notebook does not perform hyperparameter tuning, so you can set these parameters here if you know which values you would like to use.\n",
    "XGB_PARAMS = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.80,\n",
    "    \"colsample_bytree\": 0.80,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "# --------------------------------\n",
    "# Inference\n",
    "# --------------------------------\n",
    "# When distributing the inference records, we can set the batch size here.\n",
    "# If the number is too high, inference on a large number of records might use up all available memory.\n",
    "INFERENCE_APPROX_BATCH_SIZE = 200\n",
    "\n",
    "# --------------------------------\n",
    "# Calculated Constants\n",
    "# --------------------------------\n",
    "# Establish the name of the table that will hold model binaries.\n",
    "# This will be a Snowflake table in your project schema.\n",
    "MODEL_BINARY_STORAGE_TBL_NM = f\"MODEL_STORAGE_{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "name": "md_objects"
   },
   "source": [
    "-----\n",
    "# Establish objects needed for this run\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_other_objects"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session warehouse:          WH_XS\n",
      "Modeling warehouse:         STANDARD_XL \n",
      "\n",
      "Modeling Frequency:         day\n",
      "Train Separate Lead Models: False\n",
      "Model FORECAST_MODEL_BUILDER.MODELING.TEST_MODEL_1 already exists. This notebook will build a new version.\n"
     ]
    }
   ],
   "source": [
    "# DERIVED OBJECTS\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Notebook Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "SESSION_WH = session.connection.warehouse\n",
    "print(f\"Session warehouse:          {SESSION_WH}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Check Modeling Warehouse\n",
    "# -----------------------------------------------------------------------\n",
    "# Check that the user specified an available warehouse as MODELING_WH. If not, use the session warehouse.\n",
    "available_warehouses = [\n",
    "    row[\"NAME\"]\n",
    "    for row in session.sql(\"SHOW WAREHOUSES\")\n",
    "    .select(F.col('\"name\"').alias(\"NAME\"))\n",
    "    .collect()\n",
    "]\n",
    "\n",
    "if MODELING_WH in available_warehouses:\n",
    "    print(f\"Modeling warehouse:         {MODELING_WH} \\n\")\n",
    "else:\n",
    "    print(\n",
    "        f\"WARNING: User does not have access to MODELING_WH = '{MODELING_WH}'. Model training will use '{SESSION_WH}' instead. \\n\"\n",
    "    )\n",
    "    MODELING_WH = SESSION_WH\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Fully qualified MODEL NAME\n",
    "# -----------------------------------------------------------------------\n",
    "qualified_model_name = f\"{MODEL_DB}.{MODEL_SCHEMA}.{MODEL_NAME}\"\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create dictionary of user settings to log with the model\n",
    "# -----------------------------------------------------------------------\n",
    "user_settings_dict = {\n",
    "    \"MODEL_NAME\": MODEL_NAME,\n",
    "    \"SAVE_MODEL_VERSION_THIS_RUN\": SAVE_MODEL_VERSION_THIS_RUN,\n",
    "    \"TS_DB\": TS_DB,\n",
    "    \"TS_SCHEMA\": TS_SCHEMA,\n",
    "    \"TS_TABLE_NM\": TS_TABLE_NM,\n",
    "    \"MODEL_DB\": MODEL_DB,\n",
    "    \"MODEL_SCHEMA\": MODEL_SCHEMA,\n",
    "    \"MODEL_BINARY_STORAGE_TBL_NM\": MODEL_BINARY_STORAGE_TBL_NM,\n",
    "    \"SESSION_WH\": SESSION_WH,\n",
    "    \"MODELING_WH\": MODELING_WH,\n",
    "    \"TIME_PERIOD_COLUMN\": TIME_PERIOD_COLUMN,\n",
    "    \"TARGET_COLUMN\": TARGET_COLUMN,\n",
    "    \"PARTITION_COLUMNS\": PARTITION_COLUMNS,\n",
    "    \"EXOGENOUS_COLUMNS\": EXOGENOUS_COLUMNS,\n",
    "    \"ALL_EXOG_COLS_HAVE_FUTURE_VALS\": ALL_EXOG_COLS_HAVE_FUTURE_VALS,\n",
    "    \"CREATE_LAG_FEATURE\": CREATE_LAG_FEATURE,\n",
    "    \"CURRENT_FREQUENCY\": CURRENT_FREQUENCY,\n",
    "    \"ROLLUP_FREQUENCY\": ROLLUP_FREQUENCY,\n",
    "    \"ROLLUP_AGGREGATIONS\": ROLLUP_AGGREGATIONS,\n",
    "    \"FORECAST_HORIZON\": FORECAST_HORIZON,\n",
    "    \"VALIDATION_DAYS\": VALIDATION_DAYS,\n",
    "    \"XGB_PARAMS\": XGB_PARAMS,\n",
    "    \"INFERENCE_APPROX_BATCH_SIZE\": INFERENCE_APPROX_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# BACKEND SETUP: Create Model Schema\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a schema to hold our models if it does not already exist\n",
    "schema_exists = (\n",
    "    session.table(f\"{MODEL_DB}.INFORMATION_SCHEMA.SCHEMATA\")\n",
    "    .filter(F.upper(F.col(\"SCHEMA_NAME\")) == F.upper(F.lit(MODEL_SCHEMA)))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "if schema_exists == 0:\n",
    "    try:\n",
    "        session.sql(f\"create schema if not exists {MODEL_DB}.{MODEL_SCHEMA}\").collect()\n",
    "    except Exception as e:\n",
    "        if \"insufficient privileges\" in str(e).lower():\n",
    "            raise PermissionError(f\"\"\"Schema {MODEL_SCHEMA} does not already exist in {MODEL_DB}, and user does not have sufficient privileges to CREATE SCHEMA. \n",
    "            Please specify an existing schema for MODEL_SCHEMA constant.\"\"\") from e\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"An error occurred while attempting to create schema {MODEL_DB}.{MODEL_SCHEMA}: {e}\"\n",
    "            ) from e\n",
    "\n",
    "# Reset the schema to the original session schema. (If we created a new schema, the session schema was set to the new schema)\n",
    "session.use_schema(session_db_schema)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a window spec\n",
    "# -----------------------------------------------------------------------\n",
    "window_spec = Window.partitionBy(PARTITION_COLUMNS).orderBy(TIME_PERIOD_COLUMN)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Create a variable for the frequency at which we will be modeling\n",
    "# -----------------------------------------------------------------------\n",
    "CURRENT_FREQUENCY = CURRENT_FREQUENCY.lower()\n",
    "\n",
    "if ROLLUP_FREQUENCY is not None:\n",
    "    ROLLUP_FREQUENCY = ROLLUP_FREQUENCY.lower()\n",
    "    if ROLLUP_FREQUENCY.lower() == \"none\":\n",
    "        ROLLUP_FREQUENCY = None\n",
    "\n",
    "modeling_frequency = CURRENT_FREQUENCY if ROLLUP_FREQUENCY is None else ROLLUP_FREQUENCY\n",
    "print(f\"Modeling Frequency:         {modeling_frequency}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Varible for modeling pattern\n",
    "# -----------------------------------------------------------------------\n",
    "# Either (1) train_separate_lead_models = False : all features have future values in the inference data, so we don't need a separate model for each lead\n",
    "# or (2) train_separate_lead_models = True : data contains exogenous variables that the inference data won't have future values for, requiring direct multi-step (lead) modeling\n",
    "train_separate_lead_models = (\n",
    "    False\n",
    "    if ALL_EXOG_COLS_HAVE_FUTURE_VALS is True or len(EXOGENOUS_COLUMNS) == 0\n",
    "    else True\n",
    ")\n",
    "print(f\"Train Separate Lead Models: {train_separate_lead_models}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Establish model registry object\n",
    "# -----------------------------------------------------------------------\n",
    "reg = registry.Registry(\n",
    "    session=session, database_name=MODEL_DB, schema_name=MODEL_SCHEMA\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# BACKEND SETUP: Create Backend Tables\n",
    "# -----------------------------------------------------------------------\n",
    "# Create Model Storage table if it does not already exist\n",
    "# It will be created in the schema associated with the notebook (which is the schema that was created for this project).\n",
    "session.sql(\n",
    "    f\"\"\"\n",
    "create table if not exists {MODEL_BINARY_STORAGE_TBL_NM} (\n",
    "    GROUP_IDENTIFIER VARIANT,\n",
    "    GROUP_IDENTIFIER_STRING VARCHAR,\n",
    "    MODEL_NAME VARCHAR(100),\n",
    "    MODEL_VERSION VARCHAR(100),\n",
    "    ALGORITHM VARCHAR(100),\n",
    "    MODEL_TRAINED_DTTM TIMESTAMP,\n",
    "    MODEL_BINARY BINARY,\n",
    "    METADATA VARIANT,\n",
    "    ENVIRONMENT_SPECS VARIANT\n",
    "    )\n",
    "comment = '{query_tag}'\n",
    "\"\"\"\n",
    ").collect()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# Does model already exist in the registry?\n",
    "# -----------------------------------------------------------------------\n",
    "try:\n",
    "    number_of_versions = len(reg.get_model(qualified_model_name).show_versions())\n",
    "    if number_of_versions > 0:\n",
    "        print(\n",
    "            f\"Model {qualified_model_name} already exists. This notebook will build a new version.\"\n",
    "        )\n",
    "except Exception:\n",
    "    print(f\"This will be the first version of model {qualified_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "name": "md_train_test_split"
   },
   "source": [
    "-----\n",
    "# TRAIN/TEST SPLIT\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_train_split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set row count: 344750\n",
      "First time period in training set: 2021-01-01 00:00:00\n",
      "Last time period in training set:  2024-10-10 00:00:00\n",
      "Total Partition Count: 250\n",
      "-----------------------------------------------------------------------------------------\n",
      "|\"ORDER_TIMESTAMP\"    |\"TARGET\"           |\"STORE_ID\"  |\"PRODUCT_ID\"  |\"FEATURE_1\"      |\n",
      "-----------------------------------------------------------------------------------------\n",
      "|2022-05-14 00:00:00  |306.2132575120000  |22          |8             |320.12911991500  |\n",
      "|2022-05-15 00:00:00  |325.2240795290000  |22          |8             |452.84631991700  |\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN/TEST SPLIT\n",
    "\n",
    "# Create Snowpark DataFrame from table in Snowflake\n",
    "sdf = session.table(f\"{TS_DB}.{TS_SCHEMA}.{TS_TABLE_NM}\")\n",
    "\n",
    "# Only keep the columns specified in the config\n",
    "sdf = sdf.select(\n",
    "    TIME_PERIOD_COLUMN, TARGET_COLUMN, *PARTITION_COLUMNS, *EXOGENOUS_COLUMNS\n",
    ")\n",
    "\n",
    "# TRAIN/VALIDATION SPLIT\n",
    "if VALIDATION_DAYS == 0:\n",
    "    sdf_train = sdf\n",
    "elif VALIDATION_DAYS > 0:\n",
    "    # Get the last time period in the dataset\n",
    "    last_time_period = sdf.select(\n",
    "        F.max(TIME_PERIOD_COLUMN).alias(\"MAX_DTTM\")\n",
    "    ).collect()[0][\"MAX_DTTM\"]\n",
    "    # Remove the validation records from the training set\n",
    "    sdf_train = sdf.filter(\n",
    "        F.date_trunc(\"day\", TIME_PERIOD_COLUMN)\n",
    "        < F.dateadd(\"day\", F.lit(-VALIDATION_DAYS), F.lit(last_time_period))\n",
    "    )\n",
    "\n",
    "# Inspect the data\n",
    "training_dttm_boundaries = sdf_train.select(\n",
    "    F.min(TIME_PERIOD_COLUMN).alias(\"MIN_DTTM\"),\n",
    "    F.max(TIME_PERIOD_COLUMN).alias(\"MAX_DTTM\"),\n",
    ").collect()[0]\n",
    "print(f\"Training set row count: {sdf_train.count()}\")\n",
    "print(f\"First time period in training set: {training_dttm_boundaries['MIN_DTTM']}\")\n",
    "print(f\"Last time period in training set:  {training_dttm_boundaries['MAX_DTTM']}\")\n",
    "if len(PARTITION_COLUMNS) > 0:\n",
    "    print(\n",
    "        f\"Total Partition Count: {sdf_train.select(PARTITION_COLUMNS).distinct().count()}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No partitions specified.\")\n",
    "sdf_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_prelim_checks"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common time between consecutive records (frequency): 1.0 day(s)\n",
      "    The current frequency appears to be in DAY granularity.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# Preliminary checks\n",
    "# -----------------------------------------\n",
    "# Verify valid rollup specification.\n",
    "# Raise an error if the user specifies a rollup frequency that is finer grain than the current frequency\n",
    "# Raise an error if the user does not specify a rollup aggregation for the target and all exogenous columns\n",
    "verify_valid_rollup_spec(\n",
    "    CURRENT_FREQUENCY, ROLLUP_FREQUENCY, ROLLUP_AGGREGATIONS, EXOGENOUS_COLUMNS\n",
    ")\n",
    "\n",
    "# Roughly verify the current frequency (datetime difference between consecutive records) of the time series data\n",
    "verify_current_frequency(sdf_train, TIME_PERIOD_COLUMN, window_spec, CURRENT_FREQUENCY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "name": "md_feature_engineering"
   },
   "source": [
    "-----\n",
    "# Feature Engineering\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_feature_engineering"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total record count after rolling up:   344750\n",
      "Total record count of final data:      344750\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_TIMESTAMP\"    |\"TARGET\"       |\"FEATURE_1\"    |\"YEAR\"  |\"MONTH_SIN\"         |\"MONTH_COS\"          |\"WEEK_OF_YEAR_SIN\"  |\"WEEK_OF_YEAR_COS\"   |\"DAY_OF_WEEK_SUN\"  |\"DAY_OF_WEEK_MON\"  |\"DAY_OF_WEEK_TUE\"  |\"DAY_OF_WEEK_WED\"  |\"DAY_OF_WEEK_THU\"  |\"DAY_OF_WEEK_FRI\"  |\"DAY_OF_WEEK_SAT\"  |\"DAY_OF_YEAR_SIN\"   |\"DAY_OF_YEAR_COS\"    |\"DAYS_SINCE_JAN2020\"  |\"MODEL_TARGET\"  |\"GROUP_IDENTIFIER\"  |\"GROUP_IDENTIFIER_STRING\"  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2022-05-14 00:00:00  |306.213257512  |320.129119915  |2022    |0.5000000000000003  |-0.8660254037844385  |0.748510748171101   |-0.6631226582407953  |0                  |0                  |0                  |0                  |0                  |0                  |1                  |0.7412220108485956  |-0.6712599575675317  |864                   |306.213257512   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "|2022-05-15 00:00:00  |325.224079529  |452.846319917  |2022    |0.5000000000000003  |-0.8660254037844385  |0.748510748171101   |-0.6631226582407953  |1                  |0                  |0                  |0                  |0                  |0                  |0                  |0.7295575540864875  |-0.6839194216246106  |865                   |325.224079529   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |               |        |                    |                     |                    |                     |                   |                   |                   |                   |                   |                   |                   |                    |                     |                      |                |}                   |                           |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First Convert Decimal data types to Floats (because DecimalType doesn't work in modeling algorithms)\n",
    "sdf_converted = sdf_train.select(\n",
    "    [\n",
    "        (\n",
    "            F.col(field.name).cast(T.FloatType()).alias(field.name)\n",
    "            if isinstance(field.datatype, T.DecimalType)\n",
    "            else F.col(field.name)\n",
    "        )\n",
    "        for field in sdf_train.schema\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ROLL UP to specified frequency\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_rollup = roll_up(\n",
    "    sdf_converted,\n",
    "    TIME_PERIOD_COLUMN,\n",
    "    PARTITION_COLUMNS,\n",
    "    TARGET_COLUMN,\n",
    "    EXOGENOUS_COLUMNS,\n",
    "    ROLLUP_FREQUENCY,\n",
    "    ROLLUP_AGGREGATIONS,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create time-derived features\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_engineered = expand_datetime(sdf_rollup, TIME_PERIOD_COLUMN, modeling_frequency)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create rolling average of most recent time periods\n",
    "# ------------------------------------------------------------------------\n",
    "# NOTE: We can only generate recent rolling average features if we are training separate lead models (direct multi-step forecasting).\n",
    "if CREATE_LAG_FEATURE & train_separate_lead_models:\n",
    "    sdf_engineered = recent_rolling_avg(\n",
    "        sdf_engineered, [TARGET_COLUMN], window_spec, modeling_frequency\n",
    "    )\n",
    "\n",
    "# Cached Results\n",
    "sdf_engineered = sdf_engineered.cache_result()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create LAG features (and possibly LEAD feature) of the TARGET variable\n",
    "# ------------------------------------------------------------------------\n",
    "final_sdf = apply_functions_in_a_loop(\n",
    "    train_separate_lead_models=train_separate_lead_models,\n",
    "    partition_column_list=PARTITION_COLUMNS,\n",
    "    input_sdf=sdf_engineered,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    time_step_frequency=modeling_frequency,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    w_spec=window_spec,\n",
    "    create_lag_feature=CREATE_LAG_FEATURE,\n",
    ")\n",
    "\n",
    "# Before model training, remove records where MODEL_TARGET is null\n",
    "final_sdf = final_sdf.filter(F.col(\"MODEL_TARGET\").isNotNull())\n",
    "\n",
    "# Cache the final SDF\n",
    "final_sdf = final_sdf.cache_result()\n",
    "\n",
    "# Inspect data\n",
    "print(f\"Total record count after rolling up:   {sdf_rollup.count()}\")\n",
    "print(f\"Total record count of final data:      {final_sdf.count()}\")\n",
    "final_sdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "name": "md_model_training"
   },
   "source": [
    "-----\n",
    "# Model Training\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_create_udtf_for_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registration complete\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Define and register a UDTF to perform model training\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# # Get all of the column names except the partition columns and the column LEAD\n",
    "training_udtf_input_col_nms = [\n",
    "    colnm\n",
    "    for colnm in final_sdf.columns\n",
    "    if colnm not in [\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"]\n",
    "]\n",
    "\n",
    "\n",
    "def train_model(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Trains a forecasting model and returns the model binary and metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the model binary and metadata.\n",
    "\n",
    "    \"\"\"\n",
    "    # NOTE: In a vectorized UDTF we need to RENAME the columns to match the input dataset\n",
    "    df.columns = training_udtf_input_col_nms\n",
    "\n",
    "    # Set the index\n",
    "    df = df.set_index(pd.to_datetime(df.pop(TIME_PERIOD_COLUMN)))\n",
    "\n",
    "    # Create X and y dataframes.\n",
    "    X = df.drop(columns=[TARGET_COLUMN, \"MODEL_TARGET\"])\n",
    "    y = df[\"MODEL_TARGET\"]\n",
    "\n",
    "    # train a model\n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    model.fit(X, y)\n",
    "    # Save the model binary\n",
    "    model_binary = pickle.dumps(model)\n",
    "    # Obtain feature importances\n",
    "    feature_importance_dict = dict(\n",
    "        zip(X.columns, [float(val) for val in model.feature_importances_])\n",
    "    )\n",
    "    metadata = {\n",
    "        \"feature_importance\": feature_importance_dict,\n",
    "    }\n",
    "\n",
    "    # Save the environment specs\n",
    "    module_dict = {}\n",
    "    for finder, module_name, is_pkg in pkgutil.iter_modules():\n",
    "        try:\n",
    "            distribution = importlib.metadata.distribution(module_name)\n",
    "            version = distribution.version\n",
    "            module_dict[module_name] = version\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            continue\n",
    "    model_df = pd.DataFrame(\n",
    "        [[model.__class__.__name__, model_binary, module_dict, metadata]],\n",
    "        columns=[\"ALGORITHM\", \"MODEL_BINARY\", \"ENVIRONMENT_SPECS\", \"METADATA\"],\n",
    "    )\n",
    "\n",
    "    return model_df\n",
    "\n",
    "\n",
    "# Define UDTF class\n",
    "class ModelTrainingUDTF:\n",
    "    \"\"\"Class which is registered as a UDTF to train forecasting models.\"\"\"\n",
    "\n",
    "    def end_partition(self, df):\n",
    "        \"\"\"End partition method which utilizes the train model function.\"\"\"\n",
    "        forecast_df = train_model(df)\n",
    "        yield forecast_df\n",
    "\n",
    "\n",
    "# Get the data types for the input dataframe\n",
    "vect_udtf_input_dtypes = [\n",
    "    T.PandasDataFrameType(\n",
    "        [\n",
    "            field.datatype\n",
    "            for field in final_sdf.schema.fields\n",
    "            if field.name not in [\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"]\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Register the class as a temporary UDTF\n",
    "# Give the UDTF a unique name so that it doesn't conflict with anyone else running the same notebook\n",
    "udtf_name = f\"MODEL_TRAINER_{MODEL_NAME}_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}__{random.randint(1, 999)}\"\n",
    "session.udtf.register(\n",
    "    ModelTrainingUDTF,\n",
    "    name=udtf_name,\n",
    "    input_types=vect_udtf_input_dtypes,\n",
    "    output_schema=T.PandasDataFrameType(\n",
    "        [T.StringType(), T.BinaryType(), T.VariantType(), T.VariantType()],\n",
    "        [\"ALGORITHM\", \"MODEL_BINARY\", \"ENVIRONMENT_SPECS\", \"METADATA\"],\n",
    "    ),\n",
    "    packages=[\n",
    "        \"snowflake-snowpark-python\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"xgboost\",\n",
    "        \"scikit-learn\",\n",
    "    ],\n",
    "    replace=True,\n",
    "    is_permanent=False,\n",
    "    comment=query_tag,\n",
    ")\n",
    "\n",
    "print(\"Registration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_train_models"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "session.use_warehouse(MODELING_WH)\n",
    "\n",
    "# Run the UDTF\n",
    "udtf_models = final_sdf.select(\n",
    "    \"GROUP_IDENTIFIER\",\n",
    "    \"GROUP_IDENTIFIER_STRING\",\n",
    "    F.call_table_function(udtf_name, *training_udtf_input_col_nms).over(\n",
    "        partition_by=[\"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\"],\n",
    "        order_by=TIME_PERIOD_COLUMN,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Add additional columns to the output\n",
    "if train_separate_lead_models:\n",
    "    total_leads_modeled_this_run = FORECAST_HORIZON\n",
    "elif not train_separate_lead_models:\n",
    "    total_leads_modeled_this_run = None\n",
    "\n",
    "udtf_models = udtf_models.select(\n",
    "    \"GROUP_IDENTIFIER\",\n",
    "    \"GROUP_IDENTIFIER_STRING\",\n",
    "    F.lit(MODEL_NAME).alias(\"MODEL_NAME\"),\n",
    "    \"ALGORITHM\",\n",
    "    F.lit(run_dttm).alias(\"MODEL_TRAINED_DTTM\"),\n",
    "    \"MODEL_BINARY\",\n",
    "    F.object_insert(\n",
    "        F.col(\"METADATA\"),\n",
    "        F.lit(\"total_leads_modeled_this_run\"),\n",
    "        F.lit(total_leads_modeled_this_run),\n",
    "    )\n",
    "    .astype(T.VariantType())\n",
    "    .alias(\"METADATA\"),\n",
    "    \"ENVIRONMENT_SPECS\",\n",
    ")\n",
    "\n",
    "# Cache results for faster downstream usage of the udtf_models DataFrame\n",
    "udtf_models = udtf_models.cache_result()\n",
    "\n",
    "# Switch back to the original warehouse\n",
    "session.use_warehouse(SESSION_WH)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "collapsed": false,
    "name": "md_model_registry"
   },
   "source": [
    "-----\n",
    "# Model Registry\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_log_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version name: SHAGGY_LEECH_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>default_version_name</th>\n",
       "      <th>versions</th>\n",
       "      <th>aliases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-24 18:07:50.146000-07:00</td>\n",
       "      <td>BC_HOURLY_SINGLE_PARTITION</td>\n",
       "      <td>USER_MODEL</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>SCS_ROLE</td>\n",
       "      <td>PROUD_SKUNK_4</td>\n",
       "      <td>[\"PROUD_SKUNK_4\"]</td>\n",
       "      <td>{\"DEFAULT\":\"PROUD_SKUNK_4\",\"FIRST\":\"PROUD_SKUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-24 17:24:28.068000-07:00</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>USER_MODEL</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>SCS_ROLE</td>\n",
       "      <td>QUIET_BEAR_2</td>\n",
       "      <td>[\"HARD_BULLFROG_1\",\"NEW_GAZELLE_2\",\"QUIET_BEAR...</td>\n",
       "      <td>{\"DEFAULT\":\"QUIET_BEAR_2\",\"FIRST\":\"WICKED_SEAL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on                        name  model_type  \\\n",
       "0 2025-03-24 18:07:50.146000-07:00  BC_HOURLY_SINGLE_PARTITION  USER_MODEL   \n",
       "1 2025-03-24 17:24:28.068000-07:00                TEST_MODEL_1  USER_MODEL   \n",
       "\n",
       "            database_name schema_name  \\\n",
       "0  FORECAST_MODEL_BUILDER    MODELING   \n",
       "1  FORECAST_MODEL_BUILDER    MODELING   \n",
       "\n",
       "                                             comment     owner  \\\n",
       "0  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  SCS_ROLE   \n",
       "1  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  SCS_ROLE   \n",
       "\n",
       "  default_version_name                                           versions  \\\n",
       "0        PROUD_SKUNK_4                                  [\"PROUD_SKUNK_4\"]   \n",
       "1         QUIET_BEAR_2  [\"HARD_BULLFROG_1\",\"NEW_GAZELLE_2\",\"QUIET_BEAR...   \n",
       "\n",
       "                                             aliases  \n",
       "0  {\"DEFAULT\":\"PROUD_SKUNK_4\",\"FIRST\":\"PROUD_SKUN...  \n",
       "1  {\"DEFAULT\":\"QUIET_BEAR_2\",\"FIRST\":\"WICKED_SEAL...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Define the Partitioned Custom Model\n",
    "# --------------------------------------------------------\n",
    "\n",
    "model_input_predictor_features = [\n",
    "    colnm\n",
    "    for colnm in final_sdf.columns\n",
    "    if colnm\n",
    "    not in [\n",
    "        \"GROUP_IDENTIFIER\",\n",
    "        \"GROUP_IDENTIFIER_STRING\",\n",
    "        TIME_PERIOD_COLUMN,\n",
    "        TARGET_COLUMN,\n",
    "        \"MODEL_TARGET\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "class ForecastingModelPickleInput(custom_model.CustomModel):\n",
    "    \"\"\"Custom model class.\"\"\"\n",
    "\n",
    "    def __init__(self, context: Optional[custom_model.ModelContext] = None) -> None:\n",
    "        \"\"\"Initialize object.\"\"\"\n",
    "        super().__init__(context)\n",
    "        self.partition_id = None\n",
    "        self.model = None\n",
    "\n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Make predictions using unpickled model.\"\"\"\n",
    "        if self.partition_id != input_df[\"GROUP_IDENTIFIER_STRING\"][0]:\n",
    "            self.partition_id = input_df[\"GROUP_IDENTIFIER_STRING\"][0]\n",
    "            # Get the model binary from the first row of the input DataFrame where the column is not null\n",
    "            self.model = pickle.loads(\n",
    "                input_df.loc[\n",
    "                    input_df[\"MODEL_BINARY\"].first_valid_index(), \"MODEL_BINARY\"\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        model_output = self.model.predict(input_df[model_input_predictor_features])\n",
    "        res = pd.DataFrame(model_output, columns=[\"_PRED_\"])\n",
    "        res[\"GROUP_IDENTIFIER_STRING\"] = input_df[\"GROUP_IDENTIFIER_STRING\"]\n",
    "        res[TIME_PERIOD_COLUMN] = input_df[TIME_PERIOD_COLUMN]\n",
    "        return res\n",
    "\n",
    "\n",
    "m = ForecastingModelPickleInput()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Log Model to Model Registry\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Define the input/output signature\n",
    "input_signature = [\n",
    "    FeatureSpec(dtype=DataType.from_snowpark_type(field.datatype), name=field.name)\n",
    "    for field in final_sdf.drop(\"GROUP_IDENTIFIER\").schema\n",
    "] + [FeatureSpec(dtype=DataType.BYTES, name=\"MODEL_BINARY\")]\n",
    "\n",
    "\n",
    "output_signature = [\n",
    "    FeatureSpec(dtype=DataType.FLOAT, name=\"_PRED_\"),\n",
    "    FeatureSpec(dtype=DataType.STRING, name=\"GROUP_IDENTIFIER_STRING_OUT_\"),\n",
    "    FeatureSpec(dtype=DataType.STRING, name=f\"{TIME_PERIOD_COLUMN}_OUT_\"),\n",
    "]\n",
    "\n",
    "signature = ModelSignature(\n",
    "    inputs=input_signature,\n",
    "    outputs=output_signature,\n",
    ")\n",
    "\n",
    "# Log the model to the model registry\n",
    "options = {\"function_type\": \"TABLE_FUNCTION\", \"relax_version\": False}\n",
    "metrics_to_log = {\n",
    "    \"direct_multi_step_forecasting\": train_separate_lead_models,\n",
    "    \"frequency\": modeling_frequency,\n",
    "    \"training_data_start\": training_dttm_boundaries[\"MIN_DTTM\"].strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S\"\n",
    "    ),\n",
    "    \"training_data_end\": training_dttm_boundaries[\"MAX_DTTM\"].strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S\"\n",
    "    ),\n",
    "    \"user_settings\": user_settings_dict,\n",
    "}\n",
    "mv = reg.log_model(\n",
    "    m,\n",
    "    model_name=qualified_model_name,\n",
    "    options=options,\n",
    "    metrics=metrics_to_log,\n",
    "    conda_dependencies=[\"pandas\", \"xgboost\"],\n",
    "    signatures={\"predict\": signature},\n",
    "    comment=query_tag,\n",
    ")\n",
    "\n",
    "# In addition to setting the query tag for the model version, we also set it for the model itself\n",
    "reg.get_model(qualified_model_name).comment = query_tag\n",
    "\n",
    "print(f\"Model version name: {mv.version_name}\")\n",
    "\n",
    "# Confirm that the new model/version is in the registry\n",
    "reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce110000-1111-2222-3333-ffffff000022",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_save_version_if_desired"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version 'SHAGGY_LEECH_2' saved to model storage table and set as the default version in the registry.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>comment</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>is_default_version</th>\n",
       "      <th>functions</th>\n",
       "      <th>metadata</th>\n",
       "      <th>user_data</th>\n",
       "      <th>model_attributes</th>\n",
       "      <th>size</th>\n",
       "      <th>environment</th>\n",
       "      <th>runnable_in</th>\n",
       "      <th>inference_services</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-03-24 19:06:00.833000-07:00</td>\n",
       "      <td>SPICY_OWL_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...</td>\n",
       "      <td>7683</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.9\",\"snowflake-...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-03-24 20:02:14.798000-07:00</td>\n",
       "      <td>QUIET_BEAR_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...</td>\n",
       "      <td>7683</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.9\",\"snowflake-...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-03-24 20:10:20.058000-07:00</td>\n",
       "      <td>SHAGGY_LEECH_2</td>\n",
       "      <td>[\"DEFAULT\",\"LAST\"]</td>\n",
       "      <td>{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...</td>\n",
       "      <td>FORECAST_MODEL_BUILDER</td>\n",
       "      <td>MODELING</td>\n",
       "      <td>TEST_MODEL_1</td>\n",
       "      <td>true</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"direct_multi_step_forecasting\": ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...</td>\n",
       "      <td>8347</td>\n",
       "      <td>{\"default\":{\"python_version\":\"3.11\",\"snowflake...</td>\n",
       "      <td>[\"WAREHOUSE\"]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on            name             aliases  \\\n",
       "5 2025-03-24 19:06:00.833000-07:00     SPICY_OWL_1                  []   \n",
       "6 2025-03-24 20:02:14.798000-07:00    QUIET_BEAR_2                  []   \n",
       "7 2025-03-24 20:10:20.058000-07:00  SHAGGY_LEECH_2  [\"DEFAULT\",\"LAST\"]   \n",
       "\n",
       "                                             comment           database_name  \\\n",
       "5  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "6  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "7  {\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", ...  FORECAST_MODEL_BUILDER   \n",
       "\n",
       "  schema_name    model_name is_default_version    functions  \\\n",
       "5    MODELING  TEST_MODEL_1              false  [\"PREDICT\"]   \n",
       "6    MODELING  TEST_MODEL_1              false  [\"PREDICT\"]   \n",
       "7    MODELING  TEST_MODEL_1               true  [\"PREDICT\"]   \n",
       "\n",
       "                                            metadata user_data  \\\n",
       "5  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "6  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "7  {\"metrics\": {\"direct_multi_step_forecasting\": ...        {}   \n",
       "\n",
       "                                    model_attributes  size  \\\n",
       "5  {\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...  7683   \n",
       "6  {\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...  7683   \n",
       "7  {\"framework\":\"custom\",\"task\":\"UNKNOWN\",\"client...  8347   \n",
       "\n",
       "                                         environment    runnable_in  \\\n",
       "5  {\"default\":{\"python_version\":\"3.9\",\"snowflake-...  [\"WAREHOUSE\"]   \n",
       "6  {\"default\":{\"python_version\":\"3.9\",\"snowflake-...  [\"WAREHOUSE\"]   \n",
       "7  {\"default\":{\"python_version\":\"3.11\",\"snowflake...  [\"WAREHOUSE\"]   \n",
       "\n",
       "  inference_services  \n",
       "5                 []  \n",
       "6                 []  \n",
       "7                 []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Save the model version to the model storage table\n",
    "#   and set it as the default version in the registry\n",
    "# --------------------------------------------------------\n",
    "\n",
    "if SAVE_MODEL_VERSION_THIS_RUN:\n",
    "    # Append model binaries and metadata to the model binary storage table in Snowflake\n",
    "    udtf_models_w_version = udtf_models.with_column(\n",
    "        \"MODEL_VERSION\", F.lit(mv.version_name)\n",
    "    ).select(session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\").columns)\n",
    "\n",
    "    udtf_models_w_version.write.save_as_table(\n",
    "        f\"{MODEL_BINARY_STORAGE_TBL_NM}\", mode=\"append\"\n",
    "    )\n",
    "\n",
    "    # Set default version of the model to this version name\n",
    "    reg.get_model(qualified_model_name).default = mv.version_name\n",
    "\n",
    "    print(\n",
    "        f\"Model version '{mv.version_name}' saved to model storage table and set as the default version in the registry.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"\"\"Model version '{mv.version_name}' was NOT saved to the model storage table and will be deleted from the registry at the end of this notebook.\n",
    "    If you wish to save this version, set SAVE_MODEL_VERSION_THIS_RUN = True.\"\"\"\n",
    "    )\n",
    "\n",
    "# Look at the most recent 3 versions of the model\n",
    "reg.get_model(qualified_model_name).show_versions().tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "name": "md_model_evaluation"
   },
   "source": [
    "-----\n",
    "# MODEL EVALUATION\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce110000-1111-2222-3333-ffffff000025",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_test_feature_eng"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# TEST SET FEATURE ENGINEERING\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# First Convert Decimal data types to Floats (because DecimalType doesn't work in modeling algorithms)\n",
    "sdf_converted = sdf.select(\n",
    "    [\n",
    "        (\n",
    "            F.col(field.name).cast(T.FloatType()).alias(field.name)\n",
    "            if isinstance(field.datatype, T.DecimalType)\n",
    "            else F.col(field.name)\n",
    "        )\n",
    "        for field in sdf.schema\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ROLL UP to specified frequency\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_rollup_test = roll_up(\n",
    "    sdf_converted,\n",
    "    TIME_PERIOD_COLUMN,\n",
    "    PARTITION_COLUMNS,\n",
    "    TARGET_COLUMN,\n",
    "    EXOGENOUS_COLUMNS,\n",
    "    ROLLUP_FREQUENCY,\n",
    "    ROLLUP_AGGREGATIONS,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create time-derived features\n",
    "# ------------------------------------------------------------------------\n",
    "sdf_engineered_test = expand_datetime(\n",
    "    sdf_rollup_test, TIME_PERIOD_COLUMN, modeling_frequency\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create rolling average of most recent time periods\n",
    "# ------------------------------------------------------------------------\n",
    "# NOTE: We can only generate recent rolling average features if we are training separate lead models.\n",
    "if CREATE_LAG_FEATURE & train_separate_lead_models:\n",
    "    sdf_engineered_test = recent_rolling_avg(\n",
    "        sdf_engineered_test, [TARGET_COLUMN], window_spec, modeling_frequency\n",
    "    )\n",
    "\n",
    "# Cached Results\n",
    "sdf_engineered_test = sdf_engineered_test.cache_result()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Create LAG features (and possibly LEAD feature) of the TARGET variable\n",
    "# ------------------------------------------------------------------------\n",
    "final_sdf_all = apply_functions_in_a_loop(\n",
    "    train_separate_lead_models=train_separate_lead_models,\n",
    "    partition_column_list=PARTITION_COLUMNS,\n",
    "    input_sdf=sdf_engineered_test,\n",
    "    target_column=TARGET_COLUMN,\n",
    "    time_step_frequency=modeling_frequency,\n",
    "    forecast_horizon=FORECAST_HORIZON,\n",
    "    w_spec=window_spec,\n",
    "    create_lag_feature=CREATE_LAG_FEATURE,\n",
    ")\n",
    "\n",
    "\n",
    "# Cache the final SDF\n",
    "final_sdf_all = final_sdf_all.cache_result()\n",
    "\n",
    "print(\"Validation set feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000024",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_test_split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set row count: 22750\n",
      "First time period in test set: 2024-10-11 00:00:00\n",
      "Last time period in test set:  2025-01-09 00:00:00\n",
      "Total Partition Count: 250\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDER_TIMESTAMP\"    |\"TARGET\"       |\"FEATURE_1\"   |\"YEAR\"  |\"MONTH_SIN\"         |\"MONTH_COS\"          |\"WEEK_OF_YEAR_SIN\"   |\"WEEK_OF_YEAR_COS\"   |\"DAY_OF_WEEK_SUN\"  |\"DAY_OF_WEEK_MON\"  |\"DAY_OF_WEEK_TUE\"  |\"DAY_OF_WEEK_WED\"  |\"DAY_OF_WEEK_THU\"  |\"DAY_OF_WEEK_FRI\"  |\"DAY_OF_WEEK_SAT\"  |\"DAY_OF_YEAR_SIN\"    |\"DAY_OF_YEAR_COS\"    |\"DAYS_SINCE_JAN2020\"  |\"MODEL_TARGET\"  |\"GROUP_IDENTIFIER\"  |\"GROUP_IDENTIFIER_STRING\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2024-10-11 00:00:00  |203.160877549  |46.55032257   |2024    |-0.866025403784439  |0.49999999999999933  |-0.9709418174260519  |0.23931566428755824  |0                  |0                  |0                  |0                  |0                  |1                  |0                  |-0.9813064702716096  |0.19245158197082907  |1745                  |203.160877549   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |}                   |                           |\n",
      "|2024-10-12 00:00:00  |203.690051318  |269.96818042  |2024    |-0.866025403784439  |0.49999999999999933  |-0.9709418174260519  |0.23931566428755824  |0                  |0                  |0                  |0                  |0                  |0                  |1                  |-0.9778483415056569  |0.209314645963048    |1746                  |203.690051318   |{                   |STORE_ID_22_PRODUCT_ID_8   |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |  \"PRODUCT_ID\": 8,  |                           |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |  \"STORE_ID\": 22    |                           |\n",
      "|                     |               |              |        |                    |                     |                     |                     |                   |                   |                   |                   |                   |                   |                   |                     |                     |                      |                |}                   |                           |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# TEST SET DATAFRAME\n",
    "# ------------------------------------------------------------------------\n",
    "# NOTE: We separate the test set AFTER the feature engineering in case there are lag features\n",
    "\n",
    "if VALIDATION_DAYS == 0:\n",
    "    final_sdf_test = final_sdf_all\n",
    "    print(\n",
    "        \"NOTE: VALIDATION_DAYS is set to 0, so no test set was created. Instead, all metrics will be calculated on the training set.\"\n",
    "    )\n",
    "elif VALIDATION_DAYS > 0:\n",
    "    # Remove the validation records from the training set\n",
    "    final_sdf_test = final_sdf_all.filter(\n",
    "        F.date_trunc(\"day\", TIME_PERIOD_COLUMN)\n",
    "        >= F.dateadd(\"day\", F.lit(-VALIDATION_DAYS), F.lit(last_time_period))\n",
    "    )\n",
    "\n",
    "# Partition Count\n",
    "inference_partition_count = (\n",
    "    final_sdf_test.select(\"GROUP_IDENTIFIER_STRING\").distinct().count()\n",
    ")\n",
    "\n",
    "# Inspect the data\n",
    "dttm_boundaries = final_sdf_test.select(\n",
    "    F.min(TIME_PERIOD_COLUMN).alias(\"MIN_DTTM\"),\n",
    "    F.max(TIME_PERIOD_COLUMN).alias(\"MAX_DTTM\"),\n",
    ").collect()[0]\n",
    "\n",
    "if VALIDATION_DAYS == 0:\n",
    "    print(f\"Eval row count: {final_sdf_test.count()}\")\n",
    "else:\n",
    "    print(f\"Test set row count: {final_sdf_test.count()}\")\n",
    "print(f\"First time period in test set: {dttm_boundaries['MIN_DTTM']}\")\n",
    "print(f\"Last time period in test set:  {dttm_boundaries['MAX_DTTM']}\")\n",
    "if len(PARTITION_COLUMNS) > 0:\n",
    "    print(f\"Total Partition Count: {inference_partition_count}\")\n",
    "else:\n",
    "    print(\"No partitions specified.\")\n",
    "final_sdf_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000026",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_test_inference"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation is using model binaries created in this run. \n",
      "\n",
      "Inference data row count: 22750\n",
      "Largest partition record count: 91\n",
      "Number of partitions:   250\n",
      "Approx. Batch Size:     200\n",
      "Approx. Number of batches for largest partition: 1\n",
      "Predictions (row count: 22750)\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"GROUP_IDENTIFIER\"  |\"GROUP_IDENTIFIER_STRING\"  |\"INFERENCE_PERFORMED_ON_DTTM\"  |\"FUTURE_DTTM\"        |\"ACTUAL\"       |\"PREDICTED\"         |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|{                   |STORE_ID_17_PRODUCT_ID_3   |2025-03-24 23:10:40.947291     |2024-10-28 00:00:00  |509.766836792  |519.90771484375     |\n",
      "|  \"PRODUCT_ID\": 3,  |                           |                               |                     |               |                    |\n",
      "|  \"STORE_ID\": 17    |                           |                               |                     |               |                    |\n",
      "|}                   |                           |                               |                     |               |                    |\n",
      "|{                   |STORE_ID_17_PRODUCT_ID_3   |2025-03-24 23:10:40.947291     |2024-10-31 00:00:00  |490.214862199  |499.67340087890625  |\n",
      "|  \"PRODUCT_ID\": 3,  |                           |                               |                     |               |                    |\n",
      "|  \"STORE_ID\": 17    |                           |                               |                     |               |                    |\n",
      "|}                   |                           |                               |                     |               |                    |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# TEST SET INFERENCE\n",
    "# ------------------------------------------------------------------------\n",
    "# Establish inference DataFrame\n",
    "inference_input_df = final_sdf_test.drop(\"GROUP_IDENTIFIER\")\n",
    "\n",
    "# Establish model binary DataFrame\n",
    "if \"udtf_models\" in globals():\n",
    "    model_bytes_table = udtf_models.select(\"GROUP_IDENTIFIER_STRING\", \"MODEL_BINARY\")\n",
    "    print(\"Evaluation is using model binaries created in this run. \\n\")\n",
    "else:\n",
    "    model_bytes_table = (\n",
    "        session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\")\n",
    "        .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "        .filter(\n",
    "            F.col(\"MODEL_VERSION\")\n",
    "            == reg.get_model(qualified_model_name).default.version_name\n",
    "        )\n",
    "        .select(\n",
    "            \"GROUP_IDENTIFIER_STRING\",\n",
    "            \"MODEL_BINARY\",\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Evaluation is using model binaries from the model registry default model version (previously-created). \\n\"\n",
    "    )\n",
    "\n",
    "# Join model binary object to inference input data\n",
    "inference_input_df = inference_input_df.join(\n",
    "    # model_bytes_table, on=[\"GROUP_IDENTIFIER_STRING\", \"PARTITION_ROW_NUMBER\"], how=\"left\"\n",
    "    model_bytes_table,\n",
    "    on=[\"GROUP_IDENTIFIER_STRING\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Add a column called BATCH_GROUP,\n",
    "#   which has the property that for each unique value there are roughly the number of records specified in batch_size.\n",
    "# Use that to create a PARTITION_ID column that will be used to run inference in batches.\n",
    "# We do this to avoid running out of memory when performing inference on a large number of records.\n",
    "largest_partition_record_count = (\n",
    "    final_sdf_test.group_by(\"GROUP_IDENTIFIER_STRING\")\n",
    "    .agg(F.count(\"*\").alias(\"PARTITION_RECORD_COUNT\"))\n",
    "    .agg(F.max(\"PARTITION_RECORD_COUNT\").alias(\"MAX_PARTITION_RECORD_COUNT\"))\n",
    "    .collect()[0][\"MAX_PARTITION_RECORD_COUNT\"]\n",
    ")\n",
    "batch_size = INFERENCE_APPROX_BATCH_SIZE\n",
    "number_of_batches = math.ceil(largest_partition_record_count / batch_size)\n",
    "inference_input_df = (\n",
    "    inference_input_df.with_column(\n",
    "        \"BATCH_GROUP\", F.abs(F.random(123)) % F.lit(number_of_batches)\n",
    "    )\n",
    "    .with_column(\n",
    "        \"PARTITION_ID\",\n",
    "        F.concat_ws(\n",
    "            F.lit(\"__\"), F.col(\"GROUP_IDENTIFIER_STRING\"), F.col(\"BATCH_GROUP\")\n",
    "        ),\n",
    "    )\n",
    "    .drop(\"RANDOM_NUMBER\", \"BATCH_GROUP\")\n",
    ")\n",
    "\n",
    "# Stats related to inference dataset\n",
    "print(f\"Inference data row count: {final_sdf_test.count()}\")\n",
    "print(f\"Largest partition record count: {largest_partition_record_count}\")\n",
    "print(f\"Number of partitions:   {inference_partition_count}\")\n",
    "print(f\"Approx. Batch Size:     {batch_size}\")\n",
    "print(f\"Approx. Number of batches for largest partition: {number_of_batches}\")\n",
    "\n",
    "# Perform inference from the model registry\n",
    "session.use_warehouse(MODELING_WH)\n",
    "\n",
    "inference_result = mv.run(\n",
    "    inference_input_df,\n",
    "    partition_column=\"PARTITION_ID\",\n",
    ").select(\n",
    "    \"_PRED_\",\n",
    "    F.col(\"GROUP_IDENTIFIER_STRING_OUT_\").alias(\"GROUP_IDENTIFIER_STRING\"),\n",
    "    F.col(f\"{TIME_PERIOD_COLUMN}_OUT_\").alias(TIME_PERIOD_COLUMN),\n",
    ")\n",
    "\n",
    "# Bring in the ACTUALS as well as the GROUP_IDENTIFIER variant column into the results for easier filtering during evaluation\n",
    "inference_result = inference_result.join(\n",
    "    final_sdf_test.select(\n",
    "        \"GROUP_IDENTIFIER\",\n",
    "        \"GROUP_IDENTIFIER_STRING\",\n",
    "        TIME_PERIOD_COLUMN,\n",
    "        \"MODEL_TARGET\",\n",
    "    ),\n",
    "    on=[\"GROUP_IDENTIFIER_STRING\", TIME_PERIOD_COLUMN],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add a column for the date on which we are running inference and a column for the future date for which we are forecasting\n",
    "if train_separate_lead_models:\n",
    "    inference_result = (\n",
    "        inference_result.with_column_renamed(\n",
    "            TIME_PERIOD_COLUMN, \"INFERENCE_PERFORMED_ON_DTTM\"\n",
    "        )\n",
    "        .with_column(\n",
    "            \"FUTURE_DTTM\",\n",
    "            F.dateadd(\n",
    "                modeling_frequency,\n",
    "                F.col(\"GROUP_IDENTIFIER\").getItem(\"LEAD\"),\n",
    "                F.col(\"INFERENCE_PERFORMED_ON_DTTM\"),\n",
    "            ),\n",
    "        )\n",
    "        .select(\n",
    "            \"GROUP_IDENTIFIER\",\n",
    "            \"GROUP_IDENTIFIER_STRING\",\n",
    "            \"INFERENCE_PERFORMED_ON_DTTM\",\n",
    "            \"FUTURE_DTTM\",\n",
    "            F.col(\"MODEL_TARGET\").alias(\"ACTUAL\"),\n",
    "            F.col(\"_PRED_\").alias(\"PREDICTED\"),\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    inference_dttm = datetime.now()\n",
    "    inference_result = (\n",
    "        inference_result.with_column(\n",
    "            \"INFERENCE_PERFORMED_ON_DTTM\", F.lit(inference_dttm)\n",
    "        )\n",
    "        .with_column_renamed(TIME_PERIOD_COLUMN, \"FUTURE_DTTM\")\n",
    "        .select(\n",
    "            \"GROUP_IDENTIFIER\",\n",
    "            \"GROUP_IDENTIFIER_STRING\",\n",
    "            \"INFERENCE_PERFORMED_ON_DTTM\",\n",
    "            \"FUTURE_DTTM\",\n",
    "            F.col(\"MODEL_TARGET\").alias(\"ACTUAL\"),\n",
    "            F.col(\"_PRED_\").alias(\"PREDICTED\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Cache result\n",
    "inference_result = inference_result.cache_result()\n",
    "\n",
    "# Switch back to the original warehouse\n",
    "session.use_warehouse(SESSION_WH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66d4bf7f-b360-4e5e-976a-0a28f3c5e45b",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_set_lead_if_multi_lead_modeling"
   },
   "outputs": [],
   "source": [
    "# Filter to a single lead for evaluation\n",
    "if train_separate_lead_models:\n",
    "    inference_result = inference_result.filter(\n",
    "        F.col(\"GROUP_IDENTIFIER_STRING\").endswith(\"LEAD_1\")\n",
    "    )\n",
    "\n",
    "# Write the validation scores to a Snowflake table in case the user just wants to re-run the model performance cells without re-running the model training cells.\n",
    "inference_result.write.save_as_table(\n",
    "    f\"VALIDATION_PREDS_FROM_{MODEL_NAME}\",\n",
    "    mode=\"overwrite\",\n",
    "    comment='{\"origin\":\"sf_sit\", \"name\":\"sit_forecasting\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"component\":\"validation\"}}',\n",
    ")\n",
    "print(\n",
    "    f\"Validation predictions saved to Snowflake table: {session_db_schema}.VALIDATION_PREDS_FROM_{MODEL_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf02e1b-4ad1-4ddd-9383-44f58d47cfd3",
   "metadata": {
    "collapsed": false,
    "name": "md_entry_point_explanation"
   },
   "source": [
    "If users walk away from the notebook long enough for __the notebook session to end__ after it finishes running, they have the option to come back to the model evaluation section below at a later time. The __scored validation set__ was saved as a __Snowflake table__ after model training. So users can __re-activate__ this notebook and __re-run the evaluation cells__ below __without having to re-run inference on the validation set__. \n",
    "\n",
    "If the notebook has become inactive, and users wish to re-run the evalution cells below, they should follow these steps: \n",
    "1. Click __Start__ in the upper right corner of the notebook to activate a new session\n",
    "2. At the top of this notebook, __run all of the cells above__ the ___md_model_training___ markdown cell\n",
    "3. Run the ___test_feature_eng___ and the ___test_split___ cells above. (No need to re-run the inference cell.)\n",
    "4. Click the 3 dots in the upper right corner of the cell below (___validation_scores_sdf___) and select __\"Run all below\"__ to re-run all the evaluation cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_validation_scores_sdf"
   },
   "outputs": [],
   "source": [
    "# validation_scores_sdf cell\n",
    "\n",
    "# Create a DataFrame from the saved table\n",
    "validation_scores = session.table(f\"VALIDATION_PREDS_FROM_{MODEL_NAME}\")\n",
    "\n",
    "# Look at a couple rows of predictions\n",
    "print(f\"Number of partitions:  {inference_partition_count}\")\n",
    "print(f\"Validation row count:  {validation_scores.count()}\")\n",
    "validation_scores.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdb832-ae9d-4ff6-88c8-803012c36416",
   "metadata": {
    "collapsed": false,
    "name": "md_overall_performance"
   },
   "source": [
    "# Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35beb03a-c85a-456a-b43b-486583cb217e",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_overal_metrics"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:10:57.301 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.346 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/kmason/Documents/GitLab/forecast-model-builder/.venv/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-24 23:10:57.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Row-level metrics\n",
    "row_actual_v_fcst = (\n",
    "    validation_scores.with_column(\n",
    "        \"ABS_ERROR\", F.abs(F.col(\"ACTUAL\") - F.col(\"PREDICTED\"))\n",
    "    )\n",
    "    .with_column(\n",
    "        \"APE\",\n",
    "        F.when(F.col(\"ACTUAL\") == 0, F.lit(None)).otherwise(\n",
    "            F.abs(F.col(\"ABS_ERROR\") / F.col(\"ACTUAL\"))\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"SQ_ERROR\", F.pow(F.col(\"ACTUAL\") - F.col(\"PREDICTED\"), 2))\n",
    ")\n",
    "\n",
    "# Metrics per partition\n",
    "partition_metrics = row_actual_v_fcst.group_by(\"GROUP_IDENTIFIER_STRING\").agg(\n",
    "    F.avg(\"APE\").alias(\"MAPE\"),\n",
    "    F.avg(\"ABS_ERROR\").alias(\"MAE\"),\n",
    "    F.sqrt(F.avg(\"SQ_ERROR\")).alias(\"RMSE\"),\n",
    ")\n",
    "\n",
    "# Overall modeling process across all partitions\n",
    "overall_avg_metrics = partition_metrics.agg(\n",
    "    F.avg(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "    F.avg(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "    F.avg(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    ").with_column(\"AGGREGATION\", F.lit(\"AVG\"))\n",
    "overall_median_metrics = partition_metrics.agg(\n",
    "    F.median(\"MAPE\").alias(\"OVERALL_MAPE\"),\n",
    "    F.median(\"MAE\").alias(\"OVERALL_MAE\"),\n",
    "    F.median(\"RMSE\").alias(\"OVERALL_RMSE\"),\n",
    ").with_column(\"AGGREGATION\", F.lit(\"MEDIAN\"))\n",
    "overall_metrics = overall_avg_metrics.union(overall_median_metrics).select(\n",
    "    \"AGGREGATION\", \"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\"\n",
    ")\n",
    "\n",
    "# Show the metrics\n",
    "if inference_partition_count == 1:\n",
    "    st.write(\n",
    "        \"There is only 1 partition, so these values are the metrics for that single model:\"\n",
    "    )\n",
    "    st.dataframe(\n",
    "        overall_median_metrics.select(\"OVERALL_MAPE\", \"OVERALL_MAE\", \"OVERALL_RMSE\"),\n",
    "        use_container_width=True,\n",
    "    )\n",
    "else:\n",
    "    st.write(\"Avg and Median of each metric over all the partitions:\")\n",
    "    st.dataframe(overall_metrics, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43114e5-856d-4b59-a4c3-18205c28149e",
   "metadata": {
    "collapsed": false,
    "name": "md_partition_importance"
   },
   "source": [
    "# Partition Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c81b5b2-25df-4588-a48d-6addf7187b57",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_metric_distribution"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:10:57.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.721 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-24 23:10:57.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:57.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.385 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:10:58.934 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "if (len(PARTITION_COLUMNS) > 0) & (inference_partition_count > 1):\n",
    "    # Metric Distribution plot with dynamic filtering\n",
    "    metric = st.selectbox(\"Metric\", [\"MAPE\", \"MAE\", \"RMSE\"])\n",
    "    st.subheader(f\"{metric} Distribution\")\n",
    "    distribution_df = partition_metrics.to_pandas()\n",
    "\n",
    "    # Add a slider to filter outliers\n",
    "    value_min, value_max = st.slider(\n",
    "        f\"Filter {metric} range in plot:\",\n",
    "        float(distribution_df[metric].min()),\n",
    "        float(distribution_df[metric].max()),\n",
    "        (float(distribution_df[metric].min()), float(distribution_df[metric].max())),\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame based on the slider values\n",
    "    filtered_df = distribution_df[\n",
    "        (distribution_df[metric] >= value_min) & (distribution_df[metric] <= value_max)\n",
    "    ]\n",
    "\n",
    "    fig = px.box(\n",
    "        filtered_df,\n",
    "        x=metric,  # Horizontal orientation\n",
    "        points=\"all\",  # Show individual data points as dots\n",
    "        title=f\"{metric} Distribution ({value_min:.2f} - {value_max:.2f})\",\n",
    "        labels={metric: metric, \"GROUP_IDENTIFIER_STRING\": \"Partition\"},\n",
    "        hover_data=[\"GROUP_IDENTIFIER_STRING\"],  # Add this for hover info\n",
    "    )\n",
    "\n",
    "    fig.update_layout(template=\"plotly_white\", showlegend=True)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # Layout with two columns\n",
    "    col1, col2 = st.columns(2)\n",
    "\n",
    "    # Column 1: Tables\n",
    "    with col1:\n",
    "        # Look at the best performing partitions\n",
    "        st.subheader(\"BEST Performing Partitions\")\n",
    "        st.dataframe(partition_metrics.sort(F.abs(metric)))\n",
    "    with col2:\n",
    "        # Look at the worst performing partitions\n",
    "        st.subheader(\"WORST Performing Partitions\")\n",
    "        st.dataframe(partition_metrics.sort(F.abs(metric).desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b05ef0-02ab-4040-9dd3-b76468420a9e",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_plots"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Visualize individual partition actual vs pred on a time series line chart\n",
    "# ------------------------------------------------------------------------------\n",
    "# Select a single partition to visualize\n",
    "partitions = sorted(\n",
    "    validation_scores.select(\"GROUP_IDENTIFIER_STRING\").distinct().collect()\n",
    ")\n",
    "partition_choice = st.selectbox(\"Partition\", partitions)\n",
    "# Create a pandas dataframe\n",
    "partition_choice_df = (\n",
    "    validation_scores.filter(F.col(\"GROUP_IDENTIFIER_STRING\") == partition_choice)\n",
    "    .sort(\"FUTURE_DTTM\")\n",
    "    .to_pandas()\n",
    ")\n",
    "partition_choice_df[\"FUTURE_DTTM\"] = pd.to_datetime(partition_choice_df[\"FUTURE_DTTM\"])\n",
    "tabs = st.tabs(\n",
    "    [\n",
    "        \"Line Plot: Validation Actual & Predicted\",\n",
    "        \"Scatter Plot: Validation Actual vs. Predicted\",\n",
    "        \"Line Plot: Training Actuals\",\n",
    "    ]\n",
    ")\n",
    "# Validation Actuals & Predictions Line Plot\n",
    "tabs[0].line_chart(partition_choice_df, x=\"FUTURE_DTTM\", y=[\"ACTUAL\", \"PREDICTED\"])\n",
    "\n",
    "# Validation Actuals vs. Predictions Scatter Plot\n",
    "\n",
    "# Plot: Prediction vs. Actual\n",
    "fig_scatter = px.scatter(\n",
    "    partition_choice_df,\n",
    "    x=\"ACTUAL\",\n",
    "    y=\"PREDICTED\",\n",
    "    title=\"Predicted vs. Actual Visits\",\n",
    "    labels={\"VISITS\": \"Actual Visits\", \"PREDICTION\": \"Predicted Visits\"},\n",
    "    opacity=0.6,\n",
    "    trendline=\"ols\",  # Add trendline (linear regression)\n",
    "    hover_data=[\"PREDICTED\", \"ACTUAL\", \"FUTURE_DTTM\"],  # Include date in hover\n",
    ")\n",
    "\n",
    "# Add expected trendline (y = x)\n",
    "min_visits = min(partition_choice_df[\"ACTUAL\"])\n",
    "max_visits = max(partition_choice_df[\"ACTUAL\"])\n",
    "\n",
    "fig_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[min_visits, max_visits],\n",
    "        y=[min_visits, max_visits],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", dash=\"dash\"),\n",
    "        name=\"Expected Trend (y = x)\",  # Add to legend\n",
    "        showlegend=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Render the plot in Streamlit\n",
    "tabs[1].plotly_chart(fig_scatter, use_container_width=True)\n",
    "\n",
    "# Look at the partition's actuals in the TRAINING set to assess the overall trens\n",
    "partition_choice_df = (\n",
    "    final_sdf.filter(F.col(\"GROUP_IDENTIFIER_STRING\") == partition_choice)\n",
    "    .sort(TIME_PERIOD_COLUMN)\n",
    "    .to_pandas()\n",
    ")\n",
    "partition_choice_df[TIME_PERIOD_COLUMN] = pd.to_datetime(\n",
    "    partition_choice_df[TIME_PERIOD_COLUMN]\n",
    ")\n",
    "# Plot the data\n",
    "if (TARGET_COLUMN.startswith('\"') and TARGET_COLUMN.endswith('\"')) or (\n",
    "    TARGET_COLUMN.startswith(\"'\") and TARGET_COLUMN.endswith(\"'\")\n",
    "):\n",
    "    y_name = TARGET_COLUMN[1:-1]\n",
    "else:\n",
    "    y_name = TARGET_COLUMN\n",
    "tabs[2].line_chart(partition_choice_df, x=TIME_PERIOD_COLUMN, y=y_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75797aa6-773b-4f43-adff-67b36678c755",
   "metadata": {
    "collapsed": false,
    "name": "md_feature_importance"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6501d20-ec07-4a88-a158-7cae7b42ce8a",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_feature_importance"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:11:01.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:01.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Establish model binary DataFrame\n",
    "if \"udtf_models\" in globals():\n",
    "    models_sdf = udtf_models\n",
    "else:\n",
    "    models_sdf = (\n",
    "        session.table(f\"{MODEL_BINARY_STORAGE_TBL_NM}\")\n",
    "        .filter(F.col(\"MODEL_NAME\") == MODEL_NAME)\n",
    "        .filter(\n",
    "            F.col(\"MODEL_VERSION\")\n",
    "            == reg.get_model(qualified_model_name).default.version_name\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"Feature Importances are for model version {reg.get_model(qualified_model_name).default.version_name} in table {MODEL_BINARY_STORAGE_TBL_NM}.\"\n",
    "    )\n",
    "\n",
    "model_df = models_sdf.select(\n",
    "    \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"METADATA\"\n",
    ").to_pandas()\n",
    "\n",
    "\n",
    "def preprocess_model_data(df):\n",
    "    \"\"\"Preprocess model data by extracting feature importance from the METADATA column.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts the \"feature_importance\" dictionary from the \"METADATA\" column.\n",
    "    2. Converts the extracted feature importance data into a new DataFrame where each row\n",
    "       represents a feature and its corresponding importance for a specific model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing model data with at least\n",
    "                           the columns \"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\",\n",
    "                           and \"METADATA\".\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The original DataFrame with an additional \"FEATURE_IMPORTANCE\" column.\n",
    "            - pd.DataFrame: A new DataFrame containing the extracted features and their importance,\n",
    "              with columns [\"MODEL_NAME\", \"GROUP_IDENTIFIER_STRING\", \"FEATURE\", \"IMPORTANCE\"].\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract feature importance from METADATA\n",
    "    df[\"FEATURE_IMPORTANCE\"] = df[\"METADATA\"].apply(\n",
    "        lambda x: (\n",
    "            json.loads(x).get(\"feature_importance\", {})\n",
    "            if isinstance(x, str)\n",
    "            else x.get(\"feature_importance\", {})\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Explode feature importance into rows\n",
    "    feature_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        for feature, importance in row[\"FEATURE_IMPORTANCE\"].items():\n",
    "            feature_rows.append(\n",
    "                {\n",
    "                    \"MODEL_NAME\": row[\"MODEL_NAME\"],\n",
    "                    \"GROUP_IDENTIFIER_STRING\": row[\"GROUP_IDENTIFIER_STRING\"],\n",
    "                    \"FEATURE\": feature,\n",
    "                    \"IMPORTANCE\": importance,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    feature_df = pd.DataFrame(feature_rows)\n",
    "    return df, feature_df\n",
    "\n",
    "\n",
    "def calculate_average_rank(feature_df):\n",
    "    \"\"\"Calculate the average rank and importance of features across different group partitions.\n",
    "\n",
    "    This function:\n",
    "    1. Computes the rank of each feature within its \"GROUP_IDENTIFIER_STRING\" based on\n",
    "       feature importance in descending order.\n",
    "    2. Aggregates the average rank and average importance for each feature across all groups.\n",
    "    3. Returns the feature DataFrame with calculated ranks and a summarized DataFrame\n",
    "       sorted by average rank.\n",
    "\n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): Input DataFrame containing extracted feature importance\n",
    "                                   with at least the columns [\"GROUP_IDENTIFIER_STRING\",\n",
    "                                   \"FEATURE\", \"IMPORTANCE\"].\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The input DataFrame with an additional \"RANK\" column.\n",
    "            - pd.DataFrame: A new DataFrame containing features and their average rank and\n",
    "              importance, with columns [\"FEATURE\", \"AVERAGE_RANK\", \"AVERAGE_IMPORTANCE\"].\n",
    "\n",
    "    \"\"\"\n",
    "    feature_df = feature_df.copy()\n",
    "    feature_df.loc[:, \"RANK\"] = feature_df.groupby(\"GROUP_IDENTIFIER_STRING\")[\n",
    "        \"IMPORTANCE\"\n",
    "    ].rank(ascending=False)\n",
    "\n",
    "    avg_rank_df = (\n",
    "        feature_df.groupby(\"FEATURE\")\n",
    "        .agg({\"RANK\": \"mean\", \"IMPORTANCE\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    avg_rank_df.rename(\n",
    "        columns={\"RANK\": \"AVERAGE_RANK\", \"IMPORTANCE\": \"AVERAGE_IMPORTANCE\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    avg_rank_df = avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True)\n",
    "    return feature_df, avg_rank_df\n",
    "\n",
    "\n",
    "def plot_feature_importance(df, is_aggregated=True, top_n=20):\n",
    "    \"\"\"Create a horizontal bar plot to visualize feature importance.\n",
    "\n",
    "    This function generates a feature importance plot based on whether the data\n",
    "    is aggregated (showing average ranks across groups) or unaggregated (showing\n",
    "    importance for a selected partition).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing feature importance data.\n",
    "                           Expected columns:\n",
    "                           - If `is_aggregated=True`: [\"FEATURE\", \"AVERAGE_RANK\"]\n",
    "                           - If `is_aggregated=False`: [\"FEATURE\", \"IMPORTANCE\"]\n",
    "        is_aggregated (bool, optional): If True, plots average rank of features\n",
    "                                        across groups. If False, plots raw importance\n",
    "                                        for a single partition. Default is True.\n",
    "        top_n (int, optional): Number of top features to display in the plot.\n",
    "                               Default is 20.\n",
    "\n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: A bar plot visualizing the top feature importance.\n",
    "\n",
    "    \"\"\"\n",
    "    if is_aggregated:\n",
    "        df = df.sort_values(\"AVERAGE_RANK\", ascending=True).head(top_n)\n",
    "        x_col = \"AVERAGE_RANK\"\n",
    "        title = \"Top Feature Importance (Aggregated by Average Rank)\"\n",
    "        fig = px.bar(\n",
    "            df,\n",
    "            x=x_col,\n",
    "            y=\"FEATURE\",\n",
    "            orientation=\"h\",\n",
    "            title=title,\n",
    "            labels={\"FEATURE\": \"Feature\", x_col: \"Average Rank\"},\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(categoryorder=\"total descending\"),\n",
    "            xaxis_title=\"Average Rank\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            margin=dict(l=50, r=50, t=50, b=50),\n",
    "        )\n",
    "    else:\n",
    "        df = df.sort_values(\"IMPORTANCE\", ascending=False).head(top_n)\n",
    "        x_col = \"IMPORTANCE\"\n",
    "        title = \"Top Feature Importance for Selected Partition\"\n",
    "\n",
    "        fig = px.bar(\n",
    "            df,\n",
    "            x=x_col,\n",
    "            y=\"FEATURE\",\n",
    "            orientation=\"h\",\n",
    "            title=title,\n",
    "            labels={\"FEATURE\": \"Feature\", x_col: \"Importance\"},\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(categoryorder=\"total ascending\"),\n",
    "            xaxis_title=\"Importance\",\n",
    "            yaxis_title=\"Feature\",\n",
    "            margin=dict(l=50, r=50, t=50, b=50),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "model_df, feature_df = preprocess_model_data(model_df)\n",
    "\n",
    "# Select Partition Model ID\n",
    "partition_models = model_df[\"GROUP_IDENTIFIER_STRING\"].unique()\n",
    "selected_partition_model = st.selectbox(\n",
    "    \"Select Partition\", [None] + sorted(partition_models)\n",
    ")\n",
    "\n",
    "# Filter data based on selections\n",
    "filtered_feature_df = feature_df\n",
    "if selected_partition_model:\n",
    "    filtered_feature_df = filtered_feature_df[\n",
    "        filtered_feature_df[\"GROUP_IDENTIFIER_STRING\"] == selected_partition_model\n",
    "    ]\n",
    "\n",
    "# Select Top N Features\n",
    "top_n = st.slider(\"Number of Top Features to Show\", min_value=5, max_value=50, value=20)\n",
    "\n",
    "\n",
    "# Display Feature Importance\n",
    "st.subheader(\"Feature Importance\")\n",
    "\n",
    "if selected_partition_model:\n",
    "    fig = plot_feature_importance(filtered_feature_df, is_aggregated=False, top_n=top_n)\n",
    "else:\n",
    "    filtered_feature_df, avg_rank_df = calculate_average_rank(filtered_feature_df)\n",
    "    fig = plot_feature_importance(avg_rank_df, is_aggregated=True, top_n=top_n)\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Expander for Underlying Data\n",
    "with st.expander(\"Show Underlying Data\"):\n",
    "    if selected_partition_model:\n",
    "        st.dataframe(filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False))\n",
    "    else:\n",
    "        tabs = st.tabs([\"Average Importance\", \"Individual Importance\"])\n",
    "        tabs[0].dataframe(avg_rank_df.sort_values(\"AVERAGE_RANK\", ascending=True))\n",
    "        tabs[1].dataframe(\n",
    "            filtered_feature_df.sort_values(\"IMPORTANCE\", ascending=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7f8b2-007e-42a5-ab5f-af4afd615c70",
   "metadata": {
    "collapsed": false,
    "name": "_md_outlier_analysis"
   },
   "source": [
    "# Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee0b99-d373-42cf-a0f2-262a6c66411f",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_outlier_analysis"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:11:02.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.025 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.025 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.026 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.344 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.461 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.462 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:02.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.320 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.565 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.566 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.566 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.566 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.568 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.568 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 23:11:03.826 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row-level metrics\n",
    "n_outliers = st.slider(\"Number of Outliers\", 5, 1000, 20)\n",
    "outlier_sdf = (\n",
    "    validation_scores.with_column(\n",
    "        \"ABS_ERROR\", F.abs(F.col(\"ACTUAL\") - F.col(\"PREDICTED\"))\n",
    "    )\n",
    "    .with_column(\n",
    "        \"APE\",\n",
    "        F.when(F.col(\"ACTUAL\") == 0, F.lit(None)).otherwise(\n",
    "            F.abs(F.col(\"ABS_ERROR\") / F.col(\"ACTUAL\"))\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"SQ_ERROR\", F.pow(F.col(\"ACTUAL\") - F.col(\"PREDICTED\"), 2))\n",
    "    .order_by(F.col(\"APE\").desc())\n",
    "    .limit(n_outliers)\n",
    "    .select(\n",
    "        \"GROUP_IDENTIFIER_STRING\",\n",
    "        \"FUTURE_DTTM\",\n",
    "        \"ACTUAL\",\n",
    "        \"PREDICTED\",\n",
    "        \"ABS_ERROR\",\n",
    "        \"APE\",\n",
    "    )\n",
    ")\n",
    "\n",
    "st.dataframe(outlier_sdf, use_container_width=True)\n",
    "cols = st.columns(2)\n",
    "# Select a single partition to visualize\n",
    "outlier_partition = cols[0].selectbox(\n",
    "    \"Outlier Partition\", outlier_sdf.select(\"GROUP_IDENTIFIER_STRING\").distinct()\n",
    ")\n",
    "outlier_partition_df = outlier_sdf.filter(\n",
    "    F.col(\"GROUP_IDENTIFIER_STRING\") == outlier_partition\n",
    ")\n",
    "outlier_date = cols[1].selectbox(\n",
    "    \"Outlier Date\",\n",
    "    outlier_partition_df.select(F.col(\"FUTURE_DTTM\").cast(\"STRING\")).distinct(),\n",
    ")\n",
    "\n",
    "# Validation Actuals & Predictions Line Plot\n",
    "partition_df = (\n",
    "    validation_scores.filter(\n",
    "        F.col(\"GROUP_IDENTIFIER_STRING\") == outlier_partition\n",
    "    ).sort(\"FUTURE_DTTM\")\n",
    ").to_pandas()\n",
    "selected_date = pd.to_datetime(outlier_date)\n",
    "\n",
    "# Create time series plot\n",
    "fig = px.line(\n",
    "    partition_df,\n",
    "    x=\"FUTURE_DTTM\",\n",
    "    y=[\"ACTUAL\", \"PREDICTED\"],\n",
    "    markers=True,\n",
    "    title=f\"Actual vs Predicted: {outlier_partition}\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=[selected_date],\n",
    "    y=partition_df.loc[partition_df[\"FUTURE_DTTM\"] == selected_date, \"ACTUAL\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=12, color=\"red\", symbol=\"star\"),\n",
    "    name=\"Selected Outlier\",\n",
    ")\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "\n",
    "# View Features\n",
    "outlier_feature_sdf = final_sdf_test.filter(\n",
    "    (F.col(\"GROUP_IDENTIFIER_STRING\") == outlier_partition)\n",
    "    & (F.col(TIME_PERIOD_COLUMN).cast(\"STRING\") == outlier_date)\n",
    ")\n",
    "st.subheader(\"Date Features\")\n",
    "st.dataframe(\n",
    "    outlier_feature_sdf.drop(\n",
    "        \"GROUP_IDENTIFIER\", \"GROUP_IDENTIFIER_STRING\", \"MODEL_TARGET\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000032",
   "metadata": {
    "name": "md_clean_up"
   },
   "source": [
    "-----\n",
    "# Clean up\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000034",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "_delete_version"
   },
   "outputs": [],
   "source": [
    "# If we don't want to keep the version we just built, we can remove it from the registry\n",
    "\n",
    "# NOTE: Comment this code out if you do not want to delete the model version from the model registry\n",
    "# If the user does not want to save the current version, delete this version of the model from the registry.\n",
    "current_model = reg.get_model(qualified_model_name)\n",
    "\n",
    "if not SAVE_MODEL_VERSION_THIS_RUN:\n",
    "    deletion_message = \"\"\n",
    "    try:\n",
    "        current_model.version(mv.version_name)\n",
    "    except Exception:\n",
    "        deletion_message = f\"WARNING: Model version '{mv.version_name}' does not exist in the registry.\"\n",
    "        print(deletion_message)\n",
    "\n",
    "    if len(deletion_message) == 0:\n",
    "        try:\n",
    "            if len(current_model.versions()) == 0:\n",
    "                print(\n",
    "                    f\"WARNING: There are no versions for model '{MODEL_NAME}' in the registry.\"\n",
    "                )\n",
    "            elif (len(current_model.versions()) == 1) & (\n",
    "                current_model.default.version_name == mv.version_name\n",
    "            ):\n",
    "                reg.delete_model(MODEL_NAME)\n",
    "                print(\n",
    "                    f\" Model '{MODEL_NAME}' (which only had one version: '{mv.version_name}') was deleted from the registry.\"\n",
    "                )\n",
    "            else:\n",
    "                current_model.delete_version(mv.version_name)\n",
    "                print(\n",
    "                    f\"Model version '{mv.version_name}' was deleted from the registry.\"\n",
    "                )\n",
    "        except Exception:\n",
    "            print(\n",
    "                f\"WARNING: Model version '{mv.version_name}' was not able to be deleted from the registry.\"\n",
    "            )\n",
    "\n",
    "    reg.show_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "lastEditStatus": {
   "authorEmail": "kirk.mason@snowflake.com",
   "authorId": "340032177737",
   "authorName": "KMASON",
   "lastEditTime": 1745509329596,
   "notebookId": "3gkakoum5scrrntmgavo",
   "sessionId": "5f6fbba0-890e-43a6-9c50-ea191831313d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
