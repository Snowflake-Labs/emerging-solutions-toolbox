{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079fe083-c828-4c86-ad31-08d50310486c",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Replace Tasks with Dynamic Tables\n",
    "\n",
    "This notebook identifies eligible tasks that can be converted to Dynamic tables.  Dynamic tables simplify data engineering in Snowflake by providing a reliable, cost-effective, and automated way to transform data. Not every stream/task can or should be replaced.  \n",
    "\n",
    "This notebook will:\n",
    "- check actively running tasks in a Snowflake account to find tasks that `INSERT` or 'MERGE' into an existing table (from a base table) or create a table using CTAS\n",
    "- identify whether the target table is currently in a share\n",
    "    - **NOTE:** Data Providers should take additional steps to ensure any affected shared tables don't impact Consumers before switching to Dynamic tables\n",
    "- generate the DDL to create the Dynamic table that will replace the task\n",
    "    - **NOTE:** this will be done for each task in the task graph\n",
    "- execute the Dynamic table DDL and remove the existing stream/task (optional)\n",
    "- remove the existing target table from the share, if applicable (optional)\n",
    "- drop existing stream/task (optional)\n",
    "- drop the existing target table (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045de21-9f8c-443f-96c0-9a67c1b893d9",
   "metadata": {
    "collapsed": false,
    "name": "Prerequisites"
   },
   "source": [
    "## Prerequisites:\n",
    "\n",
    "- The user executing this notebook, must have access to the `SNOWFLAKE` database.\n",
    "- The user must have the `CREATE DYNAMIC TABLE` privilge on the schema where the new Dynamic Table will be created.\n",
    "- The user must own the tasks in the database(s) set in **STEP 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce2fba-7a16-4bc5-9929-afda2e534efb",
   "metadata": {
    "collapsed": false,
    "name": "Step_1_Label"
   },
   "source": [
    "## STEP 1: Initiaize Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7b910-866d-481d-9454-9053a2fcb075",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_1_Initialize_Session"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import streamlit as st\n",
    "from st_aggrid import AgGrid, GridUpdateMode, JsCode\n",
    "from st_aggrid.grid_options_builder import GridOptionsBuilder\n",
    "import sqlparse\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "#tag session\n",
    "session.sql(f\"\"\"ALTER SESSION SET QUERY_TAG = '{{\"origin\":\"sf_sit\",\"name\":\"dt_conversion_task\",\"version\":{{\"major\":1, \"minor\":0}},\"attributes\":\"session_tag\"}}'\"\"\").collect()\n",
    "\n",
    "#get current_role\n",
    "current_role = session.get_current_role().replace('\"','')\n",
    "\n",
    "st.success(f\"Session initialized for role: {current_role} ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66492742-c41a-4570-81ee-fc874914ed7d",
   "metadata": {
    "collapsed": false,
    "name": "Step_2_Label"
   },
   "source": [
    "## STEP 2: Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac72105-496b-4e17-a807-495078083cba",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_2_Function_Definition"
   },
   "outputs": [],
   "source": [
    "def paginate_data(df):\n",
    "\tst.divider()\n",
    "\t\t\t\n",
    "\tpagination = st.empty()\n",
    "\tbatch_size = 20  # Set the number of items per page\n",
    "\n",
    "\tif len(df) > 0:\n",
    "\t\tbottom_menu = st.columns((4, 1, 1))\n",
    "\t\twith bottom_menu[2]:\n",
    "\t\t\ttotal_pages = (\n",
    "    \t\t\tint(len(df) / batch_size) if int(len(df) / batch_size) > 0 else 1\n",
    "    \t\t)\n",
    "\t\t\tcurrent_page = st.number_input(\n",
    "    \t\t\t\"Page\", min_value=1, max_value=total_pages, step=1\n",
    "    \t\t)\n",
    "\t\twith bottom_menu[0]:\n",
    "\t\t\tst.markdown(f\"Page **{current_page}** of **{total_pages}** \")\n",
    "    \n",
    "\t\tpages = split_frame(df, batch_size)\n",
    "\t\tpagination.dataframe(data=pages[current_page - 1], use_container_width=True)\n",
    "    \n",
    "\t\tst.divider()\n",
    "\telse:\n",
    "\t\tst.caption(\"No results to display.\")\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def split_frame(input_df, rows):\n",
    "\tdf = [input_df.loc[i : i + rows - 1, :] for i in range(0, len(input_df), rows)]\n",
    "\treturn df\n",
    "\n",
    "def operator_attributes(query_id, task_settings):\n",
    "    #check if the task's definition updates table via a merge statement\n",
    "    #NOTE: currently only supporting MERGE and INSERT(from base table(s)) statements\n",
    "    df_target_operation = pd.DataFrame(session.sql(f\"\"\"\n",
    "                                                SELECT\n",
    "                                                    OPERATOR_ATTRIBUTES:table_name::varchar TARGET_TABLE\n",
    "                                                    ,OPERATOR_ATTRIBUTES:table_names[0]::varchar TARGET_TABLES\n",
    "                                                    ,ARRAY_SIZE(OPERATOR_ATTRIBUTES:table_names) TARGET_TABLES_LENGTH\n",
    "                                                    ,OPERATOR_TYPE\n",
    "                                                FROM TABLE(GET_QUERY_OPERATOR_STATS('{query_id}')) \n",
    "                                                WHERE LOWER(OPERATOR_TYPE) IN('insert', 'merge', 'update', 'delete', 'extensionfunction', 'createtableasselect')\n",
    "                                                ;\n",
    "                                                \"\"\").collect())\n",
    "    \n",
    "    if not df_target_operation.empty:\n",
    "        #get attributes\n",
    "        target_table = df_target_operation.iloc[0,0] if df_target_operation.iloc[0,0] else df_target_operation.iloc[0,1]\n",
    "        target_table_length = df_target_operation.iloc[0,2]\n",
    "        target_operation = df_target_operation.iloc[0,3]\n",
    "        \n",
    "        task_settings.update({\"target_table\" : f\"{target_table}\"})  \n",
    "        task_settings.update({\"target_table_length\" : target_table_length})\n",
    "        task_settings.update({\"target_operation\" : f\"{target_operation}\"})\n",
    "\n",
    "        return \"Success\"\n",
    "    else:\n",
    "        return f\"Operator Attributes not found for query_id: {query_id}\"\n",
    "\n",
    "def convert_task(task_settings, df_shared_objs, shared_objs, eligible_tasks, ineligible_tasks):\n",
    "    eligible_flag = True\n",
    "    \n",
    "    source_select = \"\"\n",
    "    source_table = \"\"\n",
    "    reason = \"\"\n",
    "    stream = \"\"\n",
    "    \n",
    "    #get task settings\n",
    "    task = task_settings[\"task\"]\n",
    "    task_type = task_settings[\"task_type\"]\n",
    "    root_task = task_settings[\"root_task\"]\n",
    "    warehouse = task_settings[\"warehouse\"]\n",
    "    schedule = task_settings[\"schedule\"]\n",
    "    predecessors = task_settings[\"predecessors\"]\n",
    "    step = task_settings[\"step\"]\n",
    "    definition = task_settings[\"definition\"]\n",
    "    condition = task_settings[\"condition\"]\n",
    "    target_table = task_settings[\"target_table\"]\n",
    "    target_table_length = task_settings[\"target_table_length\"]\n",
    "    target_operation = task_settings[\"target_operation\"]\n",
    "\n",
    "    #create dynamic table DDL prefix\n",
    "    create_dt_ddl = f\"\"\"CREATE OR REPLACE DYNAMIC TABLE {target_table}_DT\n",
    "                        TARGET_LAG = '{schedule}'\n",
    "                        WAREHOUSE = {warehouse}\n",
    "                        COMMENT = '{{\"origin\":\"sf_sit\",\"name\":\"dt_conversion_task\",\"version\":{{\"major\":1, \"minor\":0}},\"attributes\":{{\"source\":\"task\", \"name\":\"{task}\"}}}}'\n",
    "                        AS\n",
    "                        \"\"\"\n",
    "\n",
    "    #get stream from condition, if not null.\n",
    "    if condition != 'None':\n",
    "        cond_prefix = \"SYSTEM$STREAM_HAS_DATA('\"\n",
    "        cond_suffix = \"')\"\n",
    "\n",
    "        stream = \"\".join(condition.split(cond_prefix)[1].split(cond_suffix)[0])\n",
    "\n",
    "    #create task details dict\n",
    "    task_details = {}\n",
    "\n",
    "    task_details.update({\"task_type\" : f\"{task_type}\"})\n",
    "    task_details.update({\"target_table\" : f\"{target_table}\"})\n",
    "    task_details.update({\"target_operation\" : f\"{target_operation}\"})\n",
    "    task_details.update({\"root_task\" : f\"{root_task}\"})\n",
    "    task_details.update({\"child_tasks\" : None})\n",
    "    task_details.update({\"schedule\" : f\"{schedule}\"})\n",
    "    task_details.update({\"step\" : step})\n",
    "    task_details.update({\"predecessors\" : f\"{predecessors}\"})\n",
    "    task_details.update({\"child_objects\" : None})\n",
    "    task_details.update({\"stream\" : f\"{stream}\"})\n",
    "    task_details.update({\"child_streams\" : None})\n",
    "\n",
    "    #set share flag whether target is in a share:\n",
    "    share_details = {}\n",
    "    flag_target_shared = \"Y\" if target_table in shared_objs else \"N\"\n",
    "\n",
    "    share_details.update({\"target_shared\" : f\"{flag_target_shared}\"})\n",
    "\n",
    "    if flag_target_shared == \"Y\":\n",
    "        shares_target = []\n",
    "\n",
    "        df_shared_objs_filtered = df_shared_objs.query(f\"\"\"object == '{target_table}'\"\"\")\n",
    "\n",
    "        for index, row in df_shared_objs_filtered.iterrows():\n",
    "            share_details.update({\"object\" : f\"\"\"{row[\"object\"]}\"\"\"})\n",
    "            share_details.update({\"object_type\" : f\"\"\"{row[\"object_type\"]}\"\"\"})\n",
    "            shares_target.append(row[\"share\"])\n",
    "            \n",
    "        share_details.update({\"shares\" : shares_target})\n",
    "\n",
    "    #CTAS\n",
    "    if target_operation.lower() =='createtableasselect':\n",
    "        #if CTAS source is a select statement\n",
    "        if re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", definition):\n",
    "            #get source select statement\n",
    "            source_select = re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", definition).group(1)\n",
    "        \n",
    "            #create dynamic table DDL prefix\n",
    "            create_dt_ddl += f\"{source_select};\"\n",
    "            \n",
    "            #beautify dynamic table DDL\n",
    "            create_dt_ddl = sqlparse.format(create_dt_ddl, reindent=True, keyword_case=\"upper\")\n",
    "\n",
    "            #check if create DT statement is valid using EXPLAIN\n",
    "            try:\n",
    "                explain_dt_statement = pd.DataFrame(session.sql(f\"\"\"EXPLAIN USING JSON {create_dt_ddl}\"\"\").collect()).iloc[0,0]\n",
    "            except Exception as e:\n",
    "                eligible_flag = False\n",
    "                reason = str(e)\n",
    "        else:\n",
    "            eligible_flag = False\n",
    "            create_dt_ddl = \"N/A\"\n",
    "            reason = \"The task definition does not contain a base table\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    #INSERT\n",
    "    if target_operation.lower() == 'insert':\n",
    "        if target_table_length > 1:\n",
    "            eligible_flag = False\n",
    "            reason = \"Converting tasks with multi-table inserts is currently not supported.\"\n",
    "        else:\n",
    "            #check if insert statement contains base table\n",
    "            if re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", definition):\n",
    "                #check whether insert columns are specified (and equal the number of columns in target table)\n",
    "                #if insert columns are not specified, assuming the task's insert statement inserts the values correctly\n",
    "                if (re.search(r\"(?s)INTO(.*?)(?<=\\()(.+?)(\\))\", definition) and \n",
    "                    len(re.search(r\"(?s)INTO(.*?)(?<=\\()(.+?)(\\))\", definition).group(2).split(\",\")) == len(df_target_table_clmns)\n",
    "                    ) or re.search(r\"(?s)INTO(.*?)(?<=\\()(.+?)(\\))\", definition) is None:\n",
    "                    \n",
    "                    #get source select statement\n",
    "                    source_select = re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", definition).group(1)\n",
    "    \n",
    "                    #if select statement contains the stream, get it's source table and replace the stream in the select statement\n",
    "                    if stream and stream.lower() in source_select.lower():\n",
    "                        session.sql(f\"\"\"DESCRIBE STREAM {stream};\"\"\").collect()\n",
    "                        source_table = pd.DataFrame(session.sql(f\"\"\"SELECT \"table_name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\"\"\").collect()).iloc[0,0]\n",
    "                        source_select = source_select.replace(stream, source_table)\n",
    "    \n",
    "                    #create dynamic table DDL prefix\n",
    "                    create_dt_ddl += f\"{source_select};\"\n",
    "                    \n",
    "                    #beautify dynamic table DDL\n",
    "                    create_dt_ddl = sqlparse.format(create_dt_ddl, reindent=True, keyword_case=\"upper\")\n",
    "\n",
    "                    #check if create DT statement is valid using EXPLAIN\n",
    "                    try:\n",
    "                        explain_dt_statement = pd.DataFrame(session.sql(f\"\"\"EXPLAIN USING JSON {create_dt_ddl}\"\"\").collect()).iloc[0,0]\n",
    "                    except Exception as e:\n",
    "                        eligible_flag = False\n",
    "                        reason = str(e)\n",
    "                else:\n",
    "                    eligible_flag = False\n",
    "                    create_dt_ddl = \"N/A\"\n",
    "                    reason = \"The number of columns in the INSERT column list does not match the number of columns in the target table definition. Important columns could be missing, if converted to a dynamic table.\"\n",
    "            else:\n",
    "                eligible_flag = False\n",
    "                create_dt_ddl = \"N/A\"\n",
    "                reason = \"The task definition does not contain a base table\"\n",
    "\n",
    "    #MERGE\n",
    "    if target_operation.lower() == 'merge':        \n",
    "        #if merge source is a select statement\n",
    "        if re.search(r\"(?s)(?<=USING)(\\(*.*?\\))(?=\\s+\\w+\\s+ON\\s)\", definition):\n",
    "            source = re.search(r\"(?s)(?<=USING)(\\(*.*?\\))(?=\\s+\\w+\\s+ON\\s)\", definition).group(1)\n",
    "            #get select statement\n",
    "            source_select = re.search(r\"(?s)(?<=\\()(.*?)(?!.+\\))\", source).group(1).strip()\n",
    "\n",
    "            #if select statement contains the stream, get it's source table and replace the stream in the select statement\n",
    "            if stream and stream.lower() in source_select.lower():\n",
    "                session.sql(f\"\"\"DESCRIBE STREAM {stream};\"\"\").collect()\n",
    "                source_table = pd.DataFrame(session.sql(f\"\"\"SELECT \"table_name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\"\"\").collect()).iloc[0,0]\n",
    "                source_select = source_select.replace(stream, source_table)\n",
    "        \n",
    "        #if merge source is a table\n",
    "        elif re.search(r\"(?s)(?<=USING)([^\\(].*?[^/)])(?=\\s+\\w+\\s+ON\\s)\", definition):\n",
    "            #get source table/stream\n",
    "            source = re.search(r\"(?s)(?<=USING)([^\\(].*?[^/)])(?=\\s+\\w+\\s+ON\\s)\", definition).group(1)\n",
    "            source_table = source.replace(\"(\",\"\").replace(\")\",\"\").strip()\n",
    "\n",
    "            #if source is a stream, get source table\n",
    "            if stream and source_table.lower() == stream.lower():\n",
    "                session.sql(f\"\"\"DESCRIBE STREAM {stream};\"\"\").collect()\n",
    "                source_table = pd.DataFrame(session.sql(f\"\"\"SELECT \"table_name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\"\"\").collect()).iloc[0,0]\n",
    "\n",
    "            #get source table columns\n",
    "            session.sql(f\"\"\"DESCRIBE TABLE {source_table};\"\"\").collect()\n",
    "            df_source_table_clmns = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\"\"\").collect())\n",
    "\n",
    "            source_select = f\"\"\"SELECT \n",
    "                \"\"\"\n",
    "            for index, row in df_source_table_clmns.iterrows():\n",
    "                if index == 0:\n",
    "                    source_select += f\"\"\"{row[\"name\"]} \n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    source_select += f\"\"\",{row[\"name\"]} \n",
    "                    \"\"\"\n",
    "            source_select += f\"\"\"FROM {source_table}\"\"\"\n",
    "            \n",
    "        #update dynamic table DDL and beautify it\n",
    "        create_dt_ddl += f\"{source_select};\"\n",
    "        create_dt_ddl = sqlparse.format(create_dt_ddl, reindent=True, keyword_case=\"upper\")\n",
    "\n",
    "        #check if create DT statement is valid using EXPLAIN\n",
    "        try:\n",
    "            explain_dt_statement = pd.DataFrame(session.sql(f\"\"\"EXPLAIN USING JSON {create_dt_ddl}\"\"\").collect()).iloc[0,0]\n",
    "        except Exception as e:\n",
    "            eligible_flag = False\n",
    "            reason = str(e)\n",
    "\n",
    "    #UPDATE\n",
    "    if target_operation.lower() == 'update':\n",
    "        eligible_flag = False\n",
    "        create_dt_ddl = \"N/A\"\n",
    "        reason = \"Tasks that use UPDATE statements are not eligible to be converted to dynamic tables\"\n",
    "    #DELETE\n",
    "    if target_operation.lower() == 'delete':\n",
    "        eligible_flag = False\n",
    "        create_dt_ddl = \"N/A\"\n",
    "        reason = \"Tasks that use DELETE statements are not eligible to be converted to dynamic tables\"\n",
    "\n",
    "    #EXTENSIONFUNCTION (Stored Proc)\n",
    "    if target_operation.lower() == 'extensionfunction':\n",
    "        eligible_flag = False\n",
    "        create_dt_ddl = \"N/A\"\n",
    "        reason = \"Tasks that call Stored Prodedures are not eligible to be converted to dynamic tables\"\n",
    "    \n",
    "    if eligible_flag:\n",
    "        #append task conversion details to eligible list\n",
    "        eligible_tasks.append([False, task, json.dumps(task_details, indent=2), json.dumps([share_details], indent=2), definition, create_dt_ddl])\n",
    "    else:\n",
    "        #append task conversion details to ineligible list\n",
    "        ineligible_tasks.append([task, json.dumps(task_details, indent=2), definition, f\"{task} ({task_type.capitalize()} task): {reason}\"])\n",
    "\n",
    "    return [task, task_details, [share_details], definition, create_dt_ddl]\n",
    "\n",
    "st.success(f\"Functions created ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d14b1-8f8d-4ad6-9fb1-bff3c4ace57f",
   "metadata": {
    "collapsed": false,
    "name": "Step_3_Label"
   },
   "source": [
    "## STEP 3: Set the database(s) where the tasks reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8827920-5e0c-4781-8ffb-402d291b0183",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_3_Set_DBs"
   },
   "outputs": [],
   "source": [
    "df_task_dbs = pd.DataFrame(session.sql(f\"SHOW DATABASES\").collect())\n",
    "    \n",
    "if not df_task_dbs.empty :\n",
    "    select_task_db = st.multiselect(\"Select Database(s):\", df_task_dbs[\"name\"], placeholder=\"Choose an option\", key=\"task_select_db\")\n",
    "task_dbs = str(select_task_db)\n",
    "\n",
    "st.write(\"#\")\n",
    "st.write(\"#\")\n",
    "st.write(\"#\")\n",
    "st.write(\"#\")\n",
    "st.write(\"#\")\n",
    "st.write(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ac194-cbf3-4073-bef7-eee8628810f8",
   "metadata": {
    "collapsed": false,
    "name": "Step_4_Label"
   },
   "source": [
    "## STEP 4: Get all shared tables/views\n",
    "\n",
    "This step compiles a list of all tables/views shared by your role.  Any target tables updated in existing tasks that are in the list will be flagged and optionally removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdf769-efb5-42d8-9ea6-3ccf22ad4452",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Step_4_Shared_Objs"
   },
   "outputs": [],
   "source": [
    "list_shares_objs = []\n",
    "\n",
    "#show all shares\n",
    "session.sql(f\"\"\"SHOW SHARES;\"\"\").collect()\n",
    "\n",
    "#get outbound shares only\n",
    "df_outbound_shares = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE LOWER(\"kind\") = 'outbound' AND LOWER(\"owner\") = '{current_role.lower()}';\"\"\").collect())\n",
    "\n",
    "for index, row in df_outbound_shares.iterrows():\n",
    "    share = row[\"name\"]\n",
    "\n",
    "    try:\n",
    "        #describe shares\n",
    "        session.sql(f\"\"\"DESCRIBE SHARE {share};\"\"\").collect()\n",
    "        \n",
    "        #get shared objects\n",
    "        df_shared_objs = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\", \"kind\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) \n",
    "                                                        WHERE \n",
    "                                                            LOWER(\"kind\") IN ('table', 'view', 'materialized view') AND\n",
    "                                                            SPLIT_PART(\"name\", '.', 1) IN ({task_dbs.strip('][')});\"\"\").collect())\n",
    "\n",
    "        #add each object to the list_obj list\n",
    "        for index, row in df_shared_objs.iterrows():\n",
    "            name = row[\"name\"]\n",
    "            kind = row[\"kind\"]\n",
    "\n",
    "            if not row.empty:\n",
    "                list_shares_objs.append([share, kind, name])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#print(list_shares_objs)\n",
    "\n",
    "#create list of shares, object types, and objs shared\n",
    "list_shares = [item[0] for item in list_shares_objs]\n",
    "list_obj_types = [item[1] for item in list_shares_objs]\n",
    "list_objs = [item[2] for item in list_shares_objs]\n",
    "\n",
    "#dict = {'share': list_shares, 'object_type': list_obj_types, 'object': list_objs} \n",
    "   \n",
    "df_shared_objs = pd.DataFrame({'share': list_shares, 'object_type': list_obj_types, 'object': list_objs} )\n",
    "\n",
    "#show shared objects\n",
    "paginate_data(df_shared_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5cf79f-71c3-4040-9b5b-ec494ec39eaa",
   "metadata": {
    "collapsed": false,
    "name": "Step_5_Label"
   },
   "source": [
    "## STEP 5: Find the latest completed tasks\n",
    "\n",
    "This step compiles a list of latest completed tasks within the last 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec013ae3-0733-4def-a0f6-200f0b508f45",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_5_Get_Tasks"
   },
   "outputs": [],
   "source": [
    "task_history_range_list = ['Choose a Date Range', 'Last day', 'Last 7 days', 'Last 14 days']\n",
    "st.write(\"\")\n",
    "st.selectbox(\"Select Task History Date Range:\", task_history_range_list, key=\"sb_task_history_range\")\n",
    "\n",
    "date_time_part = \"\"\n",
    "increment = \"\"\n",
    "df_task_history_range = None\n",
    "\n",
    "#match st.session_state.sb_query_history_range:\n",
    "if st.session_state.sb_task_history_range == \"Last day\":\n",
    "    date_time_part = \"hours\"\n",
    "    increment = \"24\"\n",
    "elif st.session_state.sb_task_history_range == \"Last 7 days\":\n",
    "    date_time_part = \"days\"\n",
    "    increment = \"7\"\n",
    "elif st.session_state.sb_task_history_range == \"Last 14 days\":\n",
    "    date_time_part = \"days\"\n",
    "    increment = \"14\"\n",
    "\n",
    "if st.session_state.sb_task_history_range == \"Choose a Date Range\":\n",
    "    st.write(\"#\")\n",
    "    st.write(\"#\")\n",
    "    st.write(\"#\")\n",
    "    st.write(\"#\")\n",
    "    st.write(\"#\")\n",
    "    st.write(\"#\")\n",
    "\n",
    "if st.session_state.sb_task_history_range != \"Choose a Date Range\":\n",
    "\n",
    "    df_task_history_range = pd.DataFrame(session.sql(f\"\"\"\n",
    "                                                SELECT \n",
    "                                                    DISTINCT(DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME) AS TASK_FQN\n",
    "                                                    ,MAX(QUERY_ID) QUERY_ID\n",
    "                                                    ,MAX(COMPLETED_TIME) COMPLETED_TIME\n",
    "                                                    ,MAX(STATE) STATE\n",
    "                                                FROM \n",
    "                                                    SNOWFLAKE.ACCOUNT_USAGE.TASK_HISTORY\n",
    "                                                WHERE\n",
    "                                                    LOWER(DATABASE_NAME) IN (\n",
    "                                                        SELECT LOWER(value) FROM TABLE(FLATTEN(INPUT => {task_dbs}))\n",
    "                                                        ) AND\n",
    "                                                    LOWER(STATE) = 'succeeded' AND\n",
    "                                                    COMPLETED_TIME > DATEADD(hours, -24, CURRENT_TIMESTAMP())\n",
    "                                                GROUP BY\n",
    "                                                    TASK_FQN\n",
    "                                                ;\n",
    "                                                \"\"\").collect())\n",
    "\n",
    "    #preview dataframe\n",
    "    st.write(\"\")\n",
    "    st.subheader(f\"Latest successful tasks ({st.session_state.sb_task_history_range})\")\n",
    "    st.dataframe(df_task_history_range, hide_index=True, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076bf89-97c5-411a-af8e-6d39e9233a22",
   "metadata": {
    "collapsed": false,
    "name": "Step_6_Label"
   },
   "source": [
    "## STEP 6: Get task details\n",
    "\n",
    "This step: compiles a list of tasks eligible to be converted to dynamic tables (currently only those with `MERGE` and `INSERT` (from a base table) operations), along with whether the target table is included in a data share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf627ae-b4be-46b0-8481-8e0008784844",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Step_6_Task_Details"
   },
   "outputs": [],
   "source": [
    "#for each task, check if it's either a single task or the root task in a multi-task graph\n",
    "list_eligible_tasks = []\n",
    "list_ineligible_tasks = []\n",
    "\n",
    "for index, row in df_task_history_range.iterrows():\n",
    "    task = row[\"TASK_FQN\"]\n",
    "    query_id = row[\"QUERY_ID\"]\n",
    "    completed_time = row[\"COMPLETED_TIME\"]\n",
    "    state = row[\"STATE\"]\n",
    "    \n",
    "    task_db = task.split(\".\")[0]\n",
    "    task_sch = task.split(\".\")[1]\n",
    "    task_name = task.split(\".\")[2]\n",
    "\n",
    "    #check if task hasn't been dropped within the last 24 hours\n",
    "    session.sql(f\"\"\"SHOW TASKS IN {task_db}.{task_sch}\"\"\").collect()\n",
    "    \n",
    "    #get outbound shares only\n",
    "    df_tasks_not_dropped = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE LOWER(\"name\") = '{task_name.lower()}';\"\"\").collect())\n",
    "\n",
    "    #if the task no longer exists, skip to the next task in df_task_history_range\n",
    "    if df_tasks_not_dropped.empty:\n",
    "        continue\n",
    "    \n",
    "    df_tasks_dependents = pd.DataFrame(session.sql(f\"\"\"\n",
    "                                                    SELECT\n",
    "                                                        CREATED_ON\n",
    "                                                        ,DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS TASK_FQN\n",
    "                                                        ,OWNER\n",
    "                                                        ,WAREHOUSE\n",
    "                                                        ,SCHEDULE\n",
    "                                                        ,PREDECESSORS\n",
    "                                                        ,DEFINITION\n",
    "                                                        ,CONDITION\n",
    "                                                    FROM \n",
    "                                                        TABLE({task_db}.INFORMATION_SCHEMA.TASK_DEPENDENTS(task_name => '{task.upper()}'))\n",
    "                                                    ;\n",
    "                                                    \"\"\").collect())\n",
    "\n",
    "    #CONDITION 1 (1-task graph): the task is the only one in the task graph and does not have any predecessors      \n",
    "    #CONDITION 2 (multi-task graph): the first NAME in df_tasks_dependents = TASK AND PREDECESSORS = [], then it's the root task, with at least one child\n",
    "    if df_tasks_dependents.iloc[0,1] == task and df_tasks_dependents.iloc[0,5] == \"[]\":\n",
    "        root_task = df_tasks_dependents.iloc[0,1]\n",
    "        list_multi_eligible_tasks = []\n",
    "        list_multi_ineligible_tasks = []\n",
    "        \n",
    "        list_task_graph_definitions = []\n",
    "        list_child_tasks = []\n",
    "        list_task_graph_child_objs_details = []\n",
    "        list_task_graph_child_streams_details = []\n",
    "        list_task_graph_share_details = []\n",
    "        list_ineligible_tasks_reasons = []\n",
    "        list_create_dt_ddl = []\n",
    "\n",
    "        root_task_details = None\n",
    "        \n",
    "        for index, row in df_tasks_dependents.iterrows():\n",
    "            converted_task = None\n",
    "            task_type = \"root\" if index == 0 else \"child\"\n",
    "            definition = sqlparse.format(df_tasks_dependents.iloc[index,6], reindent=True, keyword_case=\"upper\")\n",
    "\n",
    "            dict_task_settings = {}\n",
    "            dict_task_settings.update({\"task\" : f\"{df_tasks_dependents.iloc[index,1]}\"})\n",
    "            dict_task_settings.update({\"task_type\" : f\"{task_type}\"})\n",
    "            dict_task_settings.update({\"root_task\" : f\"{task}\"}) \n",
    "            dict_task_settings.update({\"warehouse\" : f\"{df_tasks_dependents.iloc[index,3]}\"})   \n",
    "\n",
    "            #for child tasks, use root task schedule\n",
    "            schedule = df_tasks_dependents.iloc[index,4] if df_tasks_dependents.iloc[index,4] else df_tasks_dependents.iloc[0,4]\n",
    "            dict_task_settings.update({\"schedule\" : f\"{schedule}\"}) \n",
    "            \n",
    "            dict_task_settings.update({\"step\" : index+1}) \n",
    "            dict_task_settings.update({\"predecessors\" : f\"{df_tasks_dependents.iloc[index,5]}\"})\n",
    "            dict_task_settings.update({\"definition\" : f\"\"\"{definition}\"\"\"})\n",
    "            dict_task_settings.update({\"condition\" : f\"{df_tasks_dependents.iloc[index,7]}\"})\n",
    "\n",
    "            #append DDL to list_task_graph_definitions\n",
    "            list_task_graph_definitions.append(definition)\n",
    "\n",
    "            #get operator attributes for the query_id\n",
    "            query_id = df_task_history_range.query(f\"\"\"TASK_FQN == '{df_tasks_dependents.iloc[index,1]}'\"\"\").iloc[0,1]\n",
    "            oper_attr = operator_attributes(query_id, dict_task_settings)\n",
    "    \n",
    "            if oper_attr.lower() == \"success\":\n",
    "                if len(df_tasks_dependents) == 1:\n",
    "                    #call convert tasks\n",
    "                    converted_task = convert_task(dict_task_settings, df_shared_objs, list_objs, list_eligible_tasks, list_ineligible_tasks)\n",
    "                if len(df_tasks_dependents) > 1:\n",
    "                    #call convert tasks\n",
    "                    converted_task = convert_task(dict_task_settings, df_shared_objs, list_objs, list_multi_eligible_tasks, list_multi_ineligible_tasks)\n",
    "    \n",
    "                    #set root_task_details\n",
    "                    if index == 0: root_task_details = converted_task[1]\n",
    "\n",
    "                    #add child object\n",
    "                    if task_type == \"child\":\n",
    "                        #update root task details\n",
    "                        list_task_graph_child_objs_details.append(dict_task_settings[\"target_table\"])\n",
    "                        root_task_details.update({\"child_objects\": list_task_graph_child_objs_details})\n",
    "                        \n",
    "                        if converted_task[1][\"stream\"] != \"\":\n",
    "                            list_task_graph_child_streams_details.append(converted_task[1][\"stream\"])\n",
    "                            root_task_details.update({\"child_streams\": list_task_graph_child_streams_details})\n",
    "\n",
    "                        list_child_tasks.append(dict_task_settings[\"task\"])\n",
    "                        root_task_details.update({\"child_tasks\": list_child_tasks})\n",
    "    \n",
    "                    #add to list_task_graph_share_details\n",
    "                    list_task_graph_share_details.append(converted_task[2][0])\n",
    "\n",
    "        if len(df_tasks_dependents) > 1:        \n",
    "            #if any of the tasks in the task graph are ineligible, add the root/child task to list_ineligible_tasks, along with offending task and reason\n",
    "            if list_multi_ineligible_tasks:\n",
    "                for ie_task in list_multi_ineligible_tasks:\n",
    "                    reason = ie_task[3]\n",
    "                    list_ineligible_tasks_reasons.append(f\"{reason}\")\n",
    "    \n",
    "                #append root task to ineligible list\n",
    "                list_ineligible_tasks.append([root_task, json.dumps(root_task_details, indent=2), \"\\n\\n\".join(list_task_graph_definitions), \"\\n\\n\".join(list_ineligible_tasks_reasons)])\n",
    "    \n",
    "            if list_multi_eligible_tasks and not list_multi_ineligible_tasks:\n",
    "                root_schedule = \"\"\n",
    "                #append the successful DT DDL\n",
    "                for idx, e_task in enumerate(list_multi_eligible_tasks):\n",
    "                    dt_ddl = e_task[5]\n",
    "                    target_table = json.loads(e_task[2])[\"target_table\"]\n",
    "\n",
    "                    #get the root task's schedule (the root task should always be the first in the eligible list, but check to be sure)\n",
    "                    if idx == 0 and json.loads(e_task[2])[\"task_type\"].lower() == \"root\":\n",
    "                        root_schedule = json.loads(e_task[2])[\"schedule\"]\n",
    "\n",
    "                    if len(list_multi_eligible_tasks) > 1:\n",
    "                        #if this table is referenced downstream, set target_lag to DOWNSTREAM, else use the root task's schedule\n",
    "                        if (idx < len(list_multi_eligible_tasks) - 1) and (target_table.lower() in list_multi_eligible_tasks[idx+1][5].lower()):\n",
    "                            dt_ddl = re.sub(r\"(?<=TARGET_LAG\\s=\\s)(\\'.*\\')\", \"DOWNSTREAM\", dt_ddl)\n",
    "                        else:\n",
    "                            dt_ddl = re.sub(r\"(?<=TARGET_LAG\\s=\\s)(\\'.*\\')\", f\"'{root_schedule}'\", dt_ddl)\n",
    "\n",
    "                    list_create_dt_ddl.append(dt_ddl)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                #append root task to eligible list\n",
    "                list_eligible_tasks.append([False, root_task, json.dumps(root_task_details, indent=2), json.dumps(list_task_graph_share_details, indent=2), \"\\n\\n\".join(list_task_graph_definitions), \"\\n\\n\".join(list_create_dt_ddl)])\n",
    "    \n",
    "st.write(\"\")\n",
    "st.subheader(\"Ineligible Root Tasks:\")\n",
    "st.write(\"The following tasks cannot be converted to Dynamic Tables\")\n",
    "\n",
    "#create a dataframe from list_ineligible_tasks\n",
    "df_inelibible_task_clmns = ['Root Task'\n",
    "                             ,'Task Details'\n",
    "                             ,'Task Graph DDL'\n",
    "                             ,'Reason(s)'\n",
    "                            ]\n",
    "\n",
    "df_inelibible_task = pd.DataFrame(list_ineligible_tasks, columns = df_inelibible_task_clmns)\n",
    "\n",
    "#dynamically set data_editor height, based on number of rows in data frame\n",
    "de_inelibible_task_height = int((len(df_inelibible_task) + 1.5) * 35 + 3.5)\n",
    "\n",
    "de_inelibible_task = st.dataframe(\n",
    "    df_inelibible_task\n",
    "    ,height=de_inelibible_task_height\n",
    "    ,hide_index=True\n",
    "    ,use_container_width=True\n",
    ")\n",
    "\n",
    "st.write(\"#\")\n",
    "st.subheader(\"Eligible Root Tasks:\")\n",
    "st.write(\"Please choose task(s) to convert, using the `Convert` checkbox.  Any task selected will be converted in Step 6.\")\n",
    "\n",
    "#create a dataframe from list_eligible_tasks\n",
    "df_convert_task_clmns = ['Convert'\n",
    "                         ,'Root Task'\n",
    "                         ,'Task Details'\n",
    "                         ,'Shared Objects'\n",
    "                         ,'Task Graph DDL'\n",
    "                         ,'Dynamic Table DDL'\n",
    "                        ]\n",
    "\n",
    "df_convert_task = pd.DataFrame(list_eligible_tasks, columns = df_convert_task_clmns)\n",
    "\n",
    "#dynamically set data_editor height, based on number of rows in data frame\n",
    "de_convert_task_height = int((len(df_convert_task) + 1.5) * 35 + 3.5)\n",
    "\n",
    "de_convert_task = st.data_editor(\n",
    "    df_convert_task\n",
    "    ,height=de_convert_task_height\n",
    "    ,disabled=('Root Task','Task Details','Shared Objects','Task Graph DDL','Dynamic Table DDL')\n",
    "    ,hide_index=True\n",
    "    ,use_container_width=True\n",
    "    ,num_rows=\"fixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8543d0-097a-46c8-b254-1a8260e1d934",
   "metadata": {
    "collapsed": false,
    "name": "Step_7_Label"
   },
   "source": [
    "## STEP 7: Convert tasks (optional)\n",
    "\n",
    "This step converts the chosen root/child tasks from STEP 6 to dynamic tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292a590-caed-4447-ac7f-57957264586f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Step_7_Convert_Tasks"
   },
   "outputs": [],
   "source": [
    "df_selected_tasks = de_convert_task.query('Convert == True')\n",
    "\n",
    "flag_disable_convert_btn = True\n",
    "\n",
    "if True in set(de_convert_task['Convert']):\n",
    "    flag_disable_convert_btn = False\n",
    "\n",
    "btn_convert = st.button(\"Convert\", disabled=flag_disable_convert_btn, type=\"primary\")\n",
    "\n",
    "if btn_convert:\n",
    "    for index, row in df_selected_tasks.iterrows():\n",
    "        ddl = row[\"Dynamic Table DDL\"]\n",
    "\n",
    "        for stmt in ddl.rstrip(';').split(\";\"):\n",
    "            #get dt table name\n",
    "            dt =  re.search(r\"(?<=CREATE OR REPLACE DYNAMIC TABLE\\s)(.*?)(?=\\s+TARGET_LAG)\", stmt).group(1)\n",
    "            \n",
    "            #create dynamic table(s)\n",
    "            session.sql(f\"\"\"{stmt}\"\"\").collect()\n",
    "            st.success(f\"Dynamic Table: {dt} successfully created ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15270dc1-03ff-4c5f-99bb-e021b4216281",
   "metadata": {
    "collapsed": false,
    "name": "Step_8_Label"
   },
   "source": [
    "## STEP 8: Cleanup (optional)\n",
    "\n",
    "This step can perform the following:\n",
    "- removes target table(s) from any shares, including tables updated via child tasks\n",
    "- suspends and drops the existing root/child tasks\n",
    "- drops the stream associated with each root/chid task, if applicable\n",
    "- drops the target table and tables updated via child tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7206a-11e7-4785-8672-747a03bb2a03",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_8_Cleanup"
   },
   "outputs": [],
   "source": [
    "flag_disable_cleanup_btn = True\n",
    "flag_disable_checkbox = True\n",
    "\n",
    "if (True in set(de_convert_task['Convert'])):\n",
    "    flag_disable_checkbox = False\n",
    "\n",
    "\n",
    "st.checkbox(\"Remove target table(s) from shares\", key=\"cb_remove_from_share\", disabled=flag_disable_checkbox)\n",
    "st.checkbox(\"Suspend and drop existing task(s)\", key=\"cb_drop_task\", disabled=flag_disable_checkbox)\n",
    "st.checkbox(\"Drop stream(s)\", key=\"cb_drop_stream\", disabled=flag_disable_checkbox)\n",
    "st.checkbox(\"Drop target table(s)\", key=\"cb_drop_target_table\", disabled=flag_disable_checkbox)\n",
    "\n",
    "if (True in set(de_convert_task['Convert'])) and (st.session_state[\"cb_remove_from_share\"] or \n",
    "    st.session_state[\"cb_drop_task\"] or \n",
    "    st.session_state[\"cb_drop_stream\"] or\n",
    "    st.session_state[\"cb_drop_target_table\"]):\n",
    "    \n",
    "    flag_disable_cleanup_btn = False\n",
    "\n",
    "btn_cleanup = st.button(\"Cleanup\", disabled=flag_disable_cleanup_btn, type=\"primary\")\n",
    "\n",
    "if btn_cleanup:\n",
    "    list_tbls_drop = []\n",
    "    for index, row in df_selected_tasks.iterrows():\n",
    "        shared_objs = json.loads(row[\"Shared Objects\"])\n",
    "        root_task = row[\"Root Task\"]\n",
    "\n",
    "        if st.session_state[\"cb_remove_from_share\"]:\n",
    "            #REMOVE FROM SHARE(S)\n",
    "            for dict_obj in shared_objs:\n",
    "                flag_shared = dict_obj[\"target_shared\"]\n",
    "    \n",
    "                if flag_shared.lower() == 'y':\n",
    "                    obj = dict_obj[\"object\"]\n",
    "                    obj_type = dict_obj[\"object_type\"]\n",
    "                    list_share = dict_obj[\"shares\"]\n",
    "    \n",
    "                    #add obj/type to list\n",
    "                    list_tbls_drop.append([obj, obj_type])\n",
    "    \n",
    "                    for share in list_share:\n",
    "                        #remove obj from share\n",
    "                        session.sql(f\"\"\"REVOKE SELECT ON {obj_type} {obj} FROM SHARE {share}\"\"\").collect()\n",
    "                        st.success(f\"{obj_type} {obj} successfully removed from share: {share}. ðŸŽ‰\")\n",
    "                else:\n",
    "                    st.warning(f\"Parent/child objects for root task {root_task} are not currently shared\", icon=\"âš ï¸\")\n",
    "\n",
    "\n",
    "        if st.session_state[\"cb_drop_task\"]:\n",
    "            #SUSPEND AND DROP TASKS\n",
    "            list_drop_tasks = []\n",
    "            \n",
    "            #append root task\n",
    "            list_drop_tasks.append(row[\"Root Task\"])\n",
    "            \n",
    "            #append child tasks\n",
    "            child_tasks = json.loads(row[\"Task Details\"])[\"child_tasks\"]\n",
    "            \n",
    "            if child_tasks:\n",
    "                list_drop_tasks.extend(child_tasks)\n",
    "                \n",
    "            for task in reversed(list_drop_tasks):\n",
    "                session.sql(f\"\"\"ALTER TASK {task} SUSPEND\"\"\").collect()\n",
    "                session.sql(f\"\"\"DROP TASK {task}\"\"\").collect()\n",
    "                st.success(f\"Task: {task} successfully dropped. ðŸŽ‰\")\n",
    "\n",
    "        \n",
    "        if st.session_state[\"cb_drop_stream\"]:\n",
    "            #DROP STREAMS\n",
    "            list_drop_streams = []\n",
    "            \n",
    "            #append root stream\n",
    "            list_drop_streams.append(json.loads(row[\"Task Details\"])[\"stream\"])\n",
    "    \n",
    "            #append child streams\n",
    "            child_streams = json.loads(row[\"Task Details\"])[\"child_streams\"]\n",
    "            \n",
    "            if child_streams:\n",
    "                list_drop_streams.extend(child_streams)\n",
    "\n",
    "                for stream in reversed(list_drop_streams):\n",
    "                    session.sql(f\"\"\"DROP STREAM {stream}\"\"\").collect()\n",
    "                    st.success(f\"Stream: {stream} successfully dropped. ðŸŽ‰\")\n",
    "            else:\n",
    "                st.warning(f\"There are no streams for the root task {root_task} or its child tasks\", icon=\"âš ï¸\")\n",
    "        \n",
    "        if st.session_state[\"cb_drop_target_table\"]:\n",
    "            #DROP PARENT/CHILD OBJECTS\n",
    "            list_drop_objs = []\n",
    "    \n",
    "            #append root object\n",
    "            list_drop_objs.append(json.loads(row[\"Task Details\"])[\"target_table\"])\n",
    "    \n",
    "            #append child streams\n",
    "            child_objs = json.loads(row[\"Task Details\"])[\"child_objects\"]\n",
    "    \n",
    "            if child_objs:\n",
    "                list_drop_objs.extend(child_objs)\n",
    "                \n",
    "            for obj in reversed(list_drop_objs):\n",
    "                session.sql(f\"\"\"DROP TABLE {obj}\"\"\").collect()\n",
    "                st.success(f\"Table: {obj} successfully dropped. ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
