{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079fe083-c828-4c86-ad31-08d50310486c",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": "# Replace Non-Task commands with Dynamic Tables\n\nThis notebook identifies `CTAS` and `INSERT OVERWRITE` commands executed multiple times over a given time frame (not via a task), that can be converted to Dynamic tables.  Dynamic tables simplify data engineering in Snowflake by providing a reliable, cost-effective, and automated way to transform data. Not every command can or should be replaced.  \n\nThis notebook will:\n- check the `QUERY_HISTORY` account usage view for the commands that have successfully completed, more than once, over the last 24 hours.\n- identify whether the current target table is in a share.\n    - **NOTE:** Data Providers should take additional steps to ensure any affected shared tables don't impact Consumers before switching to Dynamic tables.\n- generate the DDL to create the Dynamic table that will replace the commands\n- execute the Dynamic table DDL (optional)\n- remove the existing target table from the share, if applicable (optional)\n- drop the existing target table (optional)"
  },
  {
   "cell_type": "markdown",
   "id": "5045de21-9f8c-443f-96c0-9a67c1b893d9",
   "metadata": {
    "name": "Prerequisites",
    "collapsed": false
   },
   "source": "## Prerequisites:\n\n- The user executing this notebook, must have access to the `SNOWFLAKE` database.\n- The user must have the `CREATE DYNAMIC TABLE` privilge on the schema where the new Dynamic Table will be created."
  },
  {
   "cell_type": "markdown",
   "id": "a9ce2fba-7a16-4bc5-9929-afda2e534efb",
   "metadata": {
    "name": "Step_1_Label",
    "collapsed": false
   },
   "source": "## STEP 1: Initiaize Session"
  },
  {
   "cell_type": "code",
   "id": "96f7b910-866d-481d-9454-9053a2fcb075",
   "metadata": {
    "language": "python",
    "name": "Step_1_Initialize_Session",
    "collapsed": false
   },
   "outputs": [],
   "source": "import json\nimport numpy as np\nimport pandas as pd\nimport re\nimport streamlit as st\nfrom st_aggrid import AgGrid, GridUpdateMode, JsCode\nfrom st_aggrid.grid_options_builder import GridOptionsBuilder\nimport sqlparse\n\nsession = get_active_session()\n\n#tag session\nsession.sql(f\"\"\"ALTER SESSION SET QUERY_TAG = '{{\"origin\":\"sf_sit\",\"name\":\"dt_conversion_non_task\",\"version\":{{\"major\":1, \"minor\":0}},\"attributes\":\"session_tag\"}}'\"\"\").collect()\n\nst.success(f\"Session initialized üéâ\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66492742-c41a-4570-81ee-fc874914ed7d",
   "metadata": {
    "name": "Step_2_Label",
    "collapsed": false
   },
   "source": "## STEP 2: Function definition"
  },
  {
   "cell_type": "code",
   "id": "1ac72105-496b-4e17-a807-495078083cba",
   "metadata": {
    "language": "python",
    "name": "Step_2_Function_Definition",
    "collapsed": false
   },
   "outputs": [],
   "source": "def paginate_data(df):\n\tst.divider()\n\t\t\t\n\tpagination = st.empty()\n\tbatch_size = 20  # Set the number of items per page\n \n\tbottom_menu = st.columns((4, 1, 1))\n\twith bottom_menu[2]:\n\t\ttotal_pages = (\n\t\t\tint(len(df) / batch_size) if int(len(df) / batch_size) > 0 else 1\n\t\t)\n\t\tcurrent_page = st.number_input(\n\t\t\t\"Page\", min_value=1, max_value=total_pages, step=1\n\t\t)\n\twith bottom_menu[0]:\n\t\tst.markdown(f\"Page **{current_page}** of **{total_pages}** \")\n\n\tpages = split_frame(df, batch_size)\n\tpagination.dataframe(data=pages[current_page - 1], use_container_width=True)\n\n\tst.divider()\n\n@st.cache_data(show_spinner=False)\ndef split_frame(input_df, rows):\n\tdf = [input_df.loc[i : i + rows - 1, :] for i in range(0, len(input_df), rows)]\n\treturn df\n\nst.success(f\"Functions created üéâ\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d1ac194-cbf3-4073-bef7-eee8628810f8",
   "metadata": {
    "name": "Step_3_Label",
    "collapsed": false
   },
   "source": "## STEP 3: Get all shared tables/views\n\nThis step compiles a list of all tables/views shared by your role."
  },
  {
   "cell_type": "code",
   "id": "6bbdf769-efb5-42d8-9ea6-3ccf22ad4452",
   "metadata": {
    "language": "python",
    "name": "Step_3_Shared_Objs",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "list_shares_objs = []\n\n#show all shares\nsession.sql(f\"\"\"SHOW SHARES;\"\"\").collect()\n\n#get outbound shares only\ndf_outbound_shares = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE LOWER(\"kind\") = 'outbound';\"\"\").collect())\n\nfor index, row in df_outbound_shares.iterrows():\n    share = row[\"name\"]\n\n    #describe shares\n    session.sql(f\"\"\"DESCRIBE SHARE {share};\"\"\").collect()\n    \n    #get shared objects\n    df_shared_objs = pd.DataFrame(session.sql(f\"\"\"SELECT \"name\", \"kind\" FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) \n                                                    WHERE \n                                                        LOWER(\"kind\") IN ('table', 'view', 'materialized view');\"\"\").collect())\n\n    #add each object to the list_obj list\n    for index, row in df_shared_objs.iterrows():\n        name = row[\"name\"]\n        kind = row[\"kind\"]\n\n        if not row.empty:\n            list_shares_objs.append([share, kind, name])\n\n#create list of shares, object types, and objs shared\nlist_shares = [item[0] for item in list_shares_objs]\nlist_obj_types = [item[1] for item in list_shares_objs]\nlist_objs = [item[2] for item in list_shares_objs]\n   \ndf_shared_objs = pd.DataFrame({'share': list_shares, 'object_type': list_obj_types, 'object': list_objs} )\n\n#show shared objects\npaginate_data(df_shared_objs)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f5cf79f-71c3-4040-9b5b-ec494ec39eaa",
   "metadata": {
    "name": "Step_4_Label",
    "collapsed": false
   },
   "source": "## STEP 4: Find the completed CTAS/INSERT OVERWRITE commands.\n\nThis step compiles a list of latest `CTAS` and `INSERT OVERWRITE` commands completed within the last 24 hours."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec013ae3-0733-4def-a0f6-200f0b508f45",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Step_4_Get_Cmds",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": "query_history_range_list = ['Choose a Date Range', 'Last day', 'Last 7 days', 'Last 28 days', 'Last 3 months', 'Last 6 months', 'Last 12 months']\nst.write(\"\")\nst.selectbox(\"Select Query History Date Range:\", query_history_range_list, key=\"sb_query_history_range\")\n\nif st.session_state.sb_query_history_range in ['Last 3 months', 'Last 6 months', 'Last 12 months']:\n    st.warning('Consider using either a larger warehouse or container for this Date Range', icon=\"‚ö†Ô∏è\")\n\ndate_time_part = \"\"\nincrement = \"\"\ndf_query_history_range = None\n\n#match st.session_state.sb_query_history_range:\nif st.session_state.sb_query_history_range == \"Last day\":\n    date_time_part = \"hours\"\n    increment = \"24\"\nelif st.session_state.sb_query_history_range == \"Last 7 days\":\n    date_time_part = \"days\"\n    increment = \"7\"\nelif st.session_state.sb_query_history_range == \"Last 28 days\":\n    date_time_part = \"days\"\n    increment = \"28\"\nelif st.session_state.sb_query_history_range == \"Last 3 months\":\n    date_time_part = \"months\"\n    increment = \"3\"\nelif st.session_state.sb_query_history_range == \"Last 6 months\":\n    date_time_part = \"months\"\n    increment = \"6\"\nelif st.session_state.sb_query_history_range == \"Last 12 months\":\n    date_time_part = \"months\"\n    increment = \"12\"\n\nif st.session_state.sb_query_history_range == \"Choose a Date Range\":\n    st.write(\"#\")\n    st.write(\"#\")\n    st.write(\"#\")\n    st.write(\"#\")\n    st.write(\"#\")\n    st.write(\"#\")\n\nif st.session_state.sb_query_history_range != \"Choose a Date Range\":\n    df_query_history_range = pd.DataFrame(session.sql(f\"\"\"\n                                                SELECT * \n                                                FROM (\n                                                    SELECT\n                                                        MAX(QUERY_ID) QUERY_ID\n                                                        ,QUERY_TEXT\n                                                        ,COUNT(QUERY_TEXT) NUM_OF_EXECUTIONS\n                                                        ,MIN(START_TIME) FIRST_EXECUTION\n                                                        ,MAX(END_TIME) LATEST_EXECUTION\n                                                        ,ROUND(TIMEDIFF('minute', MIN(START_TIME), MAX(END_TIME)) / (NUM_OF_EXECUTIONS-1), 0) AVG_SCHEDULE_MINS\n                                                        ,MAX(QUERY_TYPE) QUERY_TYPE\n                                                        ,USER_NAME\n                                                        ,MAX(WAREHOUSE_NAME) WAREHOUSE_NAME\n                                                        ,MAX(WAREHOUSE_SIZE) WAREHOUSE_SIZE\n                                                        ,MAX(WAREHOUSE_TYPE) WAREHOUSE_TYPE\n                                                        ,MAX(CLUSTER_NUMBER) CLUSTER_NUMBER\n                                                    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n                                                    WHERE\n                                                        (QUERY_TEXT ILIKE 'create or replace table%as select%'\n                                                        OR QUERY_TEXT ILIKE 'insert overwrite into%')\n                                                        AND LOWER(EXECUTION_STATUS) = 'success'\n                                                        AND LOWER(USER_NAME) <> 'system'\n                                                        AND END_TIME > DATEADD({date_time_part}, -{increment}, CURRENT_TIMESTAMP())\n                                                    GROUP BY\n                                                        QUERY_TEXT\n                                                        ,USER_NAME\n                                                ) qh\n                                                WHERE qh.NUM_OF_EXECUTIONS > 1\n                                                ;\n                                                \"\"\").collect())\n\n    \n    #dynamically set data_editor height, based on number of rows in data frame\n    df_query_history_range_height = int((len(df_query_history_range) + 1.5) * 35 + 3.5)\n\n    #preview dataframe\n    st.write(\"\")\n    st.subheader(f\"Latest Non-Task commands ({st.session_state.sb_query_history_range})\")\n    st.dataframe(df_query_history_range, hide_index=True, height=df_query_history_range_height, use_container_width=True)"
  },
  {
   "cell_type": "markdown",
   "id": "9076bf89-97c5-411a-af8e-6d39e9233a22",
   "metadata": {
    "name": "Step_5_Label",
    "collapsed": false
   },
   "source": "## STEP 5: Get command details\n\nThis step: compiles a list of commands eligible to be converted to dynamic tables, along with whether the target table is included in a data share."
  },
  {
   "cell_type": "code",
   "id": "edf627ae-b4be-46b0-8481-8e0008784844",
   "metadata": {
    "language": "python",
    "name": "Step_5_Cmd_Details",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#check each command\nlist_eligible_cmds = []\nlist_ineligible_cmds = []\n\nif not df_query_history_range.empty:\n    for index, row in df_query_history_range.iterrows():\n        db = \"\"\n        sch = \"\"\n        tbl = \"\"\n    \n        source_select = \"\"\n    \n        query_id = row[\"QUERY_ID\"]\n        query_text = sqlparse.format(row[\"QUERY_TEXT\"], reindent=True, keyword_case=\"upper\")\n        schedule = 1 if row[\"AVG_SCHEDULE_MINS\"] < 1 else row[\"AVG_SCHEDULE_MINS\"]\n        #query_type = row[\"QUERY_TYPE\"]\n        query_type = \"CTAS\" if row[\"QUERY_TYPE\"].lower() == 'create_table_as_select' else \"INSERT_OVERWRITE\"\n        warehouse = row[\"WAREHOUSE_NAME\"]\n    \n        #use get_query_operator_stats to get target table name\n        df_target_table_full = pd.DataFrame(session.sql(f\"\"\"SELECT\n                                                            OPERATOR_ATTRIBUTES:table_name::varchar TARGET_TABLE\n                                                            ,OPERATOR_ATTRIBUTES:table_names[0]::varchar TARGET_TABLES\n                                                        FROM TABLE(GET_QUERY_OPERATOR_STATS('{query_id}')) \n                                                        WHERE LOWER(OPERATOR_TYPE) IN('insert', 'createtableasselect')\n                                                        ;\"\"\").collect())\n            \n        target_table_full = df_target_table_full.iloc[0,0] if df_target_table_full.iloc[0,0] else df_target_table_full.iloc[0,1]\n    \n        if len(target_table_full.split(\".\")) == 4:\n            acct = target_table_full.split(\".\")[0]\n            db = target_table_full.split(\".\")[1]\n            sch = target_table_full.split(\".\")[2]\n            tbl = target_table_full.split(\".\")[3]\n    \n            target_table = f\"{db}.{sch}.{tbl}\"\n            \n        if len(target_table_full.split(\".\")) == 3:\n            target_table = target_table_full\n    \n        #set share flag whether target is in a share:\n        share_details = {}\n        flag_target_shared = \"Y\" if target_table in list_objs else \"N\"\n    \n        share_details.update({\"target_shared\" : f\"{flag_target_shared}\"})\n    \n        if flag_target_shared == \"Y\":\n            shares_target = []\n    \n            df_shared_objs_filtered = df_shared_objs.query(f\"\"\"object == '{target_table}'\"\"\")\n    \n            for index, row in df_shared_objs_filtered.iterrows():\n                share_details.update({\"object\" : f\"\"\"{row[\"object\"]}\"\"\"})\n                share_details.update({\"object_type\" : f\"\"\"{row[\"object_type\"]}\"\"\"})\n                shares_target.append(row[\"share\"])\n                \n            share_details.update({\"shares\" : shares_target})\n    \n        #create dynamic table DDL prefix        \n        create_dt_ddl = f\"\"\"CREATE OR REPLACE DYNAMIC TABLE {target_table}_DT\n                            TARGET_LAG = '{schedule} MINUTES'\n                            WAREHOUSE = {warehouse}\n                            COMMENT = '{{\"origin\":\"sf_sit\",\"name\":\"dt_conversion_non_task\",\"version\":{{\"major\":1, \"minor\":0}},\"attributes\":{{\"source\":\"command\", \"type\":\"{query_type}\"}}}}'\n                            AS\n                            \"\"\"\n        \n        #check if query selects from a base object\n        if re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", query_text):\n            #get source select statement\n            source_select = re.search(r\"(?s)(?=SELECT)(.*?\\s+FROM.*)\", query_text).group(1)\n        \n            #append source select statement to DT DDL \n            create_dt_ddl += source_select\n\n            #check if create DT statement is valid using EXPLAIN\n            try:\n                explain_dt_statement = pd.DataFrame(session.sql(f\"\"\"EXPLAIN USING JSON {create_dt_ddl}\"\"\").collect()).iloc[0,0]\n            except Exception as e:\n                reason = str(e)\n\n                #add command to ineligible list\n                list_ineligible_cmds.append([query_type, query_text, reason])\n            else:\n                #add command to eligible list\n                list_eligible_cmds.append([False, query_type, json.dumps([share_details], indent=2), query_text, create_dt_ddl])\n        else:\n            reason = \"This command does not select from a base object.\"\n            #add command to ineligible list\n            list_ineligible_cmds.append([query_type, query_text, reason])\n\n    \n    st.write(\"\")\n    st.subheader(\"Ineligible Commands:\")\n    st.write(\"The following command(s) cannot be converted to Dynamic Tables\")\n    \n    #create a dataframe from list_ineligible_cmds\n    df_inelibible_cmd_clmns = ['Command Type'\n                                 ,'Command DDL'\n                                 ,'Reason'\n                                ]\n    \n    df_inelibible_cmd = pd.DataFrame(list_ineligible_cmds, columns = df_inelibible_cmd_clmns)\n    \n    #dynamically set data_editor height, based on number of rows in data frame\n    de_inelibible_cmd_height = int((len(df_inelibible_cmd) + 1.5) * 35 + 3.5)\n    \n    de_inelibible_cmd = st.dataframe(\n        df_inelibible_cmd\n        ,height=de_inelibible_cmd_height\n        ,hide_index=True\n        ,use_container_width=True\n    )\n    \n    st.write(\"#\")\n    st.subheader(\"Eligible Commands:\")\n    st.write(\"Please choose scheduled command(s) to convert, using the `Convert` checkbox.  Any command selected will be converted in Step 6.\")\n    \n    #create a dataframe from list_eligible_cmds\n    df_convert_cmd_clmns = ['Convert'\n                             ,'Command Type'\n                             ,'Shared Objects'\n                             ,'Command DDL'\n                             ,'Dynamic Table DDL'\n                            ]\n    \n    df_convert_cmd = pd.DataFrame(list_eligible_cmds, columns = df_convert_cmd_clmns)\n    \n    #dynamically set data_editor height, based on number of rows in data frame\n    de_convert_cmd_height = int((len(df_convert_cmd) + 1.5) * 35 + 3.5)\n    \n    de_convert_cmd = st.data_editor(\n        df_convert_cmd\n        ,height=de_convert_cmd_height\n        ,disabled=('Command Type','Shared Objects','Command DDL','Dynamic Table DDL')\n        ,hide_index=True\n        ,use_container_width=True\n        ,num_rows=\"fixed\"\n    )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1e8543d0-097a-46c8-b254-1a8260e1d934",
   "metadata": {
    "name": "Step_6_Label",
    "collapsed": false
   },
   "source": "## STEP 6: Convert commands (optional)\n\nThis step converts the chosen commands from STEP 5 to Dynamic Tables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292a590-caed-4447-ac7f-57957264586f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    },
    "name": "Step_6_Convert_Cmds",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_selected_cmds = de_convert_cmd.query('Convert == True')\n\nflag_disable_convert_btn = True\n\nif True in set(de_convert_cmd['Convert']):\n    flag_disable_convert_btn = False\n\nbtn_convert = st.button(\"Convert\", disabled=flag_disable_convert_btn, type=\"primary\")\n\nif btn_convert:\n    for index, row in df_selected_cmds.iterrows():\n        ddl = row[\"Dynamic Table DDL\"]\n\n        for stmt in ddl.rstrip(';').split(\";\"):\n            #get dt table name\n            dt =  re.search(r\"(?<=CREATE OR REPLACE DYNAMIC TABLE\\s)(.*?)(?=\\s+TARGET_LAG)\", stmt).group(1)\n            \n            #create dynamic table(s)\n            session.sql(f\"\"\"{stmt}\"\"\").collect()\n            st.success(f\"Dynamic Table: {dt} successfully created üéâ\")"
  },
  {
   "cell_type": "markdown",
   "id": "15270dc1-03ff-4c5f-99bb-e021b4216281",
   "metadata": {
    "name": "Step_7_Label",
    "collapsed": false
   },
   "source": "## STEP 7: Cleanup (optional)\n\nThis step can perform the following:\n- removes target table(s) from any shares\n- drops the target table(s)"
  },
  {
   "cell_type": "code",
   "id": "82a7206a-11e7-4785-8672-747a03bb2a03",
   "metadata": {
    "language": "python",
    "name": "Step_7_Cleanup",
    "collapsed": false
   },
   "outputs": [],
   "source": "flag_disable_cleanup_btn = True\nflag_disable_checkbox = True\n\nif (True in set(de_convert_cmd['Convert'])):\n    flag_disable_checkbox = False\n    \nst.write(\"\")\nst.checkbox(\"Remove target table(s) from shares\", key=\"cb_remove_from_share\", disabled=flag_disable_checkbox)\nst.checkbox(\"Drop target table(s)\", key=\"cb_drop_target_table\", disabled=flag_disable_checkbox)\n\nif (True in set(de_convert_cmd['Convert'])) and (st.session_state[\"cb_remove_from_share\"] or st.session_state[\"cb_drop_target_table\"]):\n    flag_disable_cleanup_btn = False\n\nbtn_cleanup = st.button(\"Cleanup\", disabled=flag_disable_cleanup_btn, type=\"primary\")\n\nif btn_cleanup:\n    list_tbls_drop = []\n    for index, row in df_selected_cmds.iterrows():\n        shared_objs = json.loads(row[\"Shared Objects\"])\n        command_type = row[\"Command Type\"]\n        command_ddl = row[\"Command DDL\"]\n\n        if command_type == \"CTAS\":\n            tbl = re.search(r\"(?<=CREATE OR REPLACE TABLE\\s)(.*?)(?=\\s+AS)\", command_ddl).group(1)\n        if command_type == \"INSERT_OVERWRITE\":\n            tbl = re.search(r\"(?<=INSERT OR OVERWRITE INTO\\s)(.*?)(?=\\s+SELECT)\", command_ddl).group(1)\n        \n        if st.session_state[\"cb_remove_from_share\"]:\n            #REMOVE FROM SHARE(S)\n            for dict_obj in shared_objs:\n                flag_shared = dict_obj[\"target_shared\"]\n    \n                if flag_shared.lower() == 'y':\n                    obj = dict_obj[\"object\"]\n                    obj_type = dict_obj[\"object_type\"]\n                    list_share = dict_obj[\"shares\"]\n    \n                    #add obj/type to list\n                    list_tbls_drop.append([obj, obj_type])\n    \n                    for share in list_share:\n                        #remove obj from share\n                        session.sql(f\"\"\"REVOKE SELECT ON {obj_type} {obj} FROM SHARE {share}\"\"\").collect()\n                        st.success(f\"{obj_type} {obj} successfully removed from share: {share}. üéâ\")\n                else:    \n                    st.warning(f\"Table: {tbl} is not currently shared\", icon=\"‚ö†Ô∏è\")\n\n        \n        if st.session_state[\"cb_drop_target_table\"]:\n            session.sql(f\"\"\"DROP TABLE IF EXISTS {tbl}\"\"\").collect()\n            st.success(f\"Table: {tbl} successfully dropped. üéâ\")\n\n        st.divider()",
   "execution_count": null
  }
 ]
}